This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    001-how_to_use.mdc
.github/
  workflows/
    publish.yml
examples/
  pocketflow-rs-rag/
    src/
      nodes/
        chunk_documents.rs
        create_index.rs
        embed_documents.rs
        embed_query.rs
        file_loader.rs
        generate_answer.rs
        mod.rs
        query_rewrite.rs
        retrieve_document.rs
      lib.rs
      main.rs
      state.rs
    Cargo.toml
    README.md
  text2sql/
    example_data/
      customers.csv
      orders.csv
    src/
      flow.rs
      lib.rs
      main.rs
    .gitignore
    Cargo.toml
  basic.rs
src/
  utils/
    embedding.rs
    llm_wrapper.rs
    mod.rs
    text_chunking.rs
    vector_db.rs
    viz_debug.rs
    web_search.rs
  context.rs
  flow.rs
  lib.rs
  node.rs
.gitignore
Cargo.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/001-how_to_use.mdc">
---
description: 
globs: 
alwaysApply: false
---
## How to create a workflow

### Build Node 

1. Define State(Optional)
Each node can define different states based on the possible execution of subsequent nodes. If the node logic is simple, it can be directly implemented using BaseState

A example for define state: the SQL executing node can have states such as: execution successful/execution failed SQL error/execution failed connection error. If the execution is successful, the node can proceed to the next stage. If the execution is incorrect, it will jump to generate SQL and regenerate the node. 

```rust
pub enum SqlExecutorState{
    SqlSyntaxError,
    SqlClientError
    Default, // Success
}
```

2. implement trait function.
+ prepare(optional): Sets up necessary preconditions, preprocess the data in context.
+ execute: Performs the main logic and produces a result.
+ post_process(optional): 
    + Evaluates the execute result, updates the Context.
    + Return the corresponding state based on the result, allowing the Flow runtime library to determine which node to call next by evaluating the edge conditions.
    + If the logic of the node is simple enough and does not require post-processing, it can be omitted and the default can be used.

## Build Flow:

You can use rust macro `build_flow` and `build_batch_flow` to create a workflow for LLM.

such as:

```rust
let flow = build_flow!(
    start: ("start", node1), // define begin node, node1 is object for Node and 'start' is alias.
    nodes: [("next", node2)], // define other nodes as start.
    edges: [
        ("start", "next", MyState::Default) // start -> next, when start post_process returned state is MyState::Default
    ]
);
```
</file>

<file path=".github/workflows/publish.yml">
name: Publish to crates.io

on:
  push:
    tags:
      - 'v*' # Trigger on version tags

env:
  CARGO_TERM_COLOR: always

jobs:
  publish:
    name: Publish
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2

      - name: Run tests
        run: cargo test --all-features

      - name: Run clippy
        run: cargo clippy --all-features -- -D warnings

      - name: Run rustfmt
        run: cargo fmt --all -- --check

      - name: Publish to crates.io
        run: cargo publish --token ${CRATES_TOKEN}
        env:
          CRATES_TOKEN: ${{ secrets.CRATES_TOKEN }}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/chunk_documents.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::text_chunking::{ChunkingOptions, ChunkingStrategy, TextChunker};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::{Value, json};
use tracing::info;

pub struct ChunkDocumentsNode {
    chunker: TextChunker,
    options: ChunkingOptions,
}

impl ChunkDocumentsNode {
    pub fn new(chunk_size: usize, overlap: usize, strategy: ChunkingStrategy) -> Self {
        Self {
            chunker: TextChunker::new(),
            options: ChunkingOptions {
                chunk_size,
                overlap,
                strategy,
            },
        }
    }
}

#[async_trait]
impl Node for ChunkDocumentsNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let documents = context
            .get("documents")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No documents found in context"))?;

        let mut chunks_meta = Vec::new();
        for doc_map in documents {
            let content = doc_map
                .get("content")
                .and_then(|v| v.as_str())
                .ok_or_else(|| anyhow::anyhow!("No content found in document"))?;
            let chunks = self.chunker.chunk_text(content, &self.options);
            info!(
                "Process: {:?}, Chunks lens: {:?}",
                doc_map.get("metadata").unwrap(),
                chunks.len()
            );
            chunks_meta.push(json!({
                "chunks": chunks,
                "metadata": doc_map.get("metadata").unwrap_or(&Value::Null),
            }));
        }

        Ok(Value::Array(chunks_meta))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("documents_chunked", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "documents_chunked".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::ChunkingError,
                format!("chunking_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/create_index.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::vector_db::{
    DistanceMetric, QdrantDB, VectorDB, VectorDBOptions, VectorRecord,
};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;

pub struct CreateIndexNode {
    db: Arc<QdrantDB>,
}

impl CreateIndexNode {
    pub async fn new(
        db_url: String,
        api_key: Option<String>,
        collection: String,
        dimension: usize,
        distance_metric: DistanceMetric,
    ) -> Result<Self> {
        let options = VectorDBOptions {
            collection_name: collection,
            dimension,
            distance_metric,
        };
        let db = QdrantDB::new(db_url, api_key, options).await?;
        Ok(Self { db: Arc::new(db) })
    }
}

#[async_trait]
impl Node for CreateIndexNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let chunks_embeddings = context
            .get("chunk_embeddings")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No embeddings found in context"))?;

        let mut records = Vec::new();
        for chunk_embedding in chunks_embeddings {
            let chunks = chunk_embedding
                .get("chunks")
                .and_then(|v| v.as_array())
                .ok_or_else(|| anyhow::anyhow!("No chunks found in document"))?;
            let embeddings = chunk_embedding
                .get("embeddings")
                .and_then(|v| v.as_array())
                .ok_or_else(|| anyhow::anyhow!("No embeddings found in document"))?;
            let metadata = chunk_embedding.get("metadata").unwrap_or(&Value::Null);

            let chunks_size = chunks.len();
            for i in 0..chunks_size {
                let chunk = chunks[i].to_string();
                let default_embedding = Vec::new();
                let embedding = embeddings[i].as_array().unwrap_or(&default_embedding);
                let embedding_vec: Vec<f32> = embedding
                    .iter()
                    .filter_map(|v| v.as_f64().map(|x| x as f32))
                    .collect();
                records.push(VectorRecord {
                    id: uuid::Uuid::new_v4().to_string(),
                    vector: embedding_vec,
                    metadata: serde_json::Map::from_iter(vec![
                        ("text".to_string(), serde_json::Value::String(chunk)),
                        ("file_metadata".to_string(), metadata.clone()),
                    ]),
                });
            }
        }

        if records.is_empty() {
            return Err(anyhow::anyhow!("No valid records to insert"));
        }

        self.db
            .insert(records)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to insert records: {}", e))?;
        Ok(Value::Null)
    }

    #[allow(unused_variables)]
    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(_) => Ok(ProcessResult::new(
                RagState::Default,
                "index_created".to_string(),
            )),
            Err(e) => Ok(ProcessResult::new(
                RagState::IndexCreationError,
                format!("index_creation_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/embed_documents.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::embedding::EmbeddingGenerator;
use pocketflow_rs::utils::embedding::{EmbeddingOptions, OpenAIEmbeddingGenerator};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::{Value, json};
use std::sync::Arc;
use tracing::{debug, info};

pub struct EmbedDocumentsNode {
    generator: Arc<OpenAIEmbeddingGenerator>,
}

impl EmbedDocumentsNode {
    pub fn new(api_key: String, endpoint: String, model: String, dimension: Option<usize>) -> Self {
        Self {
            generator: Arc::new(OpenAIEmbeddingGenerator::new(
                &api_key,
                &endpoint,
                EmbeddingOptions {
                    model,
                    dimensions: dimension,
                },
            )),
        }
    }
}

#[async_trait]
impl Node for EmbedDocumentsNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let documents_chunked = context
            .get("documents_chunked")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No chunks found in context"))?;
        info!("Documents chunked: {:?}", documents_chunked.len());

        let mut embed_result = Vec::new();
        for chunk in documents_chunked {
            let chunks = chunk
                .get("chunks")
                .and_then(|v| v.as_array())
                .ok_or_else(|| anyhow::anyhow!("No chunks found in document"))?;
            let chunk_text: Vec<String> = chunks
                .iter()
                .filter_map(|v| v.as_str().map(|s| s.to_string()))
                .collect();
            debug!("Chunk text: {:?}", chunk_text);
            info!("Chunk text len: {:?}", chunk_text.len());
            let embeddings = self.generator.generate_embeddings(&chunk_text).await?;
            info!("Embeddings len: {:?}", embeddings.len());
            if embeddings.is_empty() {
                return Err(anyhow::anyhow!("Embeddings array is empty"));
            }
            info!("First Embeddings: {:?}", embeddings[0]);

            embed_result.push(json!(
                {
                    "chunks": chunk_text,
                    "embeddings": embeddings,
                    "metadata": chunk.get("metadata").unwrap_or(&Value::Null),
                }
            ));
        }

        Ok(Value::Array(embed_result))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("chunk_embeddings", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "chunks_embedded".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::EmbeddingError,
                format!("embedding_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/embed_query.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::embedding::{
    EmbeddingGenerator, EmbeddingOptions, OpenAIEmbeddingGenerator,
};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::{Value, json};
use std::sync::Arc;

pub struct EmbedQueryNode {
    generator: Arc<OpenAIEmbeddingGenerator>,
}

impl EmbedQueryNode {
    pub fn new(api_key: String, endpoint: String, model: String, dimension: Option<usize>) -> Self {
        Self {
            generator: Arc::new(OpenAIEmbeddingGenerator::new(
                &api_key,
                &endpoint,
                EmbeddingOptions {
                    model,
                    dimensions: dimension,
                },
            )),
        }
    }
}

#[async_trait]
impl Node for EmbedQueryNode {
    type State = RagState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<Value> {
        let query = context
            .get("rewritten_query")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .to_string();
        let embedding = self.generator.generate_embedding(&query).await?;
        if embedding.is_empty() {
            return Err(anyhow::anyhow!("No embedding generated for query"));
        }
        Ok(Value::Array(
            embedding.into_iter().map(|x| json!(x)).collect(),
        ))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("query_embedding", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "query_embedded".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::QueryEmbeddingError,
                format!("query_embedding_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/file_loader.rs">
use crate::state::RagState;
use anyhow::{Context, Result};
use async_trait::async_trait;
use pdf_extract::extract_text;
use pocketflow_rs::{Context as FlowContext, Node, ProcessResult};
use reqwest::Client;
use serde_json::{Value, json};
use std::fs;
use std::path::Path;
use std::sync::Arc;
use std::time::SystemTime;
use tracing::info;

#[derive(Debug)]
struct Document {
    content: String,
    metadata: Value,
}

impl Document {
    fn new(content: String, url: &str, file_type: &str) -> Self {
        let metadata = json!({
            "url": url,
            "file_type": file_type,
            "timestamp": SystemTime::now()
                .duration_since(SystemTime::UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            "content_length": content.len(),
        });
        Self { content, metadata }
    }
}

pub struct FileLoaderNode {
    urls: Vec<String>,
    client: Arc<Client>,
}

impl FileLoaderNode {
    pub fn new(urls: Vec<String>) -> Self {
        Self {
            urls,
            client: Arc::new(Client::new()),
        }
    }

    fn detect_file_type(path: &Path) -> Result<&'static str> {
        let extension = path
            .extension()
            .and_then(|ext| ext.to_str())
            .ok_or_else(|| anyhow::anyhow!("Could not determine file extension"))?;

        match extension.to_lowercase().as_str() {
            "pdf" => Ok("pdf"),
            "txt" => Ok("text"),
            _ => Err(anyhow::anyhow!("Unsupported file type: {}", extension)),
        }
    }

    async fn load_from_url(&self, url: &str) -> Result<Document> {
        info!("Loading content from URL: {}", url);
        if url.starts_with("http://") || url.starts_with("https://") {
            let response = self.client.get(url).send().await?;
            let content_type = response
                .headers()
                .get("content-type")
                .map(|header| header.to_str().unwrap_or("text/plain"));

            let mut file_type = "web";
            let content = match content_type {
                Some("text/plain") => response.text().await?,
                Some("application/pdf") => {
                    let bytes = response.bytes().await?;
                    file_type = "pdf";
                    pdf_extract::extract_text_from_mem(&bytes)?
                }
                _ => response.text().await?,
            };

            Ok(Document::new(content, url, file_type))
        } else {
            info!("Loading content from local file: {}", url);
            let path = Path::new(url);
            let file_type = Self::detect_file_type(path)?;
            let content = match file_type {
                "pdf" => extract_text(path)
                    .with_context(|| format!("Failed to extract text from PDF: {:?}", path))?,
                "text" => fs::read_to_string(path)
                    .with_context(|| format!("Failed to read text file: {:?}", path))?,
                _ => unreachable!(),
            };
            Ok(Document::new(content, url, file_type))
        }
    }
}

#[async_trait]
impl Node for FileLoaderNode {
    type State = RagState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &FlowContext) -> Result<Value> {
        let mut documents = Vec::new();

        for url in &self.urls {
            let doc = self
                .load_from_url(url)
                .await
                .with_context(|| format!("Failed to load content from URL: {}", url))?;
            info!("Document loaded: {:?}", doc.metadata);
            documents.push(json!({
                "content": doc.content,
                "metadata": doc.metadata
            }));
        }

        if documents.is_empty() {
            return Err(anyhow::anyhow!("No documents loaded from any URL"));
        }

        Ok(Value::Array(documents))
    }

    async fn post_process(
        &self,
        context: &mut FlowContext,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("documents", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "documents_loaded".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::FileLoadedError,
                format!("loading_error: {}", e),
            )),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs::File;
    use std::io::Write;
    use tempfile::tempdir;

    #[tokio::test]
    async fn test_load_text_file() {
        // Create a temporary directory
        let dir = tempdir().unwrap();
        let file_path = dir.path().join("test.txt");

        // Create a test text file
        let mut file = File::create(&file_path).unwrap();
        writeln!(file, "Hello, World!").unwrap();

        // Test loading the text file
        let loader = FileLoaderNode::new(vec![file_path.to_str().unwrap().to_string()]);
        let result = loader.execute(&FlowContext::new()).await.unwrap();

        // Verify the result
        let documents = result.as_array().unwrap();
        assert_eq!(documents.len(), 1);

        let doc = &documents[0];
        assert_eq!(doc["content"].as_str().unwrap(), "Hello, World!\n");
        assert_eq!(doc["metadata"]["file_type"].as_str().unwrap(), "text");
    }

    #[tokio::test]
    async fn test_load_multiple_files() {
        let dir = tempdir().unwrap();

        let text_path = dir.path().join("test.txt");
        let mut text_file = File::create(&text_path).unwrap();
        writeln!(text_file, "Text content").unwrap();

        let urls = vec![
            text_path.to_str().unwrap().to_string(),
            "https://pdfobject.com/pdf/sample.pdf".to_string(),
        ];

        let loader = FileLoaderNode::new(urls);
        let result = loader.execute(&FlowContext::new()).await;

        if let Ok(result) = result {
            let documents = result.as_array().unwrap();
            assert!(documents.len() > 0);

            for doc in documents {
                assert!(doc["content"].is_string());
                assert!(doc["metadata"]["url"].is_string());
                assert!(doc["metadata"]["file_type"].is_string());
                assert!(doc["metadata"]["timestamp"].is_number());
                assert!(doc["metadata"]["content_length"].is_number());
            }
        }
    }

    #[tokio::test]
    async fn test_invalid_file_type() {
        let dir = tempdir().unwrap();
        let file_path = dir.path().join("test.xyz");

        let mut file = File::create(&file_path).unwrap();
        writeln!(file, "Some content").unwrap();

        let loader = FileLoaderNode::new(vec![file_path.to_str().unwrap().to_string()]);
        let result = loader.execute(&FlowContext::new()).await;

        assert!(result.is_err());
        let error = result.unwrap_err();
        assert!(
            error
                .to_string()
                .contains("Failed to load content from URL")
        );
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/generate_answer.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::llm_wrapper::{LLMWrapper, OpenAIClient};
use pocketflow_rs::vector_db::VectorRecord;
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;

pub struct GenerateAnswerNode {
    client: Arc<OpenAIClient>,
    query: String,
}

impl GenerateAnswerNode {
    pub fn new(api_key: String, model: String, endpoint: String, query: String) -> Self {
        Self {
            client: Arc::new(OpenAIClient::new(api_key, model, endpoint)),
            query,
        }
    }
}

#[async_trait]
impl Node for GenerateAnswerNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let retrieved_docs = context
            .get("retrieved_documents")
            .and_then(|v| v.as_array())
            .ok_or_else(|| anyhow::anyhow!("No retrieved documents found in context"))?;

        let retrieved_docs_array: Vec<VectorRecord> = retrieved_docs
            .iter()
            .map(VectorRecord::parse_by_value)
            .collect();

        let retrieved_text_with_meta = retrieved_docs_array
            .iter()
            .map(|v| {
                format!(
                    "{}: {}",
                    v.metadata
                        .get("file_metadata")
                        .unwrap()
                        .get("url")
                        .unwrap()
                        .as_str()
                        .unwrap(),
                    v.metadata.get("text").unwrap()
                )
            })
            .collect::<Vec<_>>()
            .join("\n\n");

        if retrieved_text_with_meta.is_empty() {
            return Ok(Value::String("I don't know.".to_string()));
        }

        let prompt = format!("
You are a helpful assistant. Based on the following context, please answer the question. If the answer cannot be found in the context, say 'I don't know'.\n\n
Output format using markdown and add reference links to the source documents. \n\n
You can use the following context to answer the question: \n{}\n\n
Question: {}\n\n
Answer:",
        retrieved_text_with_meta,
            self.query
        );

        let response = self.client.generate(&prompt).await?;
        if response.content.is_empty() {
            return Err(anyhow::anyhow!("Empty response from LLM"));
        }

        Ok(Value::String(response.content.trim().to_string()))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("result", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "answer_generated".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::GenerationError,
                format!("generation_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/mod.rs">
mod chunk_documents;
mod create_index;
mod embed_documents;
mod embed_query;
mod file_loader;
mod generate_answer;
mod query_rewrite;
mod retrieve_document;

pub use chunk_documents::ChunkDocumentsNode;
pub use create_index::CreateIndexNode;
pub use embed_documents::EmbedDocumentsNode;
pub use embed_query::EmbedQueryNode;
pub use file_loader::FileLoaderNode;
pub use generate_answer::GenerateAnswerNode;
pub use query_rewrite::QueryRewriteNode;
pub use retrieve_document::RetrieveDocumentNode;
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/query_rewrite.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::llm_wrapper::{LLMWrapper, OpenAIClient};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;
use tracing::info;

pub struct QueryRewriteNode {
    client: Arc<OpenAIClient>,
}

impl QueryRewriteNode {
    pub fn new(api_key: String, model: String, endpoint: String) -> Self {
        Self {
            client: Arc::new(OpenAIClient::new(api_key, model, endpoint)),
        }
    }
}

#[async_trait]
impl Node for QueryRewriteNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let user_query = context.get("user_query").unwrap();
        let prompt = format!("
**Role:** You are an AI Query Enhancer for a Retrieval-Augmented Generation (RAG) system.

**Goal:** Your task is to take a raw user query and rewrite it into an optimized query string suitable for vector database search. This involves identifying the user's core intent and transforming the query into a concise, keyword-focused format that maximizes the chances of retrieving relevant documents.

**Input:** You will receive a single \"Original User Query\".

**Instructions:**

1.  **Analyze Intent:** Carefully examine the \"Original User Query\" to understand the user's underlying information need or question. What are they *really* trying to find out?
2.  **Identify Keywords:** Extract the most critical entities, concepts, and keywords from the query.
3.  **Remove Filler:** Discard conversational filler, politeness phrases (e.g., \"please\", \"can you tell me\"), and vague phrasing (\"thing\", \"stuff\", \"how about\").
4.  **Rewrite for Clarity & Conciseness:** Construct a new query string that clearly represents the intent using the identified keywords. Make it specific and direct.
5.  **Consider Expansion (Optional but Recommended):** If the original query is very sparse or could benefit from clarification, cautiously add 1-2 highly relevant synonyms or closely related terms that specify the intent further (e.g., adding \"nutrition\" if the query is just \"apples\"). Avoid overly broad expansion.
6.  **Format for Embedding:** The final rewritten query should be a simple string, optimized for being turned into a vector embedding for semantic search.

**Output:** Respond with ONLY the rewritten query string. Do not include any explanations or introductory text.

**Example 1:**
Original User Query: \"Hey, could you tell me about the financial performance of Tesla last year?\"
Rewritten Query: `Tesla financial performance 2024 earnings report revenue analysis`

**Example 2:**
Original User Query: \"What's the deal with that new AI that makes pictures?\"
Rewritten Query: `AI image generation model technology explanation diffusion transformer`

**Example 3:**
Original User Query: \"I need help understanding how to mitigate risks in my supply chain in Europe.\"
Rewritten Query: `supply chain risk mitigation strategies Europe logistics management`

**Now, process the following input:**

Original User Query: \"{}\"
Rewritten Query:",user_query);
        let response = self.client.generate(&prompt).await?;
        info!("Query rewritten: {:?}", response.content);
        Ok(Value::String(response.content.replace("`", "")))
    }

    #[allow(unused_variables)]
    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        return match result {
            Ok(value) => {
                context.set("rewritten_query", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "query_rewritten".to_string(),
                ))
            }
            Err(e) => {
                info!("Error rewriting query: {:?}", e);
                Ok(ProcessResult::new(
                    RagState::QueryRewriteError,
                    "query_rewrite_error".to_string(),
                ))
            }
        };
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/nodes/retrieve_document.rs">
use crate::state::RagState;
use anyhow::Result;
use async_trait::async_trait;
use pocketflow_rs::utils::vector_db::{QdrantDB, VectorDB};
use pocketflow_rs::vector_db::{DistanceMetric, VectorDBOptions};
use pocketflow_rs::{Context, Node, ProcessResult};
use serde_json::Value;
use std::sync::Arc;
use tracing::{error, info};

pub struct RetrieveDocumentNode {
    db: Arc<QdrantDB>,
    k: usize,
}

impl RetrieveDocumentNode {
    pub async fn new(
        db_url: String,
        api_key: Option<String>,
        collection: String,
        dimension: usize,
        distance_metric: DistanceMetric,
        k: usize,
    ) -> Result<Self> {
        let db = QdrantDB::new(
            db_url,
            api_key,
            VectorDBOptions {
                collection_name: collection,
                dimension,
                distance_metric,
            },
        )
        .await?;
        Ok(Self {
            db: Arc::new(db),
            k,
        })
    }
}

#[async_trait]
impl Node for RetrieveDocumentNode {
    type State = RagState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let query_embedding = context
            .get("query_embedding")
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_f64().map(|x| x as f32))
                    .collect::<Vec<f32>>()
            })
            .ok_or_else(|| anyhow::anyhow!("No query embedding found in context"))?;

        let records = self.db.search(query_embedding, self.k).await?;
        if records.is_empty() {
            error!("No documents retrieved");
            return Err(anyhow::anyhow!("No documents retrieved"));
        }

        info!("Retrieved documents line: {:?}", records.len());

        let result_array: Vec<Value> = records
            .into_iter()
            .map(|record| record.to_value())
            .collect();

        Ok(Value::Array(result_array))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<RagState>> {
        match result {
            Ok(value) => {
                context.set("retrieved_documents", value.clone());
                Ok(ProcessResult::new(
                    RagState::Default,
                    "documents_retrieved".to_string(),
                ))
            }
            Err(e) => Ok(ProcessResult::new(
                RagState::RetrievalError,
                format!("retrieval_error: {}", e),
            )),
        }
    }
}
</file>

<file path="examples/pocketflow-rs-rag/src/lib.rs">
pub mod nodes;
pub mod state;

pub use nodes::*;
pub use state::*;
</file>

<file path="examples/pocketflow-rs-rag/src/main.rs">
use anyhow::Result;
use clap::{Parser, Subcommand};
use pocketflow_rs::utils::{text_chunking::ChunkingStrategy, vector_db::DistanceMetric};
use pocketflow_rs::{Context as FlowContext, build_flow};
use pocketflow_rs_rag::{
    QueryRewriteNode,
    nodes::{
        ChunkDocumentsNode, CreateIndexNode, EmbedDocumentsNode, EmbedQueryNode, FileLoaderNode,
        GenerateAnswerNode, RetrieveDocumentNode,
    },
    state::RagState,
};
use serde_json::json;
use tracing::Level;
use tracing_subscriber::FmtSubscriber;

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Process documents offline
    Offline {
        /// Qdrant database URL
        #[arg(long, default_value = "http://localhost:6333")]
        db_url: String,

        /// Collection name in Qdrant
        #[arg(long, default_value = "documents")]
        collection: String,

        /// OpenAI API key
        #[arg(long)]
        api_key: String,

        /// Qdrant API key
        #[arg(long)]
        qdrant_api_key: Option<String>,

        /// OpenAI API endpoint
        #[arg(short, long, default_value = "https://api.openai.com/v1")]
        endpoint: String,

        /// Chunk size for document splitting
        #[arg(long, default_value = "1000")]
        chunk_size: usize,

        /// Overlap between chunks
        #[arg(long, default_value = "200")]
        overlap: usize,

        /// OpenAI model to use
        #[arg(long, default_value = "text-embedding-ada-002")]
        model: String,

        #[arg(long, default_value = "1024")]
        dimension: usize,

        /// Paths to document files
        #[arg(required = true)]
        files: Vec<String>,
    },
    /// Online processing: answer questions based on indexed documents
    Online {
        /// Qdrant database URL
        #[arg(long, default_value = "http://localhost:6333")]
        db_url: String,

        /// Collection name in Qdrant
        #[arg(long, default_value = "documents")]
        collection: String,

        /// OpenAI API key
        #[arg(long)]
        api_key: String,

        /// OpenAI API endpoint
        #[arg(long, default_value = "https://api.openai.com/v1")]
        endpoint: String,

        /// Number of documents to retrieve
        #[arg(short, long, default_value = "3")]
        k: usize,

        /// chat mode
        #[arg(long, default_value = "chat")]
        chat_mode: String,

        /// embedding dimension
        #[arg(long, default_value = "1024")]
        dimension: usize,

        /// Qdrant API key
        #[arg(long)]
        qdrant_api_key: Option<String>,

        /// Embedding model
        #[arg(long, default_value = "text-embedding-ada-002")]
        embedding_model: String,

        /// Question to answer
        #[arg(required = true)]
        query: String,
    },
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();
    FmtSubscriber::builder().with_max_level(Level::INFO).init();

    match cli.command {
        Commands::Offline {
            files,
            db_url,
            collection,
            api_key,
            qdrant_api_key,
            endpoint,
            chunk_size,
            overlap,
            model,
            dimension,
        } => {
            let file_loader = FileLoaderNode::new(files);
            let chunk_documents =
                ChunkDocumentsNode::new(chunk_size, overlap, ChunkingStrategy::Sentence);
            let embed_documents = EmbedDocumentsNode::new(
                api_key.clone(),
                endpoint.clone(),
                model.clone(),
                Some(dimension),
            );
            let create_index = CreateIndexNode::new(
                db_url,
                qdrant_api_key,
                collection,
                dimension,
                DistanceMetric::Cosine,
            )
            .await?;

            let flow = build_flow!(
                start: ("file_loader", file_loader),
                nodes: [
                    ("chunk_documents", chunk_documents),
                    ("embed_documents", embed_documents),
                    ("create_index", create_index)
                ],
                edges: [
                    ("file_loader", "chunk_documents", RagState::Default),
                    ("chunk_documents", "embed_documents", RagState::Default),
                    ("embed_documents", "create_index", RagState::Default)
                ]
            );

            flow.run(FlowContext::new()).await?;
        }
        Commands::Online {
            query,
            db_url,
            collection,
            api_key,
            endpoint,
            k,
            chat_mode,
            dimension,
            qdrant_api_key,
            embedding_model,
        } => {
            let mut context = FlowContext::new();
            context.set("user_query", json!(query.clone()));

            let query_rewrite_node =
                QueryRewriteNode::new(api_key.clone(), chat_mode.clone(), endpoint.clone());

            let embed_query_node = EmbedQueryNode::new(
                api_key.clone(),
                endpoint.clone(),
                embedding_model.clone(),
                Some(dimension),
            );

            let retrieve_node = RetrieveDocumentNode::new(
                db_url,
                qdrant_api_key,
                collection,
                dimension,
                DistanceMetric::Cosine,
                k,
            )
            .await?;

            let generate_node = GenerateAnswerNode::new(api_key, chat_mode, endpoint, query);

            // Build and execute online flow
            let flow = build_flow!(
                start: ("query_rewrite", query_rewrite_node),
                nodes: [
                    ("embed_query", embed_query_node),
                    ("retrieve", retrieve_node),
                    ("generate", generate_node)
                ],
                edges: [
                    ("query_rewrite", "embed_query", RagState::Default),
                    ("embed_query", "retrieve", RagState::Default),
                    ("retrieve", "generate", RagState::Default)
                ]
            );

            let result = flow.run(context).await?;

            termimad::print_text(result.as_str().unwrap());
        }
    }

    Ok(())
}
</file>

<file path="examples/pocketflow-rs-rag/src/state.rs">
use pocketflow_rs::ProcessState;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum RagState {
    // Offline states
    FileLoadedError,
    DocumentsLoaded,
    DocumentsChunked,
    ChunksEmbedded,
    IndexCreated,
    // Offline error states
    DocumentLoadError,
    ChunkingError,
    EmbeddingError,
    IndexCreationError,
    // Online states
    QueryEmbedded,
    DocumentsRetrieved,
    AnswerGenerated,
    // Online error states
    QueryEmbeddingError,
    RetrievalError,
    GenerationError,
    Default,
    QueryRewriteError,
}

impl ProcessState for RagState {
    fn is_default(&self) -> bool {
        matches!(self, RagState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            // Offline states
            RagState::FileLoadedError => "file_loaded_error".to_string(),
            RagState::DocumentsLoaded => "documents_loaded".to_string(),
            RagState::DocumentsChunked => "documents_chunked".to_string(),
            RagState::ChunksEmbedded => "chunks_embedded".to_string(),
            RagState::IndexCreated => "index_created".to_string(),
            // Offline error states
            RagState::DocumentLoadError => "document_load_error".to_string(),
            RagState::ChunkingError => "chunking_error".to_string(),
            RagState::EmbeddingError => "embedding_error".to_string(),
            RagState::IndexCreationError => "index_creation_error".to_string(),
            // Online states
            RagState::QueryEmbedded => "query_embedded".to_string(),
            RagState::DocumentsRetrieved => "documents_retrieved".to_string(),
            RagState::AnswerGenerated => "answer_generated".to_string(),
            // Online error states
            RagState::QueryEmbeddingError => "query_embedding_error".to_string(),
            RagState::RetrievalError => "retrieval_error".to_string(),
            RagState::GenerationError => "generation_error".to_string(),
            RagState::Default => "default".to_string(),
            RagState::QueryRewriteError => "query_rewrite_error".to_string(),
        }
    }
}

impl Default for RagState {
    fn default() -> Self {
        RagState::Default
    }
}
</file>

<file path="examples/pocketflow-rs-rag/Cargo.toml">
[package]
name = "pocketflow-rs-rag"
version = "0.1.0"
edition = "2024"

[dependencies]
pocketflow_rs = { path = "../../", features = ["openai", "qdrant", "debug"] }
anyhow = "1.0"
tokio = { version = "1.0", features = ["full"] }
tracing = "0.1"
tracing-subscriber = "0.3"
serde_json = "1.0"
async-trait = "0.1"
faiss = "0.12.1"
clap = { version = "4.5", features = ["derive"] }
pdf-extract = "0.9"
reqwest = { version = "0.12.15", features = ["json"] }
uuid = { version = "1.16.0", features = ["v4"] }
qdrant-client = "1.14.0"
termimad = "0.31.3"

[dev-dependencies]
tempfile = "3.8"
</file>

<file path="examples/pocketflow-rs-rag/README.md">
# PocketFlow RAG Example

## Overview

This example demonstrates how to use PocketFlow to build a Retrieval-Augmented Generation (RAG) pipeline. The implementation consists of two main components: an offline pipeline for document processing and indexing, and an online pipeline for question answering.

### Offline Pipeline

The offline pipeline processes and indexes documents for later retrieval. It consists of the following nodes:

- `FileLoaderNode`: Loads documents from local files or URLs, supporting various formats including PDF, text, and web pages.
- `ChunkDocumentsNode`: Splits documents into smaller chunks using configurable chunk size and overlap, with support for different chunking strategies.
- `EmbedDocumentsNode`: Converts document chunks into vector embeddings using OpenAI's embedding models.
- `CreateIndexNode`: Stores the embedded chunks in a Qdrant vector database with configurable distance metrics.

### Online Pipeline

The online pipeline handles real-time question answering using the indexed documents. It includes:

- `QueryRewriteNode`: Enhances the user's query using LLM to improve retrieval quality.
- `EmbedQueryNode`: Converts the rewritten query into a vector embedding.
- `RetrieveDocumentNode`: Retrieves the most relevant document chunks from the vector database.
- `GenerateAnswerNode`: Generates a comprehensive answer based on the retrieved context and the original query.

The pipeline supports various configuration options including:

- Customizable embedding models and dimensions
- Configurable chunk sizes and overlap
- Adjustable number of retrieved documents
- Different chat modes for answer generation
- Flexible vector database settings

## Workflow Diagram

```mermaid
graph TB
    subgraph Offline["Offline Pipeline"]
        direction LR
        FL[FileLoaderNode] --> CD[ChunkDocumentsNode]
        CD --> ED[EmbedDocumentsNode]
        ED --> CI[CreateIndexNode]
    end

    subgraph Online["Online Pipeline"]
        direction LR
        QR[QueryRewriteNode] --> EQ[EmbedQueryNode]
        EQ --> RD[RetrieveDocumentNode]
        RD --> GA[GenerateAnswerNode]
    end

    style Offline fill:#f9f,stroke:#333,stroke-width:2px
    style Online fill:#bbf,stroke:#333,stroke-width:2px
```

## Example Usage

### run offline pipeline

```bash
cargo run -- offline --db-url <qdrant-db-url> --collection <collection-name> --api-key <openai-api-key> --qdrant-api-key <qdrant-api-key> --endpoint <openai-endpoint> --chunk-size <chunk-size> --overlap <overlap> --model <embedding-model> --dimension <dimension> https://www.usenix.org/system/files/fast23-li-qiang_more.pdf https://www.usenix.org/system/files/fast23-li-qiang.pdf
```

### run online pipeline

```bash
cargo run -- online --db-url <qdrant-db-url> --collection <collection-name> --api-key <openai-api-key> --qdrant-api-key <qdrant-api-key> --endpoint <openai-endpoint> --embedding-model <embedding-model> --chat-mode <chat-mode> --dimension <dimension> --k <k> "Introduce Alibaba Cloud's Pangu distributed file system"
```

### Output

```markdown
Alibaba Cloud's Pangu is a large-scale, distributed storage system that has been in development and deployment since 2009. It serves as a unified storage platform for Alibaba Group and Alibaba Cloud, providing scalable, high-performance, and reliable storage services to support core businesses such as Taobao, Tmall, AntFin, and Alimama. A variety of cloud services, including Elastic Block Storage (EBS), Object Storage Service (OSS), Network-Attached Storage (NAS), PolarDB, and MaxCompute, are built on top of Pangu. Over more than a decade, Pangu has grown into a global storage system managing exabytes of data and trillions of files.

### Evolution of Pangu

Pangu's evolution can be divided into two main phases:

1. **Pangu 1.0 (2009-2015)**: This version was designed on an infrastructure composed of servers with commodity CPUs and hard disk drives (HDDs), which have millisecond-level I/O latency, and Gbps-level datacenter networks. Pangu 1.0 featured a distributed kernel-space file system based on Linux Ext4 and kernel-space TCP, gradually adding support for multiple file types (e.g., TempFile, LogFile, and random access files) as required by different storage services. During this period, the primary focus was on providing large volumes of storage space rather than high performance.

2. **Pangu 2.0 (Since 2015)**: In response to the emergence of new hardware technologies, particularly solid-state drives (SSDs) and remote direct memory access (RDMA), Pangu 2.0 was developed to provide high-performance storage services with a 100µs-level I/O latency. Key innovations include:
   - **Embracing SSD and RDMA**: To leverage the low latency of SSDs and RDMA, Pangu 2.0 introduced a series of new designs in its file system and developed a user-space storage operating system.
   - **High Throughput and IOPS**: Pangu 2.0 aims to achieve high throughput and IOPS, with an effective throughput on storage servers approaching their capacity.
   - **Unified High-Performance Support**: The system provides unified high-performance support to all services running on top of it, such as online search, data streaming analytics, EBS, OSS, and databases.

### Design Goals of Pangu 2.0

- **Low Latency**: Pangu 2.0 targets an average 100µs-level I/O latency in a computation-storage disaggregated architecture, even under dynamic environments like network traffic jitters and server failures.
- **High Throughput**: The system aims to reach an effective throughput on storage servers that approaches their capacity.
- **Unified High-Performance Support**: Pangu 2.0 provides unified high-performance support to all services, ensuring that all applications benefit from the advancements in hardware and software.

### Related Work

Pangu is part of a broader ecosystem of distributed storage systems, both open-source (e.g., HDFS and Ceph) and proprietary (e.g., GFS, Tectonic, and AWS). Alibaba has shared its experiences in various aspects of Pangu, including the large-scale deployment of RDMA, key-value engines for scale-out cloud storage, co-design of network and storage software stacks for EBS, and key designs of the namespace metadata service.

For more detailed information, you can refer to the following sources:

- [FAST '23 Paper: "Fisc: A Lightweight Client for Large-Scale Distributed File Systems"](https://www.usenix.org/system/files/fast23-li-qiang.pdf)_more.pdf)

These documents provide in-depth insights into the design, implementation, and operational experiences of Pangu.
```
</file>

<file path="examples/text2sql/example_data/customers.csv">
id,name,city,email,signup_date
1,John Doe,New York,john.doe@example.com,2023-01-15
2,Jane Smith,San Francisco,jane.smith@example.com,2023-02-20
3,Robert Johnson,New York,robert.j@example.com,2023-03-05
4,Emily Davis,Chicago,emily.d@example.com,2023-01-30
5,Michael Brown,Boston,michael.b@example.com,2023-04-10
6,Lisa Wang,Chicago,lisa.w@example.com,2023-05-12
7,David Lee,San Francisco,david.l@example.com,2023-02-28
8,Sarah Miller,Boston,sarah.m@example.com,2023-06-01
9,James Taylor,New York,james.t@example.com,2023-04-22
10,Amanda Garcia,Chicago,amanda.g@example.com,2023-03-18
</file>

<file path="examples/text2sql/example_data/orders.csv">
order_id,customer_id,product,amount,order_date
101,1,Laptop,1299.99,2023-02-10
102,3,Phone,799.99,2023-03-15
103,2,Headphones,149.99,2023-02-22
104,5,Tablet,499.99,2023-04-18
105,1,Monitor,349.99,2023-05-05
106,4,Keyboard,89.99,2023-03-10
107,3,Mouse,59.99,2023-04-02
108,7,Printer,279.99,2023-03-28
109,9,Speakers,129.99,2023-05-15
110,10,Webcam,79.99,2023-04-20
</file>

<file path="examples/text2sql/src/flow.rs">
use anyhow::{Context as AnyhowContext, Result};
use async_trait::async_trait;
use chrono::NaiveDate;
use duckdb::types::ValueRef;
use duckdb::{Connection, Result as DuckResult};
use openai_api_rust::chat::*;
use openai_api_rust::*;
use pocketflow_rs::{Context, Node, ProcessResult, ProcessState};
use serde_json::{Value, json};
use tracing::{error, info};

#[derive(Debug, Clone, PartialEq)]
pub enum SqlExecutorState {
    SchemaRetrieved,
    SqlGenerated,
    SqlExecuted,
    Default,
}

impl ProcessState for SqlExecutorState {
    fn is_default(&self) -> bool {
        matches!(self, SqlExecutorState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            SqlExecutorState::SchemaRetrieved => "schema_retrieved".to_string(),
            SqlExecutorState::SqlGenerated => "sql_generated".to_string(),
            SqlExecutorState::SqlExecuted => "sql_executed".to_string(),
            SqlExecutorState::Default => "default".to_string(),
        }
    }
}

impl Default for SqlExecutorState {
    fn default() -> Self {
        SqlExecutorState::Default
    }
}

#[derive(Debug, thiserror::Error)]
pub enum WorkflowError {
    #[error("NodeExecution: {0}")]
    NodeExecution(String),
}

pub struct SchemaRetrievalNode {
    db_path: String,
}

impl SchemaRetrievalNode {
    pub fn new(db_path: String) -> Self {
        Self { db_path }
    }
}

#[async_trait]
impl Node for SchemaRetrievalNode {
    type State = SqlExecutorState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<Value> {
        info!("Exec SchemaRetrievalNode");
        let conn = Connection::open(&self.db_path)?;

        let query = "SELECT table_name FROM information_schema.tables WHERE table_schema='main'";
        let mut stmt = conn.prepare(query)?;
        let tables = stmt.query_map([], |row| Ok(row.get(0)?));

        let tables = tables.context("获取表名失败")?;

        let mut schema = serde_json::Map::new();
        for table in tables {
            let table_name = table?;
            let query = format!(
                "SELECT column_name, data_type, is_nullable, column_default
                 FROM information_schema.columns
                 WHERE table_name='{}' AND table_schema='main'",
                table_name
            );

            let mut stmt = conn.prepare(&query)?;
            let columns = stmt
                .query_map([], |row| {
                    Ok(json!({
                        "name": row.get::<_, String>(0)?,
                        "type": row.get::<_, String>(1)?,
                        "nullable": row.get::<_, String>(2)? == "YES",
                        "default_value": row.get::<_, Option<String>>(3)?,
                    }))
                })?
                .collect::<DuckResult<Vec<Value>>>()
                .context("Get Column Info Failed")?;

            schema.insert(table_name, Value::Array(columns));
        }
        info!("Get Result Final");

        Ok(Value::Object(schema))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<SqlExecutorState>> {
        context.set("result", result.as_ref().unwrap().clone());
        Ok(ProcessResult::new(
            SqlExecutorState::SchemaRetrieved,
            "schema_retrieved".to_string(),
        ))
    }
}

pub struct OpenAISQLGenerationNode {
    api_key: String,
    user_query: String,
}

impl OpenAISQLGenerationNode {
    pub fn new(api_key: String, user_query: String) -> Self {
        Self {
            api_key,
            user_query,
        }
    }
}

pub fn print_table(headers: &[String], data: &[Vec<String>]) {
    if headers.is_empty() {
        println!("Query returned no columns.");
        return;
    }

    // Calculate column widths based on headers and data
    let mut widths: Vec<usize> = headers.iter().map(|h| h.len()).collect();
    for row in data {
        for (i, cell) in row.iter().enumerate() {
            if i < widths.len() {
                widths[i] = widths[i].max(cell.len());
            }
        }
    }

    // Print Header
    let header_line = headers
        .iter()
        .zip(&widths)
        .map(|(h, w)| format!("{:<width$}", h, width = w))
        .collect::<Vec<_>>()
        .join(" | ");
    println!("\n{}", header_line);

    // Print Separator
    let separator_line = widths
        .iter()
        .map(|w| "-".repeat(*w))
        .collect::<Vec<_>>()
        .join("-+-");
    println!("{}", separator_line);

    // Print Data Rows
    if data.is_empty() {
        println!("(No rows returned)");
    } else {
        for row in data {
            let row_line = row
                .iter()
                .zip(&widths)
                .map(|(cell, w)| format!("{:<width$}", cell, width = w))
                .collect::<Vec<_>>()
                .join(" | ");
            println!("{}", row_line);
        }
    }
}

#[async_trait]
impl Node for OpenAISQLGenerationNode {
    type State = SqlExecutorState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let schema = context.get("result").ok_or_else(|| {
            WorkflowError::NodeExecution("Failed to get database schema".to_string())
        })?;

        let system_prompt = "You are a SQL expert. Based on the provided database schema and user query, generate the correct SQL query. Only return the SQL query, do not include any explanation or other text. The condition content uses English, you can choose to query some fields first, then make a general query.";

        let schema_json =
            serde_json::to_string_pretty(schema).context("Failed to serialize database schema")?;

        let user_prompt = format!(
            "database schema:\n{}\n\nuser query:\n{}\n\nPlease generate a SQL query to answer this question.",
            schema_json, self.user_query
        );

        let auth = Auth::new(self.api_key.as_str());
        let openai = OpenAI::new(auth, "https://dashscope.aliyuncs.com/compatible-mode/v1/");
        let body = ChatBody {
            model: "qwen-plus".to_string(),
            max_tokens: Some(1024),
            temperature: Some(0.8_f32),
            top_p: Some(0_f32),
            n: Some(1),
            stream: Some(false),
            stop: None,
            presence_penalty: None,
            frequency_penalty: None,
            logit_bias: None,
            user: None,
            messages: vec![
                Message {
                    role: Role::System,
                    content: system_prompt.to_string(),
                },
                Message {
                    role: Role::User,
                    content: user_prompt,
                },
            ],
        };
        let rs = openai.chat_completion_create(&body);
        if rs.is_err() {
            error!("OpenAI Error {}", rs.as_ref().err().unwrap().to_string());
        }
        let choice = rs.unwrap().choices;
        let message = &choice[0].message.as_ref().unwrap();

        let sql = message.content.clone();

        println!("生成的SQL查询: {}", sql);

        Ok(Value::String(sql))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<SqlExecutorState>> {
        context.set("result", result.as_ref().unwrap().clone());
        Ok(ProcessResult::new(
            SqlExecutorState::SqlGenerated,
            "sql_generated".to_string(),
        ))
    }
}

pub struct ExecuteSQLNode {
    db_path: String,
}

impl ExecuteSQLNode {
    pub fn new(db_path: String) -> Self {
        Self { db_path }
    }
}

#[async_trait]
impl Node for ExecuteSQLNode {
    type State = SqlExecutorState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let conn = Connection::open(&self.db_path)?;

        let sql = context
            .get("result")
            .and_then(|v| v.as_str())
            .ok_or_else(|| {
                WorkflowError::NodeExecution("SQL query not found in context".to_string())
            })?;

        info!("ExecuteSQLNode: Get Sql: {}", sql);

        let mut stmt = conn.prepare(sql)?;
        let mut rows = stmt.query([])?;

        let mut headers = Vec::new();
        let mut data_rows = Vec::new();

        if let Some(first_row) = rows.next()? {
            // Get column names from the first row
            headers = first_row.as_ref().column_names();
            let column_count = headers.len();

            // Process first row
            let mut row_values = Vec::with_capacity(column_count);
            for i in 0..column_count {
                let value_ref = first_row.get_ref(i)?;
                let string_value = match value_ref {
                    ValueRef::Null => "NULL".to_string(),
                    ValueRef::Boolean(b) => b.to_string(),
                    ValueRef::TinyInt(i) => i.to_string(),
                    ValueRef::SmallInt(i) => i.to_string(),
                    ValueRef::Int(i) => i.to_string(),
                    ValueRef::BigInt(i) => i.to_string(),
                    ValueRef::Float(f) => f.to_string(),
                    ValueRef::Double(d) => d.to_string(),
                    ValueRef::Text(bytes) => String::from_utf8_lossy(bytes).to_string(),
                    ValueRef::Blob(_) => "[BLOB]".to_string(),
                    ValueRef::Date32(d) => {
                        let date = NaiveDate::from_num_days_from_ce_opt(d as i32 + 719163).unwrap();
                        date.format("%Y-%m-%d").to_string()
                    }
                    _ => format!("Unsupported: {:?}", value_ref),
                };
                row_values.push(string_value);
            }
            data_rows.push(row_values);

            // Process remaining rows
            while let Some(row) = rows.next()? {
                let mut row_values = Vec::with_capacity(column_count);
                for i in 0..column_count {
                    let value_ref = row.get_ref(i)?;
                    let string_value = match value_ref {
                        ValueRef::Null => "NULL".to_string(),
                        ValueRef::Boolean(b) => b.to_string(),
                        ValueRef::TinyInt(i) => i.to_string(),
                        ValueRef::SmallInt(i) => i.to_string(),
                        ValueRef::Int(i) => i.to_string(),
                        ValueRef::BigInt(i) => i.to_string(),
                        ValueRef::Float(f) => f.to_string(),
                        ValueRef::Double(d) => d.to_string(),
                        ValueRef::Text(bytes) => String::from_utf8_lossy(bytes).to_string(),
                        ValueRef::Blob(_) => "[BLOB]".to_string(),
                        ValueRef::Date32(d) => {
                            let date =
                                NaiveDate::from_num_days_from_ce_opt(d as i32 + 719163).unwrap();
                            date.format("%Y-%m-%d").to_string()
                        }
                        _ => format!("Unsupported: {:?}", value_ref),
                    };
                    row_values.push(string_value);
                }
                data_rows.push(row_values);
            }
        }

        print_table(&headers, &data_rows);

        Ok(json!({
            "columns": headers,
            "data": data_rows
        }))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<SqlExecutorState>> {
        context.set("result", result.as_ref().unwrap().clone());
        Ok(ProcessResult::new(
            SqlExecutorState::SqlExecuted,
            "sql_executed".to_string(),
        ))
    }
}
</file>

<file path="examples/text2sql/src/lib.rs">
pub mod flow;
</file>

<file path="examples/text2sql/src/main.rs">
use std::env;

use anyhow::Result;
use duckdb::Connection;
use pocketflow_rs::{Context, build_flow};
use text2sql::flow::{ExecuteSQLNode, OpenAISQLGenerationNode, SchemaRetrievalNode};

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::fmt::init();

    let db_path = "ecommerce.duckdb";
    let conn = Connection::open(db_path)?;

    conn.execute(&format!(
        "CREATE TABLE IF NOT EXISTS customers AS SELECT * FROM read_csv_auto('{}', AUTO_DETECT=TRUE)",
        "example_data/customers.csv"
    ), [])?;

    conn.execute(&format!(
        "CREATE TABLE IF NOT EXISTS orders AS SELECT * FROM read_csv_auto('{}', AUTO_DETECT=TRUE)",
        "example_data/orders.csv"
    ), [])?;

    println!("please input your query using natural language?");
    let mut user_query = String::new();
    std::io::stdin().read_line(&mut user_query)?;
    user_query = user_query.trim().to_string();

    let schema_retrieval = SchemaRetrievalNode::new(db_path.to_string());
    let openai_sql_gen =
        OpenAISQLGenerationNode::new(env::var("DASH_SCOPE_API_KEY").unwrap(), user_query);
    let execute_sql = ExecuteSQLNode::new(db_path.to_string());

    let flow = build_flow! (
        start: ("start", schema_retrieval),
        nodes: [
            ("generate_sql", openai_sql_gen),
            ("execute_sql", execute_sql),
        ],
        edges: [
            ("start", "generate_sql", text2sql::flow::SqlExecutorState::Default),
            ("generate_sql", "execute_sql", text2sql::flow::SqlExecutorState::Default)
        ]
    );
    let context = Context::new();

    let result = flow.run(context).await?;
    println!("result: {:?}", result);

    Ok(())
}
</file>

<file path="examples/text2sql/.gitignore">
ecommerce.duckdb
</file>

<file path="examples/text2sql/Cargo.toml">
[package]
name = "text2sql"
version = "0.1.0"
edition = "2024"

[[bin]]
name = "text2sql"

[dependencies]
duckdb = {version="1.2.2", features = ["bundled"]}
pocketflow_rs = { path = '../..'}
serde_json = "1.0.140"
async-trait = "0.1"
tokio = { version = "1.0", features = ["full"] }
anyhow = "1.0.98"
thiserror = "1.0.69"
openai_api_rust = "0.1.9"
chrono = "0.4.41"
tracing = "0.1.41"
tracing-subscriber = "0.3.19"
</file>

<file path="examples/basic.rs">
use anyhow::Result;
use pocketflow_rs::{Context, Node, ProcessResult, ProcessState, build_flow};
use rand::Rng;
use serde_json::Value;

#[derive(Debug, Clone, PartialEq, Default)]
enum NumberState {
    Small,
    Medium,
    Large,
    #[default]
    Default,
}

impl ProcessState for NumberState {
    fn is_default(&self) -> bool {
        matches!(self, NumberState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            NumberState::Small => "small".to_string(),
            NumberState::Medium => "medium".to_string(),
            NumberState::Large => "large".to_string(),
            NumberState::Default => "default".to_string(),
        }
    }
}

// A simple node that prints a message
struct PrintNode {
    message: String,
}

impl PrintNode {
    fn new(message: &str) -> Self {
        Self {
            message: message.to_string(),
        }
    }
}

#[async_trait::async_trait]
impl Node for PrintNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        println!("PrintNode: {}, Context: {}", self.message, context);
        Ok(Value::String(self.message.clone()))
    }
}

// A node that generates a random number
struct RandomNumberNode {
    max: i64,
}

impl RandomNumberNode {
    fn new(max: i64) -> Self {
        Self { max }
    }
}

#[async_trait::async_trait]
impl Node for RandomNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = rand::thread_rng().gen_range(0..self.max);
        println!(
            "RandomNumberNode: Generated number {}, Context: {}",
            num, context
        );
        Ok(Value::Number(num.into()))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<Value>,
    ) -> Result<ProcessResult<NumberState>> {
        let num = result.as_ref().unwrap().as_i64().unwrap_or(0);
        context.set("number", Value::Number(num.into()));
        // Return different states based on the number
        let state = if num < self.max / 3 {
            NumberState::Small
        } else if num < 2 * self.max / 3 {
            NumberState::Medium
        } else {
            NumberState::Large
        };
        let condition = state.to_condition();
        Ok(ProcessResult::new(state, condition))
    }
}

// A node that processes small numbers
struct SmallNumberNode;

#[async_trait::async_trait]
impl Node for SmallNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = context.get("number").and_then(|v| v.as_i64()).unwrap_or(0);
        println!("SmallNumberNode: Processing small number {}", num);
        Ok(Value::String(format!("Small number processed: {}", num)))
    }
}

// A node that processes medium numbers
struct MediumNumberNode;

#[async_trait::async_trait]
impl Node for MediumNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = context.get("number").and_then(|v| v.as_i64()).unwrap_or(0);
        println!("MediumNumberNode: Processing medium number {}", num);
        Ok(Value::String(format!("Medium number processed: {}", num)))
    }
}

// A node that processes large numbers
struct LargeNumberNode;

#[async_trait::async_trait]
impl Node for LargeNumberNode {
    type State = NumberState;

    async fn execute(&self, context: &Context) -> Result<Value> {
        let num = context.get("number").and_then(|v| v.as_i64()).unwrap_or(0);
        println!("LargeNumberNode: Processing large number {}", num);
        Ok(Value::String(format!("Large number processed: {}", num)))
    }
}

#[tokio::main]
async fn main() -> std::result::Result<(), Box<dyn std::error::Error>> {
    // Create nodes
    let begin_node = PrintNode::new("Begin Node");
    let random_node = RandomNumberNode::new(100);
    let small_node = SmallNumberNode;
    let medium_node = MediumNumberNode;
    let large_node = LargeNumberNode;

    // Create flow using macro
    let flow = build_flow!(
        start: ("start", begin_node),
        nodes: [
            ("rand", random_node),
            ("small", small_node),
            ("medium", medium_node),
            ("large", large_node)
        ],
        edges: [
            ("start", "rand", NumberState::Default),
            ("rand", "small", NumberState::Small),
            ("rand", "medium", NumberState::Medium),
            ("rand", "large", NumberState::Large)
        ]
    );

    // Create context
    let context = Context::new();

    // Run the flow
    println!("Starting flow execution...");
    flow.run(context).await?;
    println!("Flow execution completed!");

    Ok(())
}
</file>

<file path="src/utils/embedding.rs">
#![cfg(feature = "openai")]

use async_trait::async_trait;
use openai_api_rust::embeddings::*;
use openai_api_rust::*;
use tracing::info;

#[derive(Debug, Clone)]
pub struct EmbeddingOptions {
    pub model: String,
    pub dimensions: Option<usize>,
}

impl Default for EmbeddingOptions {
    fn default() -> Self {
        Self {
            model: "text-embedding-ada-002".to_string(),
            dimensions: None,
        }
    }
}

#[async_trait]
pub trait EmbeddingGenerator {
    async fn generate_embedding(&self, text: &str) -> anyhow::Result<Vec<f64>>;
    async fn generate_embeddings(&self, texts: &[String]) -> anyhow::Result<Vec<Vec<f64>>>;
}

#[allow(dead_code)]
pub struct OpenAIEmbeddingGenerator {
    api_key: String,
    options: EmbeddingOptions,
    client: OpenAI,
}

impl OpenAIEmbeddingGenerator {
    pub fn new(api_key: &str, endpoint: &str, options: EmbeddingOptions) -> Self {
        let auth = Auth::new(api_key);
        let client = OpenAI::new(auth, endpoint);
        Self {
            api_key: api_key.to_string(),
            options,
            client,
        }
    }
}

#[async_trait]
impl EmbeddingGenerator for OpenAIEmbeddingGenerator {
    async fn generate_embedding(&self, text: &str) -> anyhow::Result<Vec<f64>> {
        let embeds = self.generate_embeddings(&[text.to_string()]).await?;
        let result: Vec<f64> = embeds[0].to_vec();
        Ok(result)
    }

    async fn generate_embeddings(&self, texts: &[String]) -> anyhow::Result<Vec<Vec<f64>>> {
        // chunked by 10
        let chunks = texts.chunks(10).collect::<Vec<_>>();
        let mut results = Vec::new();
        for chunk in chunks {
            let embedding = EmbeddingsBody {
                model: self.options.model.clone(),
                input: chunk.to_vec(),
                user: None,
            };

            info!("Sending request to OpenAI Embedding API");
            let response = self.client.embeddings_create(&embedding).unwrap();
            let data = response.data.unwrap();
            let result: Vec<Vec<f64>> = data
                .into_iter()
                .map(|x: EmbeddingData| x.embedding.unwrap())
                .collect();
            results.extend(result);
        }
        Ok(results)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::env;

    #[tokio::test]
    #[ignore = "E2E case, requires API keys"]
    async fn test_e2e_embedding_generator() {
        let generator = OpenAIEmbeddingGenerator::new(
            &env::var("DASH_SCOPE_API_KEY").unwrap(),
            "https://dashscope.aliyuncs.com/compatible-mode/v1/",
            EmbeddingOptions {
                model: "text-embedding-v3".to_string(),
                dimensions: Some(64),
            },
        );
        let text = "Hello, world!";
        let embedding = generator.generate_embedding(text).await.unwrap();
        println!("{:?}", embedding);
    }
}
</file>

<file path="src/utils/llm_wrapper.rs">
#![cfg(feature = "openai")]

use std::{collections::HashMap, hash::RandomState};

use async_trait::async_trait;
use openai_api_rust::chat::*;
use openai_api_rust::*;
use serde::{Deserialize, Serialize};
use tracing::info;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMResponse {
    pub content: String,
    pub usage: Option<LLMUsage>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMUsage {
    pub prompt_tokens: Option<u32>,
    pub completion_tokens: Option<u32>,
    pub total_tokens: Option<u32>,
}

#[async_trait]
pub trait LLMWrapper {
    async fn generate(&self, prompt: &str) -> anyhow::Result<LLMResponse>;
    async fn generate_with_options(
        &self,
        prompt: &str,
        options: LLMOptions,
    ) -> anyhow::Result<LLMResponse>;
}

#[derive(Debug, Clone, Default)]
pub struct LLMOptions {
    pub temperature: Option<f32>,
    pub max_tokens: Option<i32>,
    pub top_p: Option<f32>,
    pub frequency_penalty: Option<f32>,
    pub presence_penalty: Option<f32>,
    pub stop: Option<Vec<String>>,
    pub logit_bias: Option<HashMap<String, String, RandomState>>,
}

#[allow(dead_code)]
pub struct OpenAIClient {
    api_key: String,
    model: String,
    endpoint: String,
    client: OpenAI,
}

impl OpenAIClient {
    pub fn new(api_key: String, model: String, endpoint: String) -> Self {
        let auth = Auth::new(&api_key);
        let client = OpenAI::new(auth, &endpoint);
        Self {
            api_key,
            model,
            endpoint,
            client,
        }
    }
}

#[async_trait]
impl LLMWrapper for OpenAIClient {
    async fn generate(&self, prompt: &str) -> anyhow::Result<LLMResponse> {
        self.generate_with_options(prompt, LLMOptions::default())
            .await
    }

    async fn generate_with_options(
        &self,
        prompt: &str,
        options: LLMOptions,
    ) -> anyhow::Result<LLMResponse> {
        let chat = ChatBody {
            model: self.model.clone(),
            temperature: options.temperature,
            max_tokens: options.max_tokens,
            presence_penalty: options.presence_penalty,
            frequency_penalty: options.frequency_penalty,
            logit_bias: options.logit_bias,
            top_p: options.top_p,
            stream: Some(false),
            stop: options.stop,
            user: None,
            n: Some(1),
            messages: vec![Message {
                role: Role::User,
                content: prompt.to_string(),
            }],
        };

        info!("Sending request to OpenAI API");
        let response = self.client.chat_completion_create(&chat).unwrap();
        let choice = response.choices;
        let content = &choice[0].message.as_ref().unwrap().content;
        let u = response.usage;
        let usage = LLMUsage {
            prompt_tokens: u.prompt_tokens,
            completion_tokens: u.completion_tokens,
            total_tokens: u.total_tokens,
        };

        Ok(LLMResponse {
            content: content.clone(),
            usage: Some(usage),
        })
    }
}
</file>

<file path="src/utils/mod.rs">
pub mod embedding;
pub mod llm_wrapper;
pub mod text_chunking;
pub mod vector_db;
pub mod viz_debug;
pub mod web_search;
</file>

<file path="src/utils/text_chunking.rs">
use regex::Regex;
use tracing::info;

#[derive(Debug, Clone)]
pub struct ChunkingOptions {
    pub chunk_size: usize,
    pub overlap: usize,
    pub strategy: ChunkingStrategy,
}

#[derive(Debug, Clone)]
pub enum ChunkingStrategy {
    FixedSize,
    Sentence,
    Paragraph,
}

impl Default for ChunkingOptions {
    fn default() -> Self {
        Self {
            chunk_size: 1000,
            overlap: 100,
            strategy: ChunkingStrategy::FixedSize,
        }
    }
}

pub struct TextChunker {
    sentence_regex: Regex,
    paragraph_regex: Regex,
}

impl Default for TextChunker {
    fn default() -> Self {
        Self::new()
    }
}

impl TextChunker {
    pub fn new() -> Self {
        Self {
            sentence_regex: Regex::new(r"[.!?]+[\s]+").unwrap(),
            paragraph_regex: Regex::new(r"\n\s*\n").unwrap(),
        }
    }

    pub fn chunk_text(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        info!("Chunking text with strategy: {:?}", options.strategy);
        match options.strategy {
            ChunkingStrategy::FixedSize => self.chunk_by_size(text, options),
            ChunkingStrategy::Sentence => self.chunk_by_sentence(text, options),
            ChunkingStrategy::Paragraph => self.chunk_by_paragraph(text, options),
        }
    }

    fn chunk_by_size(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        let mut chunks = Vec::new();
        let mut start = 0;
        let text_size = text.len();

        while start < text_size {
            let end = (start + options.chunk_size).min(text_size);

            // Try to find a good breaking point (space or punctuation)
            let mut actual_end = end;
            if actual_end < text_size {
                while actual_end > start && !text[actual_end..].starts_with(char::is_whitespace) {
                    actual_end -= 1;
                }
                // If we couldn't find a good breaking point, force a break at the chunk size
                if actual_end == start {
                    actual_end = end;
                }
            }

            let chunk = text[start..actual_end].trim().to_string();
            if !chunk.is_empty() {
                chunks.push(chunk);
            }

            // Ensure we always advance by at least 1 character to prevent infinite loop
            let new_start = actual_end.saturating_sub(options.overlap);
            if new_start <= start {
                start = actual_end;
            } else {
                start = new_start;
            }
        }

        chunks
    }

    fn chunk_by_sentence(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();

        for sentence in self.sentence_regex.split(text) {
            let sentence = sentence.trim();
            if sentence.is_empty() {
                continue;
            }

            if current_chunk.len() + sentence.len() < options.chunk_size {
                if !current_chunk.is_empty() {
                    current_chunk.push(' ');
                }
                current_chunk.push_str(sentence);
            } else {
                if !current_chunk.is_empty() {
                    chunks.push(current_chunk);
                }
                current_chunk = sentence.to_string();
            }
        }

        if !current_chunk.is_empty() {
            chunks.push(current_chunk);
        }

        // Add overlap between chunks
        if options.overlap > 0 && chunks.len() > 1 {
            let mut overlapped_chunks = Vec::with_capacity(chunks.len());
            overlapped_chunks.push(chunks[0].clone());

            for i in 1..chunks.len() {
                let prev_chunk = &chunks[i - 1];
                let current_chunk = &chunks[i];

                // Find the last sentence in the previous chunk
                let last_sentences: Vec<&str> = self
                    .sentence_regex
                    .split(prev_chunk)
                    .filter(|s| !s.trim().is_empty())
                    .collect();

                if let Some(last_sentence) = last_sentences.last() {
                    let mut new_chunk = last_sentence.trim().to_string();
                    new_chunk.push(' ');
                    new_chunk.push_str(current_chunk);
                    overlapped_chunks.push(new_chunk);
                } else {
                    overlapped_chunks.push(current_chunk.clone());
                }
            }

            chunks = overlapped_chunks;
        }

        chunks
    }

    fn chunk_by_paragraph(&self, text: &str, options: &ChunkingOptions) -> Vec<String> {
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();

        for paragraph in self.paragraph_regex.split(text) {
            let paragraph = paragraph.trim();
            if paragraph.is_empty() {
                continue;
            }

            if current_chunk.len() + paragraph.len() + 2 <= options.chunk_size {
                if !current_chunk.is_empty() {
                    current_chunk.push_str("\n\n");
                }
                current_chunk.push_str(paragraph);
            } else {
                if !current_chunk.is_empty() {
                    chunks.push(current_chunk);
                }
                current_chunk = paragraph.to_string();
            }
        }

        if !current_chunk.is_empty() {
            chunks.push(current_chunk);
        }

        // Add overlap between chunks
        if options.overlap > 0 && chunks.len() > 1 {
            let mut overlapped_chunks = Vec::with_capacity(chunks.len());
            overlapped_chunks.push(chunks[0].clone());

            for i in 1..chunks.len() {
                let prev_chunk = &chunks[i - 1];
                let current_chunk = &chunks[i];

                // Find the last paragraph in the previous chunk
                let last_paragraphs: Vec<&str> = self
                    .paragraph_regex
                    .split(prev_chunk)
                    .filter(|p| !p.trim().is_empty())
                    .collect();

                if let Some(last_paragraph) = last_paragraphs.last() {
                    let mut new_chunk = last_paragraph.trim().to_string();
                    new_chunk.push_str("\n\n");
                    new_chunk.push_str(current_chunk);
                    overlapped_chunks.push(new_chunk);
                } else {
                    overlapped_chunks.push(current_chunk.clone());
                }
            }

            chunks = overlapped_chunks;
        }

        chunks
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_fixed_size_chunking() {
        let chunker = TextChunker::new();
        let text = "This is a test. This is another test. This is a third test.";
        let options = ChunkingOptions {
            chunk_size: 20,
            overlap: 5,
            strategy: ChunkingStrategy::FixedSize,
        };

        let chunks = chunker.chunk_text(text, &options);
        assert_eq!(chunks.len(), 5);
        for chunk in chunks {
            assert!(chunk.len() <= 20);
        }
    }

    #[test]
    fn test_sentence_chunking() {
        let chunker = TextChunker::new();
        let text = "This is a test. This is another test. This is a third test.";
        let options = ChunkingOptions {
            chunk_size: 30,
            overlap: 10,
            strategy: ChunkingStrategy::Sentence,
        };

        let chunks = chunker.chunk_text(text, &options);
        assert_eq!(chunks.len(), 3);
        assert!(chunks[0].contains("This is a test"));
        assert!(chunks[1].contains("This is another test"));
        assert!(chunks[2].contains("This is a third test"));
    }

    #[test]
    fn test_paragraph_chunking() {
        let chunker = TextChunker::new();
        let text = "This is a test.\n\nThis is another test.\n\nThis is a third test.";
        let options = ChunkingOptions {
            chunk_size: 30,
            overlap: 10,
            strategy: ChunkingStrategy::Paragraph,
        };

        let chunks = chunker.chunk_text(text, &options);
        assert_eq!(chunks.len(), 3);
        assert!(chunks[0].contains("This is a test"));
        assert!(chunks[1].contains("This is another test"));
        assert!(chunks[2].contains("This is a third test"));
    }
}
</file>

<file path="src/utils/vector_db.rs">
#![cfg(feature = "qdrant")]

use async_trait::async_trait;
use qdrant_client::Qdrant;
use qdrant_client::qdrant::{
    CreateCollectionBuilder, DeletePointsBuilder, Distance, PointStruct, ScoredPoint,
    SearchPointsBuilder, UpsertPointsBuilder, VectorParamsBuilder,
};
use qdrant_client::qdrant::{Value as QdrantValue, value::Kind as QdrantKind};

use serde_json::{Map as SerdeMap, Number as SerdeNumber, Value as SerdeValue, json};

use tracing::info;

#[derive(Debug, Clone)]
pub struct VectorDBOptions {
    pub collection_name: String,
    pub dimension: usize,
    pub distance_metric: DistanceMetric,
}

#[derive(Debug, Clone)]
pub enum DistanceMetric {
    Cosine,
    Euclidean,
    DotProduct,
}

#[derive(Debug, Clone)]
pub struct VectorRecord {
    pub id: String,
    pub vector: Vec<f32>,
    pub metadata: serde_json::Map<String, serde_json::Value>,
}

impl VectorRecord {
    pub fn parse_by_value(value: &serde_json::Value) -> Self {
        let id = value.get("id").unwrap().as_str().unwrap().to_string();
        let vector = value
            .get("vector")
            .unwrap()
            .as_array()
            .unwrap()
            .iter()
            .map(|v| v.as_f64().unwrap() as f32)
            .collect();
        let metadata = value.get("metadata").unwrap().as_object().unwrap().clone();
        Self {
            id,
            vector,
            metadata,
        }
    }

    pub fn to_value(&self) -> serde_json::Value {
        json!({
            "id": self.id,
            "vector": self.vector,
            "metadata": self.metadata
        })
    }
}

fn qdrant_value_to_serde_json(q_val: QdrantValue) -> SerdeValue {
    match q_val.kind {
        Some(QdrantKind::NullValue(_)) => SerdeValue::Null,
        Some(QdrantKind::BoolValue(b)) => SerdeValue::Bool(b),
        Some(QdrantKind::DoubleValue(d)) => {
            SerdeNumber::from_f64(d).map_or(SerdeValue::Null, SerdeValue::Number)
        }
        Some(QdrantKind::IntegerValue(i)) => SerdeValue::Number(i.into()),
        Some(QdrantKind::StringValue(s)) => SerdeValue::String(s),
        Some(QdrantKind::ListValue(list_value)) => {
            let serde_list: Vec<SerdeValue> = list_value
                .values
                .into_iter()
                .map(qdrant_value_to_serde_json)
                .collect();
            SerdeValue::Array(serde_list)
        }
        Some(QdrantKind::StructValue(struct_value)) => {
            let mut serde_map = SerdeMap::new();
            for (key, val) in struct_value.fields {
                serde_map.insert(key, qdrant_value_to_serde_json(val));
            }
            SerdeValue::Object(serde_map)
        }
        None => SerdeValue::Null, // Treat absence of kind as Null
    }
}

impl VectorRecord {
    pub fn from_scored_point(point: ScoredPoint) -> Option<Self> {
        let id_str = match point.id {
            Some(point_id) => match point_id.point_id_options {
                Some(qdrant_client::qdrant::point_id::PointIdOptions::Num(n)) => n.to_string(),
                Some(qdrant_client::qdrant::point_id::PointIdOptions::Uuid(s)) => s,
                None => return None,
            },
            None => return None,
        };
        let vector_data = match point.vectors {
            Some(vector) => match vector.vectors_options {
                Some(qdrant_client::qdrant::vectors_output::VectorsOptions::Vector(v)) => v.data,
                _ => return None,
            },
            None => return None,
        };
        // 3. Convert Payload
        let metadata_map: SerdeMap<String, SerdeValue> = point
            .payload
            .into_iter()
            .map(|(key, q_val)| (key, qdrant_value_to_serde_json(q_val)))
            .collect();

        Some(VectorRecord {
            id: id_str,
            vector: vector_data,
            metadata: metadata_map,
        })
    }
}

#[async_trait]
pub trait VectorDB {
    async fn insert(&self, records: Vec<VectorRecord>) -> anyhow::Result<()>;
    async fn search(&self, query: Vec<f32>, k: usize) -> anyhow::Result<Vec<VectorRecord>>;
    async fn delete(&self, ids: Vec<String>) -> anyhow::Result<()>;
}

pub struct QdrantDB {
    client: Qdrant,
    options: VectorDBOptions,
}

impl QdrantDB {
    pub async fn new(
        db_url: String,
        api_key: Option<String>,
        options: VectorDBOptions,
    ) -> anyhow::Result<Self> {
        let client = match api_key {
            Some(api_key) => Qdrant::from_url(db_url.as_str()).api_key(api_key).build()?,
            None => Qdrant::from_url(db_url.as_str()).build()?,
        };

        // Create collection if it doesn't exist
        let collections = client.list_collections().await?;
        if !collections
            .collections
            .iter()
            .any(|c| c.name == options.collection_name)
        {
            let distance = match options.distance_metric {
                DistanceMetric::Cosine => Distance::Cosine,
                DistanceMetric::Euclidean => Distance::Euclid,
                DistanceMetric::DotProduct => Distance::Dot,
            };
            let request = CreateCollectionBuilder::new(options.collection_name.clone())
                .vectors_config(VectorParamsBuilder::new(options.dimension as u64, distance));
            client.create_collection(request).await?;
        }

        Ok(Self { client, options })
    }
}

#[async_trait]
impl VectorDB for QdrantDB {
    async fn insert(&self, records: Vec<VectorRecord>) -> anyhow::Result<()> {
        let points: Vec<PointStruct> = records
            .into_iter()
            .map(|record| PointStruct::new(record.id, record.vector, record.metadata))
            .collect();
        let points_request = UpsertPointsBuilder::new(&self.options.collection_name, points);

        info!("Inserting points into Qdrant");
        self.client.upsert_points(points_request).await?;
        Ok(())
    }

    async fn search(&self, query: Vec<f32>, k: usize) -> anyhow::Result<Vec<VectorRecord>> {
        info!(
            "Searching points in Qdrant, collection: {}",
            self.options.collection_name
        );
        let response = self
            .client
            .search_points(
                SearchPointsBuilder::new(&self.options.collection_name, query, k as u64)
                    .with_payload(true)
                    .with_vectors(true),
            )
            .await?;
        let results = response
            .result
            .into_iter()
            .filter_map(VectorRecord::from_scored_point)
            .collect::<Vec<_>>();
        info!("Retrieved results len: {:?}", results.len());

        Ok(results)
    }

    async fn delete(&self, ids: Vec<String>) -> anyhow::Result<()> {
        info!("Deleting points from Qdrant");
        self.client
            .delete_points(DeletePointsBuilder::new(&self.options.collection_name).points(ids))
            .await?;
        Ok(())
    }
}
</file>

<file path="src/utils/viz_debug.rs">
#![cfg(feature = "debug")]
use std::fmt::Debug;

pub trait DebugVisualizer {
    fn visualize<T: Debug>(&self, data: &T) -> String;
    fn visualize_flow(&self, flow_data: &[u8]) -> String;
}

pub struct ConsoleDebugVisualizer;

impl DebugVisualizer for ConsoleDebugVisualizer {
    fn visualize<T: Debug>(&self, data: &T) -> String {
        format!("{:?}", data)
    }

    #[allow(unused_variables)]
    fn visualize_flow(&self, flow_data: &[u8]) -> String {
        // TODO: Implement flow visualization
        "Flow visualization not implemented".to_string()
    }
}

pub struct GraphDebugVisualizer;

impl DebugVisualizer for GraphDebugVisualizer {
    fn visualize<T: Debug>(&self, data: &T) -> String {
        // TODO: Implement graph visualization
        format!("Graph visualization of {:?}", data)
    }

    #[allow(unused_variables)]
    fn visualize_flow(&self, flow_data: &[u8]) -> String {
        // TODO: Implement flow graph visualization
        "Flow graph visualization not implemented".to_string()
    }
}
</file>

<file path="src/utils/web_search.rs">
#![cfg(feature = "websearch")]

use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use tracing::info;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    pub title: String,
    pub url: String,
    pub snippet: String,
}

#[async_trait]
pub trait WebSearcher {
    async fn search(&self, query: &str) -> anyhow::Result<Vec<SearchResult>>;
    async fn search_with_options(
        &self,
        query: &str,
        options: SearchOptions,
    ) -> anyhow::Result<Vec<SearchResult>>;
}

#[derive(Debug, Clone, Default)]
pub struct SearchOptions {
    pub max_results: Option<usize>,
    pub language: Option<String>,
    pub region: Option<String>,
}

pub struct GoogleSearcher {
    api_key: String,
    search_engine_id: String,
    client: Client,
}

impl GoogleSearcher {
    pub fn new(api_key: String, search_engine_id: String) -> Self {
        Self {
            api_key,
            search_engine_id,
            client: Client::new(),
        }
    }
}

#[async_trait]
impl WebSearcher for GoogleSearcher {
    async fn search(&self, query: &str) -> anyhow::Result<Vec<SearchResult>> {
        self.search_with_options(query, SearchOptions::default())
            .await
    }

    async fn search_with_options(
        &self,
        query: &str,
        options: SearchOptions,
    ) -> anyhow::Result<Vec<SearchResult>> {
        let mut url = format!(
            "https://www.googleapis.com/customsearch/v1?key={}&cx={}&q={}",
            self.api_key, self.search_engine_id, query
        );

        if let Some(lang) = options.language {
            url.push_str(&format!("&lr=lang_{}", lang));
        }
        if let Some(region) = options.region {
            url.push_str(&format!("&cr=country{}", region));
        }
        if let Some(max_results) = options.max_results {
            url.push_str(&format!("&num={}", max_results));
        }

        info!("Sending request to Google Search API");
        let response = self.client.get(&url).send().await?;
        let search_response: serde_json::Value = response.json().await?;
        let default_val: Vec<serde_json::Value> = vec![];
        let items = search_response["items"].as_array().unwrap_or(&default_val);
        let results = items
            .iter()
            .map(|item| SearchResult {
                title: item["title"].as_str().unwrap_or("").to_string(),
                url: item["link"].as_str().unwrap_or("").to_string(),
                snippet: item["snippet"].as_str().unwrap_or("").to_string(),
            })
            .collect();

        Ok(results)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::env;

    #[tokio::test]
    #[ignore = "E2E case, requires API keys"]
    async fn test_e2e_google_searcher() {
        let searcher = GoogleSearcher::new(
            env::var("GOOGLE_API_KEY").unwrap(),
            env::var("GOOGLE_SEARCH_ENGINE_ID").unwrap(),
        );
        let results = searcher
            .search("Beijing's temperature today")
            .await
            .unwrap();
        println!("{:?}", results);
    }
}
</file>

<file path="src/context.rs">
use serde_json::Value;
use std::collections::HashMap;
use std::fmt;

#[derive(Debug, Clone, Default)]
pub struct Context {
    data: HashMap<String, Value>,
    metadata: HashMap<String, Value>,
}

impl Context {
    pub fn new() -> Self {
        Self {
            data: HashMap::new(),
            metadata: HashMap::new(),
        }
    }

    pub fn from_data(data: HashMap<String, Value>) -> Self {
        Self {
            data,
            metadata: HashMap::new(),
        }
    }

    pub fn get(&self, key: &str) -> Option<&Value> {
        self.data.get(key)
    }

    pub fn get_metadata(&self, key: &str) -> Option<&Value> {
        self.metadata.get(key)
    }

    pub fn set(&mut self, key: &str, value: Value) {
        self.data.insert(key.to_string(), value);
    }

    pub fn set_metadata(&mut self, key: &str, value: Value) {
        self.metadata.insert(key.to_string(), value);
    }

    pub fn remove(&mut self, key: &str) -> Option<Value> {
        self.data.remove(key)
    }

    pub fn remove_metadata(&mut self, key: &str) -> Option<Value> {
        self.metadata.remove(key)
    }

    pub fn get_all_data(&self) -> &HashMap<String, Value> {
        &self.data
    }

    pub fn get_all_metadata(&self) -> &HashMap<String, Value> {
        &self.metadata
    }

    pub fn merge(&mut self, other: &Context) {
        for (key, value) in &other.data {
            self.data.insert(key.clone(), value.clone());
        }
        for (key, value) in &other.metadata {
            self.metadata.insert(key.clone(), value.clone());
        }
    }

    pub fn clear(&mut self) {
        self.data.clear();
        self.metadata.clear();
    }

    pub fn contains_key(&self, key: &str) -> bool {
        self.data.contains_key(key)
    }

    pub fn contains_metadata_key(&self, key: &str) -> bool {
        self.metadata.contains_key(key)
    }
}

impl fmt::Display for Context {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        writeln!(f, "Context {{")?;

        // Display data
        writeln!(f, "  data: {{")?;
        for (key, value) in &self.data {
            writeln!(f, "    \"{}\": {},", key, value)?;
        }
        writeln!(f, "  }},")?;

        // Display metadata
        writeln!(f, "  metadata: {{")?;
        for (key, value) in &self.metadata {
            writeln!(f, "    \"{}\": {},", key, value)?;
        }
        writeln!(f, "  }}")?;

        write!(f, "}}")
    }
}

impl From<HashMap<String, Value>> for Context {
    fn from(data: HashMap<String, Value>) -> Self {
        Self::from_data(data)
    }
}
</file>

<file path="src/flow.rs">
use crate::{
    context::Context,
    node::{Node, ProcessState},
};
use anyhow::Result;
use serde_json::Value;
use std::collections::HashMap;
use std::sync::Arc;
use tracing::info;

pub struct Flow<S: ProcessState + Default> {
    nodes: HashMap<String, Arc<dyn Node<State = S>>>,
    edges: HashMap<String, Vec<(String, String)>>, // (to_node, condition)
    start_node: String,
}

impl<S: ProcessState + Default> Flow<S> {
    pub fn new(start_node_name: &str, start_node: Arc<dyn Node<State = S>>) -> Self {
        let mut nodes = HashMap::new();
        nodes.insert(start_node_name.to_string(), start_node);

        Self {
            nodes,
            edges: HashMap::new(),
            start_node: start_node_name.to_string(),
        }
    }

    pub fn add_node(&mut self, name: &str, node: Arc<dyn Node<State = S>>) {
        self.nodes.insert(name.to_string(), node);
    }

    pub fn add_edge(&mut self, from: &str, to: &str, condition: S) {
        self.edges
            .entry(from.to_string())
            .or_default()
            .push((to.to_string(), condition.to_condition()));
    }

    pub async fn run(&self, mut context: Context) -> Result<Value> {
        let mut current_node = self.start_node.clone();

        while let Some(node) = self.nodes.get(&current_node) {
            // Prepare
            info!("Preparing node: {}", current_node);
            node.prepare(&mut context).await?;

            // Execute
            info!("Executing node: {}", current_node);
            let result = node.execute(&context).await;

            // Post process
            info!("Post processing node: {}", current_node);
            let process_result = node.post_process(&mut context, &result).await?;

            // Find next node based on the state returned by post_process
            if let Some(edges) = self.edges.get(&current_node) {
                // Get the condition from the node state
                let condition = process_result.state.to_condition();

                // Try to find an edge matching the condition
                let next_node_info = edges
                    .iter()
                    .find(|(_, edge_condition)| edge_condition == &condition);

                if let Some((next, _)) = next_node_info {
                    current_node = next.clone();
                } else {
                    // If no matching edge found, try the default condition
                    let default_edge = edges
                        .iter()
                        .find(|(_, edge_condition)| edge_condition == "default");

                    if let Some((next, _)) = default_edge {
                        current_node = next.clone();
                    } else {
                        info!(
                            "No edge found for node '{}' with condition '{}'. Stopping flow.",
                            current_node, condition
                        );
                        break;
                    }
                }
            } else {
                info!(
                    "Node '{}' has no outgoing edges. Stopping flow.",
                    current_node
                );
                break;
            }
        }

        Ok(context.get("result").unwrap_or(&Value::Null).clone())
    }
}

#[allow(dead_code)]
pub struct BatchFlow<S: ProcessState + Default> {
    flow: Flow<S>,
    batch_size: usize,
}

impl<S: ProcessState + Default> BatchFlow<S> {
    pub fn new(
        start_node_name: &str,
        start_node: Arc<dyn Node<State = S>>,
        batch_size: usize,
    ) -> Self {
        Self {
            flow: Flow::new(start_node_name, start_node),
            batch_size,
        }
    }

    pub async fn run_batch(&self, contexts: Vec<Context>) -> Result<()> {
        info!(
            "Starting batch flow execution with {} items",
            contexts.len()
        );

        for context in contexts {
            self.flow.run(context).await?;
        }

        info!("Batch flow execution completed");
        Ok(())
    }
}

#[macro_export]
macro_rules! build_flow {
    (start: ($name: expr, $node:expr)) => {{
        $crate::flow::Flow::new($name, std::sync::Arc::new($node))
    }};

    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?]
    ) => {{
        let mut g = $crate::flow::Flow::new($start_name, std::sync::Arc::new($start_node));
        $(
            g.add_node($name, std::sync::Arc::new($node));
        )*
        g
    }};

    // Complete version with proper-edge handling
    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?],
        edges: [
            $($edge:tt),* $(,)?
        ]
    ) => {{
        let mut g = $crate::flow::Flow::new($start_name, std::sync::Arc::new($start_node));
        // Add all nodes first
        $(
            g.add_node($name, std::sync::Arc::new($node));
        )*
        // Handle edges appropriately
        $(
            build_flow!(@edge g, $edge);
        )*
        g
    }};


    (@edge $g:expr, ($from:expr, $to:expr, $condition:expr)) => {
        $g.add_edge($from, $to, $condition);
    };
}

#[macro_export]
macro_rules! build_batch_flow {
    (start: ($name: expr, $node:expr), batch_size: $batch_size:expr) => {{
        BatchFlow::new($name, std::sync::Arc::new($node), $batch_size)
    }};

    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?],
        batch_size: $batch_size:expr
    ) => {{
        let mut g = BatchFlow::new($start_name, std::sync::Arc::new($start_node), $batch_size);
        $(
            g.flow.add_node($name, std::sync::Arc::new($node));
        )*
        g
    }};

    // Complete version with proper-edge handling
    (
        start: ($start_name:expr, $start_node:expr),
        nodes: [$(($name:expr, $node:expr)),* $(,)?],
        edges: [
            $($edge:tt),* $(,)?
        ],
        batch_size: $batch_size:expr
    ) => {{
        let mut g = BatchFlow::new($start_name, std::sync::Arc::new($start_node), $batch_size);
        // Add all nodes first
        $(
            g.flow.add_node($name, std::sync::Arc::new($node));
        )*
        // Handle edges appropriately
        $(
            build_flow!(@edge g.flow, $edge);
        )*
        g
    }};
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::node::{Node, ProcessResult, ProcessState};
    use async_trait::async_trait;
    use serde_json::json;

    #[derive(Debug, Clone, PartialEq)]
    #[allow(dead_code)]
    #[derive(Default)]
    enum CustomState {
        Success,
        Failure,
        #[default]
        Default,
    }

    impl ProcessState for CustomState {
        fn is_default(&self) -> bool {
            matches!(self, CustomState::Default)
        }

        fn to_condition(&self) -> String {
            match self {
                CustomState::Success => "success".to_string(),
                CustomState::Failure => "failure".to_string(),
                CustomState::Default => "default".to_string(),
            }
        }
    }

    struct TestNode {
        result: Value,
        state: CustomState,
    }

    impl TestNode {
        fn new(result: Value, state: CustomState) -> Self {
            Self { result, state }
        }
    }

    #[async_trait]
    impl Node for TestNode {
        type State = CustomState;

        async fn execute(&self, _context: &Context) -> Result<Value> {
            Ok(self.result.clone())
        }

        async fn post_process(
            &self,
            context: &mut Context,
            result: &Result<Value>,
        ) -> Result<ProcessResult<CustomState>> {
            match result {
                Ok(value) => {
                    context.set("result", value.clone());
                    Ok(ProcessResult::new(self.state.clone(), "test".to_string()))
                }
                Err(e) => {
                    context.set("error", json!(e.to_string()));
                    Ok(ProcessResult::new(CustomState::Default, e.to_string()))
                }
            }
        }
    }

    #[tokio::test]
    async fn test_flow_with_custom_state() {
        let node1 = Arc::new(TestNode::new(
            json!({"data": "test1"}),
            CustomState::Success,
        ));
        let node2 = Arc::new(TestNode::new(
            json!({"data": "test2"}),
            CustomState::Default,
        ));
        let end_node = Arc::new(TestNode::new(
            json!({"final_result": "finished"}),
            CustomState::Default,
        ));

        let mut flow = Flow::<CustomState>::new("start", node1);
        flow.add_node("next", node2);
        flow.add_node("end", end_node);

        flow.add_edge("start", "next", CustomState::Success);
        flow.add_edge("next", "end", CustomState::Default);

        let context = Context::new();
        let result = flow.run(context).await.unwrap();

        assert_eq!(result, json!({"final_result": "finished"}));
    }

    #[tokio::test]
    async fn test_batch_flow() {
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let node2 = TestNode::new(json!({"data": "test2"}), CustomState::Default);

        let mut batch_flow = BatchFlow::<CustomState>::new("start", Arc::new(node1), 10);
        batch_flow.flow.add_node("next", Arc::new(node2));
        batch_flow
            .flow
            .add_edge("start", "next", CustomState::Success);
        batch_flow
            .flow
            .add_edge("next", "end", CustomState::Default);

        let contexts = vec![Context::new(), Context::new()];
        batch_flow.run_batch(contexts).await.unwrap();
    }

    #[tokio::test]
    async fn test_build_flow_macro() {
        // Test basic flow with start node only
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let flow1 = build_flow!(
            start: ("start", node1)
        );
        let context = Context::new();
        let result = flow1.run(context).await.unwrap();
        assert_eq!(result, json!({"data": "test1"}));

        // Test flow with multiple nodes
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let node2 = TestNode::new(json!({"data": "test2"}), CustomState::Default);
        let end_node = TestNode::new(json!({"final_result": "finished"}), CustomState::Default);
        let flow2 = build_flow!(
            start: ("start", node1),
            nodes: [("next", node2), ("end", end_node)],
            edges: [
                ("start", "next", CustomState::Success),
                ("next", "end", CustomState::Default)
            ]
        );
        let context = Context::new();
        let result = flow2.run(context).await.unwrap();
        assert_eq!(result, json!({"final_result": "finished"}));

        // Test flow with default edges
        let node1 = TestNode::new(json!({"data": "test1"}), CustomState::Success);
        let node2 = TestNode::new(json!({"data": "test2"}), CustomState::Default);
        let flow3 = build_flow!(
            start: ("start", node1),
            nodes: [("next", node2)],
            edges: [
                ("start", "next", CustomState::Default)
            ]
        );
        let context = Context::new();
        let result = flow3.run(context).await.unwrap();
        assert_eq!(result, json!({"data": "test2"}));
    }
}
</file>

<file path="src/lib.rs">
pub mod context;
pub mod flow;
pub mod node;
pub mod utils;

pub use context::Context;
pub use flow::*;
pub use node::*;
pub use utils::*;

pub type Params = std::collections::HashMap<String, serde_json::Value>;
</file>

<file path="src/node.rs">
use crate::{Params, context::Context};
use anyhow::Result;
use async_trait::async_trait;
use std::collections::HashMap;
use std::sync::Arc;

pub trait ProcessState: Send + Sync {
    fn is_default(&self) -> bool;
    fn to_condition(&self) -> String;
}

#[derive(Debug, Clone, PartialEq, Default)]
pub enum BaseState {
    Success,
    Failure,
    #[default]
    Default,
}

impl ProcessState for BaseState {
    fn is_default(&self) -> bool {
        matches!(self, BaseState::Default)
    }

    fn to_condition(&self) -> String {
        match self {
            BaseState::Success => "success".to_string(),
            BaseState::Failure => "failure".to_string(),
            BaseState::Default => "default".to_string(),
        }
    }
}

#[derive(Debug, Clone, PartialEq)]
pub struct ProcessResult<S: ProcessState> {
    pub state: S,
    pub message: String,
}

impl<S: ProcessState> ProcessResult<S> {
    pub fn new(state: S, message: String) -> Self {
        Self { state, message }
    }
}

impl<S: ProcessState + Default> Default for ProcessResult<S> {
    fn default() -> Self {
        Self {
            state: S::default(),
            message: "default".to_string(),
        }
    }
}

#[async_trait]
pub trait Node: Send + Sync {
    type State: ProcessState + Default;

    #[allow(unused_variables)]
    async fn prepare(&self, context: &mut Context) -> Result<()> {
        Ok(())
    }

    async fn execute(&self, context: &Context) -> Result<serde_json::Value>;

    #[allow(unused_variables)]
    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<serde_json::Value>,
    ) -> Result<ProcessResult<Self::State>> {
        match result {
            Ok(value) => {
                context.set("result", value.clone());
                Ok(ProcessResult::default())
            }
            Err(e) => {
                context.set("error", serde_json::Value::String(e.to_string()));
                Ok(ProcessResult::new(Self::State::default(), e.to_string()))
            }
        }
    }
}

pub trait BaseNodeTrait: Node<State = BaseState> {}

#[allow(dead_code)]
pub struct BaseNode {
    params: Params,
    next_nodes: HashMap<String, Arc<dyn BaseNodeTrait>>,
}

impl BaseNode {
    pub fn new(params: Params) -> Self {
        Self {
            params,
            next_nodes: HashMap::new(),
        }
    }

    pub fn add_next(&mut self, action: String, node: Arc<dyn BaseNodeTrait>) {
        self.next_nodes.insert(action, node);
    }
}

#[async_trait]
impl Node for BaseNode {
    type State = BaseState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<serde_json::Value> {
        Ok(serde_json::Value::Null)
    }
}

impl BaseNodeTrait for BaseNode {}

#[allow(dead_code)]
pub struct BatchNode {
    base: BaseNode,
    batch_size: usize,
}

impl BatchNode {
    pub fn new(params: Params, batch_size: usize) -> Self {
        Self {
            base: BaseNode::new(params),
            batch_size,
        }
    }
}

#[async_trait]
impl Node for BatchNode {
    type State = BaseState;

    #[allow(unused_variables)]
    async fn execute(&self, context: &Context) -> Result<serde_json::Value> {
        Ok(serde_json::Value::Null)
    }
}

impl BaseNodeTrait for BatchNode {}
</file>

<file path=".gitignore">
/target
.idea/

target
.env

Cargo.lock
.vscode/
</file>

<file path="Cargo.toml">
[package]
name = "pocketflow_rs"
version = "0.1.0"
edition = "2024"
description = "PocketFlow implemented by rust"
authors = ["Yan Lu <luyanfcp@gmail.com>"]
license = "MIT"

[lib]
name = "pocketflow_rs"
path = "src/lib.rs"

[[example]]
name = "basic"
path = "examples/basic.rs"

[workspace]
members = [ 
    "examples/pocketflow-rs-rag",
    "examples/text2sql"
]

[dependencies]
anyhow = "1.0"
async-trait = "0.1"
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
thiserror = "1.0"
tracing = "0.1"
rand = "0.8"
openai_api_rust = { version = "0.1.9", optional = true}
regex = "1.11.1"
qdrant-client = {version = "1.14.0", optional = true}
reqwest = { version = "0.12", features = ["json"], optional = true }

[features]
openai = ["dep:openai_api_rust"]
websearch = ["dep:reqwest"]
qdrant = ["dep:qdrant-client"]
debug = []
default = [
    "openai",
]
</file>

<file path="README.md">
<div align="center">
  <img src="./static/pocketflow_rust_title.png" alt="Pocket Flow – 100-line minimalist LLM framework" width="400"/>
</div>

A Rust implementation of [PocketFlow](https://github.com/The-Pocket/PocketFlow), a minimalist flow-based programming framework.

📋 [Get started quickly with our template →](#template)

## Features

- Type-safe state transitions using enums
- Macro-based flow construction
- Async node execution and post-processing
- Batch flow support
- Custom state management
- Extensible node system

## Quick Start

### 0. Setup

```bash
cargo add pocketflow_rs
```

### 1. Define Custom States

```rust
use pocketflow_rs::ProcessState;

#[derive(Debug, Clone, PartialEq)]
pub enum MyState {
    Success,
    Failure,
    Default,
}

impl ProcessState for MyState {
    fn is_default(&self) -> bool {
        matches!(self, MyState::Default)
    }
    fn to_condition(&self) -> String {
        match self {
            MyState::Success => "success".to_string(),
            MyState::Failure => "failure".to_string(),
            MyState::Default => "default".to_string(),
        }
    }
}

impl Default for MyState {
    fn default() -> Self {
        MyState::Default
    }
}
```

### 2. Implement Nodes

```rust
use pocketflow_rs::{Node, ProcessResult, Context};
use anyhow::Result;
use async_trait::async_trait;

struct MyNode;

#[async_trait]
impl Node for MyNode {
    type State = MyState;

    async fn execute(&self, context: &Context) -> Result<serde_json::Value> {
        // Your node logic here
        Ok(serde_json::json!({"data": 42}))
    }

    async fn post_process(
        &self,
        context: &mut Context,
        result: &Result<serde_json::Value>,
    ) -> Result<ProcessResult<MyState>> {
        // Your post-processing logic here
        Ok(ProcessResult::new(MyState::Success, "success".to_string()))
    }
}
```

### 3. Build Flows

```rust
use pocketflow_rs::{build_flow, Context};

let node1 = MyNode;
let node2 = MyNode;

let flow = build_flow!(
    start: ("start", node1),
    nodes: [("next", node2)],
    edges: [
        ("start", "next", MyState::Success)
    ]
);

let context = Context::new();
let result = flow.run(context).await?;
```

### 4. Batch Processing

```rust
use pocketflow_rs::build_batch_flow;

let batch_flow = build_batch_flow!(
    start: ("start", node1),
    nodes: [("next", node2)],
    edges: [
        ("start", "next", MyState::Success)
    ],
    batch_size: 10
);

let contexts = vec![Context::new(); 10];
batch_flow.run_batch(contexts).await?;
```

## Advanced Usage

### Custom State Management

Define your own states to control flow transitions:

```rust
#[derive(Debug, Clone, PartialEq)]
pub enum WorkflowState {
    Initialized,
    Processing,
    Completed,
    Error,
    Default,
}

impl ProcessState for WorkflowState {
    fn is_default(&self) -> bool {
        matches!(self, WorkflowState::Default)
    }
    fn to_condition(&self) -> String {
        match self {
            WorkflowState::Initialized => "initialized".to_string(),
            WorkflowState::Processing => "processing".to_string(),
            WorkflowState::Completed => "completed".to_string(),
            WorkflowState::Error => "error".to_string(),
            WorkflowState::Default => "default".to_string(),
        }
    }
}
```

### Complex Flow Construction

Build complex workflows with multiple nodes and state transitions:

```rust
let flow = build_flow!(
    start: ("start", node1),
    nodes: [
        ("process", node2),
        ("validate", node3),
        ("complete", node4)
    ],
    edges: [
        ("start", "process", WorkflowState::Initialized),
        ("process", "validate", WorkflowState::Processing),
        ("validate", "process", WorkflowState::Error),
        ("validate", "complete", WorkflowState::Completed)
    ]
);
```

## Available Features

The following features are available: (feature for [utility_function](https://the-pocket.github.io/PocketFlow/utility_function/))

- `openai` (default): Enable OpenAI API integration for LLM capabilities
- `websearch`: Enable web search functionality using Google Custom Search API
- `qdrant`: Enable vector database integration using Qdrant
- `debug`: Enable additional debug logging and information

To use specific features, add them to your `Cargo.toml`:

```toml
[dependencies]
pocketflow_rs = { version = "0.1.0", features = ["openai", "websearch"] }
```

Or use them in the command line:

```bash
cargo add pocketflow_rs --features "openai websearch"
```

## Examples

Check out the `examples/` directory for more detailed examples:

- basic.rs: Basic flow with custom states
- text2sql: Text-to-SQL workflow example
- [pocketflow-rs-rag](./examples/pocketflow-rs-rag/README.md): Retrieval-Augmented Generation (RAG) workflow example

## Template

Fork the [PocketFlow-Template-Rust](https://github.com/The-Pocket/PocketFlow-Template-Rust) repository and use it as a template for your own project.

## License

MIT
</file>

</files>
