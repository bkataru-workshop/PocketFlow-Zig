This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/*.csv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  ISSUE_TEMPLATE/
    bug_report.md
    feature_request.md
  workflows/
    ci.yml
    code-quality.yml
  PULL_REQUEST_TEMPLATE.md
cookbook/
  pocketflow-batch-flow/
    src/
      flow.ts
      main.ts
      nodes.ts
      type.ts
    package.json
    README.md
  pocketflow-batch-node/
    src/
      flow.ts
      main.ts
      nodes.ts
      types.ts
    package.json
    README.md
  pocketflow-hello-world/
    hello-world.ts
    package.json
    README.md
  pocketflow-parallel-batch-flow/
    output/
      report.txt
    src/
      flows/
        image_processing_flow.ts
      nodes/
        completion_report_node.ts
        image_processing_node.ts
        image_scanner_node.ts
        index.ts
      utils/
        index.ts
        process_image.ts
        read_directory.ts
      index.ts
      types.ts
    design.md
    package.json
    README.md
docs/
  core_abstraction/
    batch.md
    communication.md
    flow.md
    index.md
    node.md
    parallel.md
  design_pattern/
    agent.md
    index.md
    mapreduce.md
    multi_agent.md
    rag.md
    structure.md
    workflow.md
  utility_function/
    chunking.md
    embedding.md
    index.md
    llm.md
    text_to_speech.md
    vector.md
    viz.md
    websearch.md
  _config.yml
  guide.md
  index.md
src/
  index.ts
tests/
  batch-flow.test.ts
  batch-node.test.ts
  core-abstraction-examples.test.ts
  fallback.test.ts
  flow-basic.test.ts
  flow-composition.test.ts
  mapreduce-pattern.test.ts
  multi-agent-pattern.test.ts
  parallel-batch-flow.test.ts
  parallel-batch-node.test.ts
  qa-pattern.test.ts
  rag-pattern.test.ts
.cursorrules
.gitignore
.npmignore
CODE_OF_CONDUCT.md
CONTRIBUTING.md
jest.config.js
LICENSE
package.json
README.md
tsconfig.json
tsup.config.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/ISSUE_TEMPLATE/bug_report.md">
---
name: Bug Report
about: Create a report to help us improve
title: "[BUG] "
labels: bug
assignees: ""
---

## Bug Description

A clear and concise description of the bug.

## Steps To Reproduce

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

## Expected Behavior

A clear and concise description of what you expected to happen.

## Actual Behavior

A clear and concise description of what actually happened.

## Screenshots

If applicable, add screenshots to help explain your problem.

## Environment

- OS: [e.g. Windows 10, macOS 12.3, Ubuntu 22.04]
- Node.js version: [e.g. 16.14.2]
- npm/yarn version: [e.g. npm 8.5.0]
- PocketFlow version: [e.g. 1.0.0]

## Additional Context

Add any other context about the problem here.
</file>

<file path=".github/ISSUE_TEMPLATE/feature_request.md">
---
name: Feature Request
about: Suggest an idea for this project
title: "[FEATURE] "
labels: enhancement
assignees: ""
---

## Problem Statement

A clear and concise description of the problem you're experiencing or the use case this feature would address. Ex. I'm always frustrated when [...]

## Proposed Solution

A clear and concise description of what you want to happen.

## Alternative Solutions

A clear and concise description of any alternative solutions or features you've considered.

## Example Implementation

If possible, provide a sketch, mockup, or example of how this feature might work.

## Additional Context

Add any other context, references, or screenshots about the feature request here.
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  test-and-build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18.x, 20.x]

    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: Run tests
        run: npm test

      - name: Build
        run: npm run build

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-${{ matrix.node-version }}
          path: dist/
</file>

<file path=".github/workflows/code-quality.yml">
name: Code Quality

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

jobs:
  typescript-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Use Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18.x"
          cache: "npm"

      - name: Install dependencies
        run: npm ci

      - name: TypeScript compilation check
        run: npx tsc --noEmit
</file>

<file path=".github/PULL_REQUEST_TEMPLATE.md">
## Description

<!-- Provide a brief description of the changes made in this PR -->

## Related Issue

<!-- Link to the related issue(s) that this PR addresses -->
<!-- Use syntax: "Fixes #123" or "Resolves #123" to automatically close the issue when the PR is merged -->

## Type of Change

<!-- Please delete options that are not relevant -->

- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation update
- [ ] Performance improvement
- [ ] Code cleanup or refactor

## How Has This Been Tested?

<!-- Please describe the tests that you ran to verify your changes -->
<!-- Include relevant details for your test configuration -->

## Checklist

<!-- Please check all that apply -->

- [ ] My code follows the code style of this project
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] I have updated the documentation accordingly
- [ ] My changes generate no new warnings
- [ ] Any dependent changes have been merged and published

## Screenshots (if appropriate)

<!-- Add screenshots to help explain your changes if applicable -->

## Additional Notes

<!-- Add any other information about the PR here -->
</file>

<file path="cookbook/pocketflow-batch-flow/src/flow.ts">
import { Flow, BatchFlow } from 'pocketflow';
import { loadImageNode, applyFilterNode, saveImageNode } from './nodes';
import { SharedData, FilterType } from './type';

// Types for the image processing parameters
export type ImageProcessingParams = {
  imageName: string;
  filterType: FilterType;
};

// Connect nodes to create the processing pipeline
// Each node passes its output to the next node as input
loadImageNode.on("apply_filter", applyFilterNode);
applyFilterNode.on("save", saveImageNode);

// Create the base flow for processing a single image with a specific filter
export const imageProcessingFlow = new Flow<SharedData>(loadImageNode);

// Create the batch flow for processing multiple images with different filters
export const batchImageProcessingFlow = new BatchFlow<SharedData>(loadImageNode);

// Set batch parameters in main.ts before running
</file>

<file path="cookbook/pocketflow-batch-flow/src/main.ts">
import fs from 'fs';
import path from 'path';
import { batchImageProcessingFlow, ImageProcessingParams } from './flow';
import { SharedData } from './type';

async function main() {
  // Get all image files from the images directory
  const imageDir = path.join(__dirname, 'images');
  const imageFiles = fs.readdirSync(imageDir)
    .filter(file => /\.(jpg|jpeg|png)$/i.test(file));

  console.log('PocketFlow BatchFlow Example - Image Processor');
  console.log('==============================================');
  
  // Define the filter types to use
  const filterTypes: Array<'grayscale' | 'blur' | 'sepia'> = ['grayscale', 'blur', 'sepia'];
  
  // Create a list of parameters for batch processing
  const batchParams: ImageProcessingParams[] = [];
  
  // Create combinations of images and filters
  for (const image of imageFiles) {
    for (const filter of filterTypes) {
      batchParams.push({
        imageName: image,
        filterType: filter
      });
    }
  }
  
  console.log(`\nProcessing ${imageFiles.length} images with ${filterTypes.length} filters...`);
  console.log(`Total operations: ${batchParams.length}\n`);
  
  try {
    // Define the prep method for the BatchFlow to return our batch parameters
    const originalPrep = batchImageProcessingFlow.prep;
    batchImageProcessingFlow.prep = async () => {
      return batchParams;
    };

    // Create shared context - can be used to track progress or share data between runs
    const sharedContext: SharedData = {};

    // Execute the batch flow with the shared context
    console.time('Batch processing time');
    await batchImageProcessingFlow.run(sharedContext);
    console.timeEnd('Batch processing time');
    
    // Restore original prep method
    batchImageProcessingFlow.prep = originalPrep;
    
    console.log('\nAll images processed successfully!');
    console.log('Check the \'output\' directory for results.');
    
    // Output summary of results
    console.log('\nProcessed images:');
    batchParams.forEach((params) => {
      console.log(`- ${params.imageName} with ${params.filterType} filter`);
    });
  } catch (error) {
    console.error('Error processing images:', error);
  }
}

// Run the main function
main().catch(console.error);
</file>

<file path="cookbook/pocketflow-batch-flow/src/nodes.ts">
import fs from 'fs';
import path from 'path';
import { Node } from 'pocketflow';
import sharp from 'sharp';
import { FilterType, SharedData } from './type';

// Create output directory if it doesn't exist
if (!fs.existsSync(path.join(__dirname, '../output'))) {
  fs.mkdirSync(path.join(__dirname, '../output'));
}

// Define interfaces for node parameters and results that satisfy NonIterableObject constraint
type LoadImageParams ={
  imageName: string;
}

type LoadImageResult = {
  imageBuffer: Buffer;
}

type ApplyFilterParams = {
  filterType: FilterType;
}

type SaveImageResult = {
  outputPath: string;
}

// Node to load an image
export class LoadImageNode extends Node<SharedData, LoadImageParams> {

  async prep(): Promise<string> {
    const imageName = this._params.imageName;
    return imageName;
  }

  async exec(prepRes: string): Promise<LoadImageResult> {
    console.log(`Loading image: ${prepRes}...`);
    const imagePath = path.join(__dirname, 'images', prepRes);
    const imageBuffer = fs.readFileSync(imagePath);
    
    return { imageBuffer };
  }

  async post(shared: SharedData, prepRes: string, execRes: LoadImageResult): Promise<string> {
    shared.imageBuffer = execRes.imageBuffer;
    shared.imageName = prepRes;
    return "apply_filter";
  }
}

// Node to apply a filter to an image
export class ApplyFilterNode extends Node<SharedData, ApplyFilterParams> {

  async prep(shared: SharedData): Promise<{
    filterType: FilterType;
    imageBuffer: Buffer;
  }> {
    const filterType = this._params.filterType;
    return {
      filterType,
      imageBuffer: shared.imageBuffer || Buffer.from([])
    }
  }

  async exec(prepRes: {
    filterType: FilterType;
    imageBuffer: Buffer;
  }): Promise<Buffer> {
    const filterType = prepRes.filterType;
    
    console.log(`Processing image with ${filterType} filter...`);
    let processedImage;
    switch (filterType) {
      case 'grayscale':
        processedImage = await sharp(prepRes.imageBuffer).grayscale().toBuffer();
        break;
      case 'blur':
        processedImage = await sharp(prepRes.imageBuffer).blur(10).toBuffer();
        break;
      case 'sepia':
        processedImage = await sharp(prepRes.imageBuffer)
          .modulate({ brightness: 1, saturation: 0.8 })
          .tint({ r: 255, g: 220, b: 180 })
          .toBuffer();
        break;
      default:
        throw new Error(`Unsupported filter type: ${filterType}`);
    }
    
    return processedImage;
  }

  async post(shared: SharedData, prepRes: {
    filterType: FilterType;
  }, execRes: Buffer): Promise<string> {
    shared.processedImage = execRes;
    shared.filterType = prepRes.filterType;
    return "save";
  }
}

// Node to save a processed image
export class SaveImageNode extends Node<SharedData> {

  async prep(shared: SharedData): Promise<{
    imageBuffer: Buffer;
    imageName: string;
    filterType: FilterType;
  }> {
    return {
      imageBuffer: shared.processedImage || Buffer.from([]),
      imageName: shared.imageName || "",
      filterType: shared.filterType || "grayscale"
    }
  }

  async exec(prepRes: {
    imageBuffer: Buffer;
    imageName: string;
    filterType: FilterType;
  }): Promise<SaveImageResult> {
    const outputName = `${path.parse(prepRes.imageName).name}_${prepRes.filterType}${path.parse(prepRes.imageName).ext}`;
    const outputPath = path.join(__dirname, '../output', outputName);
    
    fs.writeFileSync(outputPath, prepRes.imageBuffer);
    console.log(`Saved processed image to: ${outputPath}`);
    
    return { outputPath };
  }
}

export const loadImageNode = new LoadImageNode();
export const applyFilterNode = new ApplyFilterNode();
export const saveImageNode = new SaveImageNode();
</file>

<file path="cookbook/pocketflow-batch-flow/src/type.ts">
export type FilterType = 'grayscale' | 'blur' | 'sepia';

export interface SharedData {
  imageName?: string;
  imageBuffer?: Buffer;
  processedImage?: Buffer;
  filterType?: FilterType;
}
</file>

<file path="cookbook/pocketflow-batch-flow/package.json">
{
    "name": "pocketflow-batch-flow",
    "dependencies": {
      "pocketflow": ">=1.0.3",
      "sharp": "^0.32.6"
    },
    "devDependencies": {
      "@types/node": "^18.0.0",
      "ts-node": "^10.9.1",
      "typescript": "^5.0.0"
    },
    "scripts": {
        "main": "npx ts-node src/main.ts"
    }
}
</file>

<file path="cookbook/pocketflow-batch-flow/README.md">
# PocketFlow BatchFlow Example

This example demonstrates the BatchFlow concept in PocketFlow by implementing an image processor that applies different filters to multiple images.

## What this Example Demonstrates

- How to use BatchFlow to run a Flow multiple times with different parameters
- Key concepts of BatchFlow:
  1. Creating a base Flow for single-item processing
  2. Using BatchFlow to process multiple items with different parameters
  3. Managing parameters across multiple Flow executions

## Project Structure

```
pocketflow-batch-flow/
├── README.md
├── package.json
├── src/
│   ├── images/
│   │   ├── cat.jpg        # Sample image 1
│   │   ├── dog.jpg        # Sample image 2
│   │   └── bird.jpg       # Sample image 3
│   ├── main.ts            # Entry point
│   ├── flow.ts            # Flow and BatchFlow definitions
│   └── nodes.ts           # Node implementations for image processing
```

## How it Works

The example processes multiple images with different filters:

1. **Base Flow**: Processes a single image

   - Load image
   - Apply filter (grayscale, blur, or sepia)
   - Save processed image

2. **BatchFlow**: Processes multiple image-filter combinations
   - Takes a list of parameters (image + filter combinations)
   - Runs the base Flow for each parameter set
   - Organizes output in a structured way

## Installation

```bash
npm install
```

## Usage

```bash
npm run main
```

## Sample Output

```
Processing images with filters...

Processing cat.jpg with grayscale filter...
Processing cat.jpg with blur filter...
Processing dog.jpg with sepia filter...
...

All images processed successfully!
Check the 'output' directory for results.
```

## Key Concepts Illustrated

1. **Parameter Management**: Shows how BatchFlow manages different parameter sets
2. **Flow Reuse**: Demonstrates running the same Flow multiple times
3. **Batch Processing**: Shows how to process multiple items efficiently
4. **Real-world Application**: Provides a practical example of batch processing
</file>

<file path="cookbook/pocketflow-batch-node/src/flow.ts">
import { Flow } from 'pocketflow';
import { csvProcessorNode, displayResultsNode } from './nodes';
import { SharedData } from './types';

// Connect nodes to create the processing pipeline
csvProcessorNode.on("display_results", displayResultsNode);

// Create and export the flow
export const csvProcessingFlow = new Flow<SharedData>(csvProcessorNode);
</file>

<file path="cookbook/pocketflow-batch-node/src/main.ts">
import path from 'path';
import { csvProcessingFlow } from './flow';
import { SharedData } from './types';
import { csvProcessorNode } from './nodes';

async function main() {
  console.log('PocketFlow BatchNode Example - CSV Processor');
  console.log('=============================================');
  
  const filePath = path.join(__dirname, '../data/sales.csv');
  console.log(`\nProcessing ${path.basename(filePath)} in chunks...`);
  
  try {
    // Create shared context with the file path
    const sharedContext: SharedData = {
      filePath
    };

    // Set parameters for the CSV processor node
    csvProcessorNode.setParams({
      filePath,
      chunkSize: 5 // Process 5 records at a time
    });

    // Execute the flow with the shared context
    console.time('Processing time');
    await csvProcessingFlow.run(sharedContext);
    console.timeEnd('Processing time');
    
    console.log('\nProcessing completed successfully!');
  } catch (error) {
    console.error('Error processing CSV:', error);
  }
}

// Run the main function
main().catch(console.error);
</file>

<file path="cookbook/pocketflow-batch-node/src/nodes.ts">
import fs from 'fs';
import path from 'path';
import { BatchNode, Node } from 'pocketflow';
import csv from 'csv-parser';
import { SalesRecord, ChunkResult, SharedData } from './types';

// Number of records per chunk
const CHUNK_SIZE = 5;

// Define parameters type for the CSV processor with index signature
type CsvProcessorParams = {
  filePath: string;
  chunkSize?: number;
}

// Node to process CSV file in batches
export class CsvProcessorNode extends BatchNode<SharedData, CsvProcessorParams> {
  async prep(shared: SharedData): Promise<SalesRecord[][]> {
    // Use filePath from params or shared context
    const filePath = this._params?.filePath || shared.filePath;
    // Use chunkSize from params or default
    const chunkSize = this._params?.chunkSize || CHUNK_SIZE;
    
    console.log(`Reading CSV file: ${filePath}`);
    
    // Read the entire CSV file
    const allRecords: SalesRecord[] = [];
    
    // Return a promise that resolves when the CSV is fully read
    return new Promise((resolve, reject) => {
      fs.createReadStream(filePath || '')
        .pipe(csv())
        .on('data', (data: SalesRecord) => {
          allRecords.push(data);
        })
        .on('end', () => {
          console.log(`Total records loaded: ${allRecords.length}`);
          
          // Split all records into chunks of specified size
          const chunks: SalesRecord[][] = [];
          for (let i = 0; i < allRecords.length; i += chunkSize) {
            chunks.push(allRecords.slice(i, i + chunkSize));
          }
          
          console.log(`Split into ${chunks.length} chunks of ~${chunkSize} records each`);
          resolve(chunks);
        })
        .on('error', (error) => {
          reject(error);
        });
    });
  }

  async exec(chunk: SalesRecord[]): Promise<ChunkResult> {
    console.log(`Processing chunk with ${chunk.length} records...`);
    
    let totalSales = 0;
    let sumForAverage = 0;
    const totalTransactions = chunk.length;
    
    // Calculate statistics for this chunk
    for (const record of chunk) {
      const saleAmount = parseFloat(record.amount);
      
      totalSales += saleAmount;
      sumForAverage += saleAmount;
    }
    
    return {
      totalSales,
      totalTransactions,
      sumForAverage
    };
  }

  async post(shared: SharedData, chunks: SalesRecord[][], results: ChunkResult[]): Promise<string> {
    console.log(`Combining results from ${results.length} chunks...`);
    
    // Store batch results in shared data
    shared.batchResults = results;
    
    // Aggregate final statistics
    const totalSales = results.reduce((sum, chunk) => sum + chunk.totalSales, 0);
    const totalTransactions = results.reduce((sum, chunk) => sum + chunk.totalTransactions, 0);
    const sumForAverage = results.reduce((sum, chunk) => sum + chunk.sumForAverage, 0);
    const averageSale = sumForAverage / totalTransactions;
    
    // Store final statistics in shared data
    shared.finalStats = {
      totalSales,
      averageSale,
      totalTransactions
    };
    
    return "display_results";
  }
}

// Node to display the final results
export class DisplayResultsNode extends Node<SharedData> {

  async prep(shared: SharedData): Promise<SharedData['finalStats']> {
    return shared.finalStats;
  }

  async exec(finalStats: SharedData['finalStats']): Promise<void> {
    // Check if finalStats exists to avoid errors
    if (!finalStats) {
      console.error("Error: Final statistics not available");
      return;
    }
    
    console.log("\nFinal Statistics:");
    console.log(`- Total Sales: $${finalStats.totalSales.toLocaleString(undefined, { 
      minimumFractionDigits: 2, 
      maximumFractionDigits: 2 
    })}`);
    console.log(`- Average Sale: $${finalStats.averageSale.toLocaleString(undefined, { 
      minimumFractionDigits: 2, 
      maximumFractionDigits: 2 
    })}`);
    console.log(`- Total Transactions: ${finalStats.totalTransactions.toLocaleString()}`);
  }
}

// Create instances of the nodes
export const csvProcessorNode = new CsvProcessorNode();
export const displayResultsNode = new DisplayResultsNode();
</file>

<file path="cookbook/pocketflow-batch-node/src/types.ts">
// Define the structure of a sales record from the CSV
export interface SalesRecord {
  date: string;
  product: string;
  amount: string;
}

// Define the analysis result for a single chunk
export interface ChunkResult {
  totalSales: number;
  totalTransactions: number;
  sumForAverage: number;
}

// Define the shared data structure for our flow
export interface SharedData {
  filePath?: string;
  batchResults?: ChunkResult[];
  finalStats?: {
    totalSales: number;
    averageSale: number;
    totalTransactions: number;
  };
}
</file>

<file path="cookbook/pocketflow-batch-node/package.json">
{
  "name": "pocketflow-batch-node",
  "dependencies": {
    "pocketflow": ">=1.0.3",
    "csv-parser": "^3.0.0",
    "fs-extra": "^11.1.1"
  },
  "devDependencies": {
    "@types/fs-extra": "^11.0.2",
    "@types/node": "^18.0.0",
    "ts-node": "^10.9.1",
    "typescript": "^5.0.0"
  },
  "scripts": {
    "main": "npx ts-node src/main.ts"
  }
}
</file>

<file path="cookbook/pocketflow-batch-node/README.md">
# PocketFlow BatchNode Example

This example demonstrates the BatchNode concept in PocketFlow by implementing a CSV processor that handles large files by processing them in chunks.

## What this Example Demonstrates

- How to use BatchNode to process large inputs in chunks
- The three key methods of BatchNode:
  1. `prep`: Splits input into chunks
  2. `exec`: Processes each chunk independently
  3. `post`: Combines results from all chunks

## Project Structure

```
pocketflow-batch-node/
├── README.md
├── data/
│   └── sales.csv      # Sample large CSV file
├── package.json
├── src/
│   ├── main.ts            # Entry point
│   ├── flow.ts            # Flow definition
│   └── nodes.ts           # BatchNode implementation
```

## How it Works

The example processes a large CSV file containing sales data:

1. **Chunking (prep)**: The CSV file is read and split into chunks of N rows
2. **Processing (exec)**: Each chunk is processed to calculate:
   - Total sales
   - Average sale value
   - Number of transactions
3. **Combining (post)**: Results from all chunks are aggregated into final statistics

## Installation

```bash
npm install
```

## Usage

```bash
npm run main
```

## Sample Output

```
Processing sales.csv in chunks...

Final Statistics:
- Total Sales: $999,359.04
- Average Sale: $99.94
- Total Transactions: 10,000
```

## Key Concepts Illustrated

1. **Chunk-based Processing**: Shows how BatchNode handles large inputs by breaking them into manageable pieces
2. **Independent Processing**: Demonstrates how each chunk is processed separately
3. **Result Aggregation**: Shows how individual results are combined into a final output
</file>

<file path="cookbook/pocketflow-hello-world/hello-world.ts">
import { Node, Flow } from 'pocketflow';

// Define a shared storage
type SharedStorage = { text?: string };

// Node that stores text
class StoreTextNode extends Node<SharedStorage> {
  constructor(private text: string) {
    super();
  }
  
  async prep(shared: SharedStorage): Promise<void> {
    shared.text = this.text;
  }
}

// Node that prints text
class PrintTextNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<void> {
    console.log(shared.text || "No text");
  }
}

// Run example
async function main() {
  const shared: SharedStorage = {};
  
  const storeNode = new StoreTextNode("Hello World");
  const printNode = new PrintTextNode();
  storeNode.next(printNode);
  
  const flow = new Flow(storeNode);
  await flow.run(shared);
}

main();
</file>

<file path="cookbook/pocketflow-hello-world/package.json">
{
    "name": "hello-pocketflow",
    "dependencies": {
      "pocketflow": ">=1.0.3"
    }
}
</file>

<file path="cookbook/pocketflow-hello-world/README.md">
- `npm install`
- `npx ts-node hello-world.ts`
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/output/report.txt">
===================================================
         IMAGE PROCESSING COMPLETION REPORT        
===================================================

Report generated on: 30/3/2025 上午9:31:46
Total images processed: 3

PROCESSING TIME SUMMARY:
Total processing time: 0.07 seconds
Theoretical sequential time: 0.39 seconds
Speedup factor (parallel vs sequential): 5.43x
Average processing time per task: 0.043 seconds

PROCESSED IMAGES:

- bird.jpg
  Filters applied: blur, grayscale, sepia
  Processing times:
    - blur: 0.037 seconds
    - grayscale: 0.032 seconds
    - sepia: 0.031 seconds

- cat.jpg
  Filters applied: blur, grayscale, sepia
  Processing times:
    - blur: 0.032 seconds
    - grayscale: 0.043 seconds
    - sepia: 0.043 seconds

- dog.jpg
  Filters applied: blur, grayscale, sepia
  Processing times:
    - blur: 0.066 seconds
    - grayscale: 0.052 seconds
    - sepia: 0.055 seconds

===================================================
                 END OF REPORT                     
===================================================
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/flows/image_processing_flow.ts">
import { Flow } from 'pocketflow';
import { ImageScannerNode } from '../nodes/image_scanner_node';
import { ImageProcessingNode } from '../nodes/image_processing_node';
import { CompletionReportNode } from '../nodes/completion_report_node';
import { SharedData } from '../types';

/**
 * Image Processing Flow
 * Orchestrates the entire image processing pipeline
 */
export class ImageProcessingFlow extends Flow {
  /**
   * Constructor
   * @param itemsPerBatch Number of images to process in each batch
   * @param concurrency Number of parallel batches to run
   */
  constructor(itemsPerBatch: number = 5, concurrency: number = 3) {
    // Create nodes
    const scannerNode = new ImageScannerNode();
    const processingNode = new ImageProcessingNode();
    const reportNode = new CompletionReportNode();
    
    // Configure parallel batch processing if custom values are provided
    if (itemsPerBatch !== 5 || concurrency !== 3) {
      processingNode.setParams({
        itemsPerBatch,
        concurrency
      });
    }
    
    // Connect nodes
    scannerNode.next(processingNode);
    processingNode.next(reportNode);
    
    // Create flow with the scanner node as the starting point
    super(scannerNode);
  }

  /**
   * Run the flow with initial shared data
   */
  async process(): Promise<SharedData> {
    const shared: SharedData = {
      inputImages: [],
      filters: [],
      outputFolder: '',
      processedImages: []
    };
    
    await this.run(shared);
    
    return shared;
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/completion_report_node.ts">
import { Node } from 'pocketflow';
import { SharedData } from '../types';
import fs from 'fs';
import path from 'path';

/**
 * Completion Report Node
 * Generates a report of all processed images
 */
export class CompletionReportNode extends Node<SharedData> {
  /**
   * Read processedImages from shared store
   */
  async prep(shared: SharedData): Promise<SharedData> {
    // Pass the entire shared data to include timing information
    return shared;
  }

  /**
   * Generate a summary report of all images and filters applied
   */
  async exec(shared: SharedData): Promise<string> {
    const { processedImages, startTime, endTime, processingTimes } = shared;
    
    // Calculate overall processing time
    const totalProcessingTimeMs = endTime && startTime ? endTime - startTime : 0;
    const totalProcessingTimeSec = (totalProcessingTimeMs / 1000).toFixed(2);
    
    // Calculate average processing time per image/filter
    const avgProcessingTimeMs = processingTimes && processingTimes.length > 0 
      ? processingTimes.reduce((sum, item) => sum + item.timeMs, 0) / processingTimes.length
      : 0;
    
    // Calculate theoretical sequential time (sum of all processing times)
    const sequentialTimeMs = processingTimes 
      ? processingTimes.reduce((sum, item) => sum + item.timeMs, 0) 
      : 0;
    const sequentialTimeSec = (sequentialTimeMs / 1000).toFixed(2);
    
    // Calculate speedup factor
    const speedupFactor = sequentialTimeMs > 0 && totalProcessingTimeMs > 0
      ? (sequentialTimeMs / totalProcessingTimeMs).toFixed(2) 
      : 'N/A';
    
    const reportLines = [
      '===================================================',
      '         IMAGE PROCESSING COMPLETION REPORT        ',
      '===================================================',
      '',
      `Report generated on: ${new Date().toLocaleString()}`,
      `Total images processed: ${processedImages.length}`,
      '',
      'PROCESSING TIME SUMMARY:',
      `Total processing time: ${totalProcessingTimeSec} seconds`,
      `Theoretical sequential time: ${sequentialTimeSec} seconds`,
      `Speedup factor (parallel vs sequential): ${speedupFactor}x`,
      `Average processing time per task: ${(avgProcessingTimeMs / 1000).toFixed(3)} seconds`,
      '',
      'PROCESSED IMAGES:',
      ''
    ];
    
    // Add details for each processed image
    for (const image of processedImages) {
      const imageName = path.basename(image.imagePath);
      reportLines.push(`- ${imageName}`);
      reportLines.push(`  Filters applied: ${image.appliedFilters.join(', ')}`);
      
      // Add processing times for this image
      if (processingTimes) {
        const imageTimes = processingTimes.filter(t => t.imagePath === imageName);
        if (imageTimes.length > 0) {
          reportLines.push('  Processing times:');
          for (const time of imageTimes) {
            reportLines.push(`    - ${time.filter}: ${(time.timeMs / 1000).toFixed(3)} seconds`);
          }
        }
      }
      
      reportLines.push('');
    }
    
    reportLines.push('===================================================');
    reportLines.push('                 END OF REPORT                     ');
    reportLines.push('===================================================');
    
    return reportLines.join('\n');
  }

  /**
   * Write report to output folder
   */
  async post(shared: SharedData, _: SharedData, execRes: string): Promise<string | undefined> {
    // Create output directory if it doesn't exist
    if (!fs.existsSync(shared.outputFolder)) {
      fs.mkdirSync(shared.outputFolder, { recursive: true });
    }
    
    // Write report to file
    const reportPath = path.join(shared.outputFolder, 'report.txt');
    fs.writeFileSync(reportPath, execRes);
    
    // Log summary of timing information to console
    const totalTime = shared.endTime && shared.startTime 
      ? (shared.endTime - shared.startTime) / 1000 
      : 0;
    
    console.log(`Report generated at ${reportPath}`);
    console.log(`Total processing time: ${totalTime.toFixed(2)} seconds`);
    
    if (shared.processingTimes && shared.processingTimes.length > 0) {
      const sequentialTime = shared.processingTimes.reduce((sum, item) => sum + item.timeMs, 0) / 1000;
      const speedup = (sequentialTime / totalTime).toFixed(2);
      console.log(`Sequential processing would have taken approximately ${sequentialTime.toFixed(2)} seconds`);
      console.log(`Parallel processing achieved a ${speedup}x speedup`);
    }
    
    return undefined; // End of flow
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/image_processing_node.ts">
import { ParallelBatchNode } from 'pocketflow';
import { SharedData } from '../types';
import { processImage } from '../utils';
import path from 'path';

/**
 * Image Processing Input
 * Structure for each batch item in the ParallelBatchNode
 */
interface ImageProcessingInput {
  imagePath: string;
  filter: string;
}

/**
 * Image Processing Result
 * Structure for the output of processing each image
 */
interface ImageProcessingResult {
  originalPath: string;
  processedPath: string;
  filter: string;
  success: boolean;
  processingTimeMs: number; // Time taken to process
}

/**
 * Image Processing Node
 * Uses ParallelBatchFlow to apply filters to images in parallel
 */
export class ImageProcessingNode extends ParallelBatchNode<SharedData> {
  // Set default batch parameters
  constructor() {
    super();
    this.setParams({
      itemsPerBatch: 5,
      concurrency: 3
    });
  }

  /**
   * Create a batch of image processing tasks
   * Each task consists of an image and a filter to apply
   */
  async prep(shared: SharedData): Promise<ImageProcessingInput[]> {
    const tasks: ImageProcessingInput[] = [];
    
    // For each image, create a task for each filter
    for (const imagePath of shared.inputImages) {
      for (const filter of shared.filters) {
        tasks.push({
          imagePath,
          filter
        });
      }
    }
    
    console.log(`Created ${tasks.length} image processing tasks`);
    return tasks;
  }

  /**
   * Process a single image with the specified filter
   */
  async exec(input: ImageProcessingInput): Promise<ImageProcessingResult> {
    console.log(`Processing ${path.basename(input.imagePath)} with ${input.filter} filter`);
    
    const startTime = Date.now();
    try {
      const processedPath = await processImage(input.imagePath, input.filter);
      const endTime = Date.now();
      const processingTimeMs = endTime - startTime;
      
      return {
        originalPath: input.imagePath,
        processedPath,
        filter: input.filter,
        success: true,
        processingTimeMs
      };
    } catch (error) {
      console.error(`Failed to process ${input.imagePath} with ${input.filter}:`, error);
      const endTime = Date.now();
      const processingTimeMs = endTime - startTime;
      
      return {
        originalPath: input.imagePath,
        processedPath: '',
        filter: input.filter,
        success: false,
        processingTimeMs
      };
    }
  }

  /**
   * Update the shared store with the results of all processed images
   */
  async post(
    shared: SharedData, 
    _: ImageProcessingInput[], 
    results: ImageProcessingResult[]
  ): Promise<string | undefined> {
    // Group results by original image path
    const imageMap = new Map<string, string[]>();
    
    // Record processing times and update processed images
    for (const result of results) {
      // Store processing time
      if (!shared.processingTimes) {
        shared.processingTimes = [];
      }
      
      shared.processingTimes.push({
        imagePath: path.basename(result.originalPath),
        filter: result.filter,
        timeMs: result.processingTimeMs
      });
      
      if (result.success) {
        const appliedFilters = imageMap.get(result.originalPath) || [];
        appliedFilters.push(result.filter);
        imageMap.set(result.originalPath, appliedFilters);
      }
    }
    
    // Update processedImages in shared store
    for (const [imagePath, appliedFilters] of imageMap.entries()) {
      shared.processedImages.push({
        imagePath,
        appliedFilters
      });
    }
    
    console.log(`Successfully processed ${shared.processedImages.length} images`);
    
    // Set the end time
    shared.endTime = Date.now();
    
    return 'default'; // Go to the next node
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/image_scanner_node.ts">
import { Node } from 'pocketflow';
import { SharedData } from '../types';
import { readDirectory } from '../utils';
import path from 'path';

/**
 * Image Scanner Node
 * Scans the src/images directory to find all input images
 */
export class ImageScannerNode extends Node<SharedData> {
  /**
   * Initialize empty inputImages array in shared store
   */
  async prep(shared: SharedData): Promise<null> {
    shared.inputImages = [];
    // Initialize timing data
    shared.startTime = Date.now();
    shared.processingTimes = [];
    return null;
  }

  /**
   * Read all files from src/images directory, filter for image files
   */
  async exec(_: null): Promise<string[]> {
    const imagesPath = path.join(process.cwd(), 'src', 'images');
    console.log(`Scanning for images in ${imagesPath}`);
    return readDirectory(imagesPath);
  }

  /**
   * Write image paths to inputImages in shared store and initialize filters array
   */
  async post(shared: SharedData, _: null, execRes: string[]): Promise<string | undefined> {
    shared.inputImages = execRes;
    shared.filters = ['blur', 'grayscale', 'sepia'];
    shared.outputFolder = path.join(process.cwd(), 'output');
    shared.processedImages = [];
    
    console.log(`Found ${shared.inputImages.length} images to process`);
    
    return 'default'; // Go to the next node
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/nodes/index.ts">
export { ImageScannerNode } from './image_scanner_node';
export { ImageProcessingNode } from './image_processing_node';
export { CompletionReportNode } from './completion_report_node';
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/utils/index.ts">
export { readDirectory } from './read_directory';
export { processImage } from './process_image';
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/utils/process_image.ts">
import fs from 'fs';
import path from 'path';
import sharp from 'sharp';

/**
 * Applies a filter to an image
 * @param imagePath Path to the image file
 * @param filter Filter to apply (blur, grayscale, sepia)
 * @returns Path to the processed image
 */
export async function processImage(imagePath: string, filter: string): Promise<string> {
  try {
    // Create a sharp instance with the input image
    let sharpImage = sharp(imagePath);
    
    // Apply the filter
    switch (filter) {
      case 'blur':
        sharpImage = sharpImage.blur(5);
        break;
      case 'grayscale':
        sharpImage = sharpImage.grayscale();
        break;
      case 'sepia':
        // Sepia is implemented using a color matrix
        sharpImage = sharpImage.recomb([
          [0.393, 0.769, 0.189],
          [0.349, 0.686, 0.168],
          [0.272, 0.534, 0.131]
        ]);
        break;
      default:
        console.warn(`Unknown filter: ${filter}`);
    }
    
    // Generate output path
    const { dir, name, ext } = path.parse(imagePath);
    const outputDir = path.join(process.cwd(), 'output');
    
    // Create output directory if it doesn't exist
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }
    
    const outputPath = path.join(outputDir, `${name}_${filter}${ext}`);
    
    // Save the processed image
    await sharpImage.toFile(outputPath);
    
    return outputPath;
  } catch (error) {
    console.error(`Error processing image ${imagePath} with filter ${filter}:`, error);
    throw error;
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/utils/read_directory.ts">
import fs from 'fs';
import path from 'path';

/**
 * Reads all files from a directory
 * @param directoryPath Path to the directory to read
 * @returns Array of file paths
 */
export function readDirectory(directoryPath: string): string[] {
  try {
    const files = fs.readdirSync(directoryPath);
    // Filter for image files (jpg, png, jpeg, gif)
    return files
      .filter(file => 
        /\.(jpg|jpeg|png|gif)$/i.test(file))
      .map(file => path.join(directoryPath, file));
  } catch (error) {
    console.error(`Error reading directory ${directoryPath}:`, error);
    return [];
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/index.ts">
import { ImageProcessingFlow } from './flows/image_processing_flow';
import fs from 'fs';
import path from 'path';

/**
 * Main function to run the image filter application
 */
async function main() {
  console.log("Starting Image Filter Application...");
  
  // Ensure the images directory exists
  const imagesDir = path.join(process.cwd(), 'src', 'images');
  if (!fs.existsSync(imagesDir)) {
    fs.mkdirSync(imagesDir, { recursive: true });
    console.log(`Created images directory at: ${imagesDir}`);
    console.log("Please add some image files to this directory and run the application again.");
    return;
  }
  
  // Check if there are any images in the directory
  const files = fs.readdirSync(imagesDir);
  const imageFiles = files.filter(file => /\.(jpg|jpeg|png|gif)$/i.test(file));
  
  if (imageFiles.length === 0) {
    console.log("No image files found in the images directory.");
    console.log("Please add some image files to the src/images directory and run the application again.");
    return;
  }
  
  console.log("--------------------------------------------------");
  console.log("Starting parallel image processing...");
  console.log("--------------------------------------------------");
  
  // Create and run the image processing flow
  try {
    // Start time for overall application
    const appStartTime = Date.now();
    
    // 5 images per batch, 3 parallel batches
    const processingFlow = new ImageProcessingFlow(5, 3);
    const result = await processingFlow.process();
    
    // End time for overall application
    const appEndTime = Date.now();
    const totalAppTime = (appEndTime - appStartTime) / 1000;
    
    console.log("--------------------------------------------------");
    console.log("Image processing completed successfully!");
    console.log(`Processed ${imageFiles.length} images with 3 filters (blur, grayscale, sepia).`);
    console.log(`Output files and report can be found in the 'output' directory.`);
    console.log("--------------------------------------------------");
    console.log(`Total application time: ${totalAppTime.toFixed(2)} seconds`);
    console.log("--------------------------------------------------");
  } catch (error) {
    console.error("Error processing images:", error);
  }
}

// Run the main function
main().catch(console.error);
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/src/types.ts">
/**
 * Interface for the shared memory data structure
 */
export interface SharedData {
  // Input data
  inputImages: string[]; // Paths to input images

  // Processing data
  filters: string[]; // List of filters to apply (blur, grayscale, sepia)

  // Output data
  outputFolder: string; // Path to output folder
  processedImages: {
    // Tracking processed images
    imagePath: string;
    appliedFilters: string[];
  }[];

  // Timing data
  startTime?: number; // Start time in milliseconds
  endTime?: number; // End time in milliseconds
  processingTimes?: {
    // Individual processing times for each image/filter combination
    imagePath: string;
    filter: string;
    timeMs: number;
  }[];
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/design.md">
# Design Doc: Image Filter Application

> Please DON'T remove notes for AI

## Requirements

> Notes for AI: Keep it simple and clear.
> If the requirements are abstract, write concrete user stories

1. The application must process multiple images located in the src/images directory
2. Three filters must be applied to each image: blur, grayscale, and sepia
3. Each image must be processed with all three filters, generating three output images
4. All processed images must be saved in an output folder with naming pattern originalName_filterName
5. Image processing must be parallelized using ParallelBatchFlow for efficiency
6. The application must generate a report of all processed images
7. The application must be compatible with Node.js environments

**User Stories:**

- As a user, I want to process multiple images with different filters so I can choose the best visual effect
- As a user, I want the processing to happen in parallel to save time
- As a user, I want processed images to be named consistently so I can easily identify them
- As a user, I want a report of all processed images to track what was done
- As a user, I want the application to run in any Node.js environment without special dependencies

## Technology Stack

- **Node.js**: Runtime environment
- **TypeScript**: Programming language
- **PocketFlow**: Framework for parallel processing
- **Sharp**: High-performance image processing library for Node.js

## Flow Design

> Notes for AI:
>
> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.
> 2. Present a concise, high-level description of the workflow.

### Applicable Design Pattern:

ParallelBatchFlow is perfect for this use case because:

1. Each image needs the same set of filters applied (consistent workload)
2. The filter operations are independent and can run in parallel
3. Batch processing provides efficient resource utilization

### Flow high-level Design:

1. **Image Scanner Node**: Finds all images in src/images directory
2. **Image Processing Node**: Uses ParallelBatchFlow to apply filters to images
3. **Completion Report Node**: Generates a report of all processed images

```mermaid
flowchart TD
    scanNode[Image Scanner Node] --> processNode[Image Processing Node]
    processNode --> reportNode[Completion Report Node]

    subgraph processNode[Image Processing Node - ParallelBatchFlow]
        subgraph batch1[Batch 1]
            image1[Image 1] --> filter1_1[Apply Filters]
            image2[Image 2] --> filter1_2[Apply Filters]
        end
        subgraph batch2[Batch 2]
            image3[Image 3] --> filter2_1[Apply Filters]
            image4[Image 4] --> filter2_2[Apply Filters]
        end
    end
```

## Utility Functions

> Notes for AI:
>
> 1. Understand the utility function definition thoroughly by reviewing the doc.
> 2. Include only the necessary utility functions, based on nodes in the flow.

1. **Read Directory** (`src/utils/read_directory.ts`)

   - _Input_: directoryPath (string)
   - _Output_: files (string[])
   - _Implementation_: Uses Node.js fs module to read directory and filter for image files
   - Used by Image Scanner Node to get all image files

2. **Image Processing** (`src/utils/process_image.ts`)
   - _Input_: imagePath (string), filter (string)
   - _Output_: processedImagePath (string)
   - _Implementation_: Uses Sharp library to apply filters to images
   - Filter implementations:
     - Blur: Uses Sharp's blur function with radius 5
     - Grayscale: Uses Sharp's built-in grayscale function
     - Sepia: Uses Sharp's recomb function with a sepia color matrix
   - Used by Image Processing Node to apply filters to images

## Node Design

### Shared Memory

> Notes for AI: Try to minimize data redundancy

The shared memory structure is organized as follows:

```typescript
interface SharedData {
  // Input data
  inputImages: string[]; // Paths to input images

  // Processing data
  filters: string[]; // List of filters to apply (blur, grayscale, sepia)

  // Output data
  outputFolder: string; // Path to output folder
  processedImages: {
    // Tracking processed images
    imagePath: string;
    appliedFilters: string[];
  }[];
}
```

### Node Steps

> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow.

1. Image Scanner Node

- _Purpose_: Scan the src/images directory to find all input images
- _Type_: Regular
- _Steps_:
  - _prep_: Initialize empty inputImages array in shared store
  - _exec_: Read all files from src/images directory, filter for image files
  - _post_: Write image paths to inputImages in shared store and initialize filters array with ["blur", "grayscale", "sepia"]

2. Image Processing Node

- _Purpose_: Apply filters to images in parallel
- _Type_: ParallelBatchFlow
- _Batch Configuration_:
  - _itemsPerBatch_: 5 (process 5 images at a time)
  - _concurrency_: 3 (run 3 parallel batches)
- _Sub-flow_:
  - Apply Filter Node (for each image)
    - _Type_: Batch
    - _Input_: Single image path and array of filters
    - _Steps_:
      - _prep_: Load the image data using Sharp
      - _exec_: For each filter, apply the filter to the image using Sharp's API
      - _post_: Save processed images to output folder with naming pattern originalName_filterName

3. Completion Report Node

- _Purpose_: Generate a report of all processed images
- _Type_: Regular
- _Steps_:
  - _prep_: Read processedImages from shared store
  - _exec_: Generate a summary report of all images and filters applied
  - _post_: Write report to output folder
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/package.json">
{
  "name": "image-filter-app",
  "scripts": {
    "start": "npx ts-node src/index.ts"
  },
  "dependencies": {
    "sharp": "^0.32.1",
    "pocketflow": "^1.0.0"
  },
  "devDependencies": {
    "@types/node": "^14.18.63",
    "typescript": "^4.9.5"
  }
}
</file>

<file path="cookbook/pocketflow-parallel-batch-flow/README.md">
# Parallel Image Processor

Demonstrates how AsyncParallelBatchFlow processes multiple images with multiple filters >8x faster than sequential processing.

## Features

```mermaid
graph TD
    subgraph AsyncParallelBatchFlow[Image Processing Flow]
        subgraph AsyncFlow[Per Image-Filter Flow]
            A[Load Image] --> B[Apply Filter]
            B --> C[Save Image]
        end
    end
```

- Processes images with multiple filters in parallel
- Applies three different filters (grayscale, blur, sepia)
- Shows significant speed improvement over sequential processing
- Manages system resources with semaphores

## Run It

```bash
npm install
npm run start
```

## Output

```
Starting Image Filter Application...
Scanning for images in C:\WorkSpace\PocketFlow-Typescript\cookbook\pocketflow-parallel-batch-flow\src\images
Found 3 images to process
Created 9 image processing tasks
Processing bird.jpg with blur filter
Processing bird.jpg with grayscale filter
Processing bird.jpg with sepia filter
Processing cat.jpg with blur filter
Processing cat.jpg with grayscale filter
Processing cat.jpg with sepia filter
Processing dog.jpg with blur filter
Processing dog.jpg with grayscale filter
Processing dog.jpg with sepia filter
Successfully processed 3 images
Report generated at report.txt
Image processing completed successfully!
Processed 3 images with 3 filters (blur, grayscale, sepia).
Output files and report can be found in the 'output' directory.
```

## Key Points

- **Sequential**: Total time = sum of all item times

  - Good for: Rate-limited APIs, maintaining order

- **Parallel**: Total time ≈ longest single item time
  - Good for: I/O-bound tasks, independent operations
</file>

<file path="docs/core_abstraction/batch.md">
---
layout: default
title: "Batch"
parent: "Core Abstraction"
nav_order: 4
---

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:

- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **array** of items to process.
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prepRes, execResList)`**: after all items are processed, receives a **list** of results (`execResList`) and returns an **Action**.

### Example: Summarize a Large File

```typescript
type SharedStorage = {
  data: string;
  summary?: string;
};

class MapSummaries extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // Chunk content into manageable pieces
    const content = shared.data;
    const chunks: string[] = [];
    const chunkSize = 10000;

    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize));
    }

    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    _: string[],
    summaries: string[]
  ): Promise<string> {
    shared.summary = summaries.join("\n");
    return "default";
  }
}

// Usage
const flow = new Flow(new MapSummaries());
await flow.run({ data: "very long text content..." });
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Example: Summarize Many Files

```typescript
type SharedStorage = {
  files: string[];
};

type FileParams = {
  filename: string;
};

class SummarizeAllFiles extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    return shared.files.map((filename) => ({ filename }));
  }
}

// Create a per-file summarization flow
const summarizeFile = new SummarizeFile();
const summarizeAllFiles = new SummarizeAllFiles(summarizeFile);

await summarizeAllFiles.run({ files: ["file1.txt", "file2.txt"] });
```

### Under the Hood

1. `prep(shared)` returns a list of param objects—e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each object and:
   - Merges it with the BatchFlow's own `params`
   - Calls `flow.run(shared)` using the merged result
3. This means the sub-Flow runs **repeatedly**, once for every param object.

---

## 3. Nested Batches

You can nest BatchFlows to handle hierarchical data processing:

```typescript
type DirectoryParams = {
  directory: string;
};

type FileParams = DirectoryParams & {
  filename: string;
};

class FileBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    const directory = this._params.directory;
    const files = await getFilesInDirectory(directory).filter((f) =>
      f.endsWith(".txt")
    );

    return files.map((filename) => ({
      directory, // Pass on directory from parent
      filename, // Add filename for this batch item
    }));
  }
}

class DirectoryBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<DirectoryParams[]> {
    return ["/path/to/dirA", "/path/to/dirB"].map((directory) => ({
      directory,
    }));
  }
}

// Process all files in all directories
const processingNode = new ProcessingNode();
const fileFlow = new FileBatchFlow(processingNode);
const dirFlow = new DirectoryBatchFlow(fileFlow);
await dirFlow.run({});
```
</file>

<file path="docs/core_abstraction/communication.md">
---
layout: default
title: "Communication"
parent: "Core Abstraction"
nav_order: 3
---

# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)**

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate _Data Schema_ from _Compute Logic_! This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](./batch.md).
     > {: .best-practice }

2. **Params (only for [Batch](./batch.md))**
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:

```typescript
interface SharedStore {
  data: Record<string, unknown>;
  summary: Record<string, unknown>;
  config: Record<string, unknown>;
  // ...other properties
}

const shared: SharedStore = { data: {}, summary: {}, config: {} /* ... */ };
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```typescript
interface SharedStore {
  data: string;
  summary: string;
}

class LoadData extends Node<SharedStore> {
  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write data to shared store
    shared.data = "Some text content";
    return "default";
  }
}

class Summarize extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<unknown> {
    // We read data from shared store
    return shared.data;
  }

  async exec(prepRes: unknown): Promise<unknown> {
    // Call LLM to summarize
    const prompt = `Summarize: ${prepRes}`;
    const summary = await callLlm(prompt);
    return summary;
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write summary to shared store
    shared.summary = execRes as string;
    return "default";
  }
}

const loadData = new LoadData();
const summarize = new Summarize();
loadData.next(summarize);
const flow = new Flow(loadData);

const shared: SharedStore = { data: "", summary: "" };
flow.run(shared);
```

Here:

- `LoadData` writes to `shared.data`.
- `Summarize` reads from `shared.data`, summarizes, and writes to `shared.summary`.

---

## 2. Params

**Params** let you store _per-Node_ or _per-Flow_ config that doesn't need to live in the shared store. They are:

- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `setParams()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow.
>
> If you need to set child node params, see [Batch](./batch.md).
> {: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```typescript
interface SharedStore {
  data: Record<string, string>;
  summary: Record<string, string>;
}

interface SummarizeParams {
  filename: string;
}

// 1) Create a Node that uses params
class SummarizeFile extends Node<SharedStore, SummarizeParams> {
  async prep(shared: SharedStore): Promise<unknown> {
    // Access the node's param
    const filename = this._params.filename;
    return shared.data[filename] || "";
  }

  async exec(prepRes: unknown): Promise<unknown> {
    const prompt = `Summarize: ${prepRes}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    const filename = this._params.filename;
    shared.summary[filename] = execRes as string;
    return "default";
  }
}

// 2) Set params
const node = new SummarizeFile();

// 3) Set Node params directly (for testing)
node.setParams({ filename: "doc1.txt" });
node.run(shared);

// 4) Create Flow
const flow = new Flow<SharedStore>(node);

// 5) Set Flow params (overwrites node params)
flow.setParams({ filename: "doc2.txt" });
flow.run(shared); // The node summarizes doc2, not doc1
```
</file>

<file path="docs/core_abstraction/flow.md">
---
layout: default
title: "Flow"
parent: "Core Abstraction"
nav_order: 2
---

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `nodeA.next(nodeB)`
   This means if `nodeA.post()` returns `"default"`, go to `nodeB`.
   (Equivalent to `nodeA.on("default", nodeB)`)

2. **Named action transition**: `nodeA.on("actionName", nodeB)`
   This means if `nodeA.post()` returns `"actionName"`, go to `nodeB`.

It's possible to create loops, branching, or multi-step flows.

## 2. Method Chaining

The transition methods support **chaining** for more concise flow creation:

### Chaining `on()` Methods

The `on()` method returns the current node, so you can chain multiple action definitions:

```typescript
// All transitions from the same node
nodeA.on("approved", nodeB).on("rejected", nodeC).on("needs_review", nodeD);

// Equivalent to:
nodeA.on("approved", nodeB);
nodeA.on("rejected", nodeC);
nodeA.on("needs_review", nodeD);
```

### Chaining `next()` Methods

The `next()` method returns the target node, allowing you to create linear sequences in a single expression:

```typescript
// Creates a linear A → B → C → D sequence
nodeA.next(nodeB).next(nodeC).next(nodeD);

// Equivalent to:
nodeA.next(nodeB);
nodeB.next(nodeC);
nodeC.next(nodeD);
```

### Combining Chain Types

You can combine both chaining styles for complex flows:

```typescript
nodeA
  .on("action1", nodeB.next(nodeC).next(nodeD))
  .on("action2", nodeE.on("success", nodeF).on("failure", nodeG));
```

## 3. Creating a Flow

A **Flow** begins with a **start** node. You call `const flow = new Flow(someNode)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```typescript
nodeA.next(nodeB);
const flow = new Flow(nodeA);
flow.run(shared);
```

- When you run the flow, it executes `nodeA`.
- Suppose `nodeA.post()` returns `"default"`.
- The flow then sees `"default"` Action is linked to `nodeB` and runs `nodeB`.
- `nodeB.post()` returns `"default"` but we didn't define a successor for `nodeB`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```typescript
// Define the flow connections
review.on("approved", payment); // If approved, process payment
review.on("needs_revision", revise); // If needs changes, go to revision
review.on("rejected", finish); // If rejected, finish the process

revise.next(review); // After revision, go back for another review
payment.next(finish); // After payment, finish the process

const flow = new Flow(review);
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action.
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
>
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
{: .warning }

## 4. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.
2. Combine multiple smaller Flows into a larger Flow for reuse.
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `undefined` for `execRes` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```typescript
// Create a sub-flow
nodeA.next(nodeB);
const subflow = new Flow(nodeA);

// Connect it to another node
subflow.next(nodeC);

// Create the parent flow
const parentFlow = new Flow(subflow);
```

When `parentFlow.run()` executes:

1. It starts `subflow`
2. `subflow` runs through its nodes (`nodeA->nodeB`)
3. After `subflow` completes, execution continues to `nodeC`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```typescript
// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation);
const paymentFlow = new Flow(validatePayment);

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory);
const inventoryFlow = new Flow(checkStock);

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup);
const shippingFlow = new Flow(createLabel);

// Connect the flows into a main order pipeline
paymentFlow.next(inventoryFlow).next(shippingFlow);

// Create the master flow
const orderPipeline = new Flow(paymentFlow);

// Run the entire pipeline
orderPipeline.run(sharedData);
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```
</file>

<file path="docs/core_abstraction/index.md">
---
layout: default
title: "Core Abstraction"
nav_order: 2
has_children: true
---
</file>

<file path="docs/core_abstraction/node.md">
---
layout: default
title: "Node"
parent: "Core Abstraction"
nav_order: 1
---

# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`

   - **Read and preprocess data** from `shared` store.
   - Examples: _query DB, read files, or serialize data into a string_.
   - Return `prepRes`, which is used by `exec()` and `post()`.

2. `exec(prepRes)`

   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: _(mostly) LLM calls, remote APIs, tool use_.
   - ⚠️ This shall be only for compute and **NOT** access `shared`.
   - ⚠️ If retries enabled, ensure idempotent implementation.
   - Return `execRes`, which is passed to `post()`.

3. `post(shared, prepRes, execRes)`
   - **Postprocess and write data** back to `shared`.
   - Examples: _update DB, change states, log results_.
   - **Decide the next action** by returning a _string_ (`action = "default"` if _None_).

> **Why 3 steps?** To enforce the principle of _separation of concerns_. The data storage and data processing are operated separately.
>
> All steps are _optional_. E.g., you can only implement `prep` and `post` if you just need to process data.
> {: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting).
  `wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```typescript
const myNode = new SummarizeFile(3, 10); // maxRetries = 3, wait = 10 seconds
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `maxRetries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `this.currentRetry`.

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```typescript
execFallback(prepRes: unknown, error: Error): unknown {
  return "There was an error processing your request.";
}
```

By default, it just re-raises the exception.

### Example: Summarize file

```typescript
type SharedStore = {
  data: string;
  summary?: string;
};

class SummarizeFile extends Node<SharedStore> {
  prep(shared: SharedStore): string {
    return shared.data;
  }

  exec(content: string): string {
    if (!content) return "Empty file content";

    const prompt = `Summarize this text in 10 words: ${content}`;
    return callLlm(prompt);
  }

  execFallback(_: string, error: Error): string {
    return "There was an error processing your request.";
  }

  post(shared: SharedStore, _: string, summary: string): string | undefined {
    shared.summary = summary;
    return undefined; // "default" action
  }
}

// Example usage
const node = new SummarizeFile(3); // maxRetries = 3
const shared: SharedStore = { data: "Long text to summarize..." };
const action = node.run(shared);

console.log("Action:", action);
console.log("Summary:", shared.summary);
```
</file>

<file path="docs/core_abstraction/parallel.md">
---
layout: default
title: "(Advanced) Parallel"
parent: "Core Abstraction"
nav_order: 6
---

# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple operations **concurrently**—for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute.

> Parallel nodes and flows excel at overlapping I/O-bound work—like LLM calls, database queries, API requests, or file I/O. TypeScript's Promise-based implementation allows for truly concurrent execution of asynchronous operations.
> {: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
>
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism.
>
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
>   {: .best-practice }

## ParallelBatchNode

Like **BatchNode**, but runs operations in **parallel** using Promise.all():

```typescript
class TextSummarizer extends ParallelBatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // e.g., multiple texts
    return shared.texts || [];
  }

  async exec(text: string): Promise<string> {
    const prompt = `Summarize: ${text}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.summaries = execRes;
    return "default";
  }
}

const node = new TextSummarizer();
const flow = new Flow(node);
```

## ParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using Promise.all():

```typescript
class SummarizeMultipleFiles extends ParallelBatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, any>[]> {
    return (shared.files || []).map((f) => ({ filename: f }));
  }
}

const subFlow = new Flow(new LoadAndSummarizeFile());
const parallelFlow = new SummarizeMultipleFiles(subFlow);
await parallelFlow.run(shared);
```
</file>

<file path="docs/design_pattern/agent.md">
---
layout: default
title: "Agent"
parent: "Design Pattern"
nav_order: 1
---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodes—and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide action—for example:

```typescript
`
### CONTEXT
Task: ${task}
Previous Actions: ${prevActions}
Current State: ${state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters: query (str)

[2] answer
  Description: Conclude based on the results
  Parameters: result (str)

### NEXT ACTION
Decide the next action based on the current context.
Return your response in YAML format:

\`\`\`yaml
thinking: <reasoning process>
action: <action_name>
parameters: <parameters>
\`\`\``;
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md).

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actions—avoiding overlap like separate `read_databases` or `read_csvs`.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks instead of all at once.
- **Overview-zoom-in:** First provide high-level structure, then allow drilling into details.
- **Parameterized/Programmable:** Enable parameterized or programmable actions.
- **Backtracking:** Let the agent undo the last step instead of restarting entirely.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

````typescript
interface SharedState {
  query?: string;
  context?: Array<{ term: string; result: string }>;
  search_term?: string;
  answer?: string;
}

class DecideAction extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    const context = shared.context
      ? JSON.stringify(shared.context)
      : "No previous search";
    return [shared.query || "", context];
  }

  async exec([query, context]: [string, string]): Promise<any> {
    const prompt = `
Given input: ${query}
Previous search results: ${context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
\`\`\`yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
\`\`\``;
    const resp = await callLlm(prompt);
    const yamlStr = resp.split("```yaml")[1].split("```")[0].trim();
    return yaml.load(yamlStr);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    result: any
  ): Promise<string> {
    if (result.action === "search") {
      shared.search_term = result.search_term;
    }
    return result.action;
  }
}

class SearchWeb extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.search_term || "";
  }

  async exec(searchTerm: string): Promise<string> {
    return await searchWeb(searchTerm);
  }

  async post(shared: SharedState, _: string, execRes: string): Promise<string> {
    shared.context = [
      ...(shared.context || []),
      { term: shared.search_term || "", result: execRes },
    ];
    return "decide";
  }
}

class DirectAnswer extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    return [
      shared.query || "",
      shared.context ? JSON.stringify(shared.context) : "",
    ];
  }

  async exec([query, context]: [string, string]): Promise<string> {
    return await callLlm(`Context: ${context}\nAnswer: ${query}`);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    execRes: string
  ): Promise<undefined> {
    shared.answer = execRes;
    return undefined;
  }
}

// Connect nodes
const decide = new DecideAction();
const search = new SearchWeb();
const answer = new DirectAnswer();

decide.on("search", search);
decide.on("answer", answer);
search.on("decide", decide); // Loop back

const flow = new Flow(decide);
await flow.run({ query: "Who won the Nobel Prize in Physics 2024?" });
````
</file>

<file path="docs/design_pattern/index.md">
---
layout: default
title: "Design Pattern"
nav_order: 3
has_children: true
---
</file>

<file path="docs/design_pattern/mapreduce.md">
---
layout: default
title: "Map Reduce"
parent: "Design Pattern"
nav_order: 4
---

# Map Reduce

MapReduce is a design pattern suitable when you have either:

- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```typescript
type SharedStorage = {
  files?: Record<string, string>;
  file_summaries?: Record<string, string>;
  all_files_summary?: string;
};

class SummarizeAllFiles extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<[string, string][]> {
    return Object.entries(shared.files || {}); // [["file1.txt", "aaa..."], ["file2.txt", "bbb..."], ...]
  }

  async exec([filename, content]: [string, string]): Promise<[string, string]> {
    const summary = await callLLM(`Summarize the following file:\n${content}`);
    return [filename, summary];
  }

  async post(
    shared: SharedStorage,
    _: [string, string][],
    summaries: [string, string][]
  ): Promise<string> {
    shared.file_summaries = Object.fromEntries(summaries);
    return "summarized";
  }
}

class CombineSummaries extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, string>> {
    return shared.file_summaries || {};
  }

  async exec(summaries: Record<string, string>): Promise<string> {
    const text_list = Object.entries(summaries).map(
      ([fname, summ]) => `${fname} summary:\n${summ}\n`
    );

    return await callLLM(
      `Combine these file summaries into one final summary:\n${text_list.join(
        "\n---\n"
      )}`
    );
  }

  async post(
    shared: SharedStorage,
    _: Record<string, string>,
    finalSummary: string
  ): Promise<string> {
    shared.all_files_summary = finalSummary;
    return "combined";
  }
}

// Create and connect flow
const batchNode = new SummarizeAllFiles();
const combineNode = new CombineSummaries();
batchNode.on("summarized", combineNode);

// Run the flow with test data
const flow = new Flow(batchNode);
flow.run({
  files: {
    "file1.txt":
      "Alice was beginning to get very tired of sitting by her sister...",
    "file2.txt": "Some other interesting text ...",
  },
});
```

> **Performance Tip**: The example above works sequentially. You can speed up the map phase by using `ParallelBatchNode` instead of `BatchNode`. See [(Advanced) Parallel](../core_abstraction/parallel.md) for more details.
> {: .note }
</file>

<file path="docs/design_pattern/multi_agent.md">
---
layout: default
title: "(Advanced) Multi-Agents"
parent: "Design Pattern"
nav_order: 6
---

# (Advanced) Multi-Agents

Multiple [Agents](./flow.md) can work together by handling subtasks and communicating the progress.
Communication between agents is typically implemented using message queues in shared storage.

> Most of time, you don't need Multi-Agents. Start with a simple solution first.
> {: .best-practice }

### Example Agent Communication: Message Queue

Here's a simple example showing how to implement agent communication using a TypeScript message queue.
The agent listens for messages, processes them, and continues listening:

```typescript
import { Node, Flow } from "../src/index";

// Define shared storage with message queue
type SharedStorage = {
  messages: string[];
  processing?: boolean;
};

class AgentNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    // Check if there are messages to process
    if (shared.messages.length === 0) {
      return undefined;
    }
    // Get the next message
    return shared.messages.shift();
  }

  async exec(message: string | undefined): Promise<string | undefined> {
    if (!message) {
      return undefined;
    }
    console.log(`Agent received: ${message}`);
    return message;
  }

  async post(
    shared: SharedStorage,
    prepRes: string | undefined,
    execRes: string | undefined
  ): Promise<string> {
    if (shared.messages.length === 0) {
      // Add a small delay to avoid tight loop CPU consumption
      await new Promise((resolve) => setTimeout(resolve, 100));
    }
    // Continue processing messages
    return "continue";
  }
}

// Message sender function
function sendSystemMessages(shared: SharedStorage) {
  let counter = 0;
  const messages = [
    "System status: all systems operational",
    "Memory usage: normal",
    "Network connectivity: stable",
    "Processing load: optimal",
  ];

  // Add a new message every second
  let intervalId: NodeJS.Timeout;
  intervalId = setInterval(() => {
    const message = `${
      messages[counter % messages.length]
    } | timestamp_${counter}`;
    shared.messages.push(message);
    counter++;

    // Stop after a few messages for demonstration
    if (counter >= 10) {
      clearInterval(intervalId);
    }
  }, 1000);
}

async function main() {
  // Create shared storage with empty message queue
  const shared: SharedStorage = {
    messages: [],
  };

  // Create agent node
  const agent = new AgentNode();
  agent.on("continue", agent); // Connect to self to continue processing

  // Create flow
  const flow = new Flow(agent);

  // Start sending messages
  sendSystemMessages(shared);

  // Run the flow
  await flow.run(shared);
}

main().catch(console.error);
```

The output:

```
Agent received: System status: all systems operational | timestamp_0
Agent received: Memory usage: normal | timestamp_1
Agent received: Network connectivity: stable | timestamp_2
Agent received: Processing load: optimal | timestamp_3
Agent received: System status: all systems operational | timestamp_4
Agent received: Memory usage: normal | timestamp_5
Agent received: Network connectivity: stable | timestamp_6
Agent received: Processing load: optimal | timestamp_7
Agent received: System status: all systems operational | timestamp_8
Agent received: Memory usage: normal | timestamp_9
```

### Interactive Multi-Agent Example: Taboo Game

Here's a more complex example where two agents play the word-guessing game Taboo.
One agent provides hints while avoiding forbidden words, and another agent tries to guess the target word:

```typescript
import { Node, Flow } from "../src/index";

// Define shared storage for the game
type SharedStorage = {
  targetWord: string;
  forbiddenWords: string[];
  pastGuesses: string[];
  hinterQueue: string[];
  guesserQueue: string[];
  gameOver: boolean;
};

// Utility function to simulate LLM call
function callLLM(prompt: string): string {
  // In a real implementation, this would call an actual LLM API
  console.log(`[LLM PROMPT]: ${prompt}`);

  // For demonstration, return predefined responses
  if (prompt.includes("Generate hint")) {
    if (prompt.includes("popsicle")) {
      return "When childhood cartoons make you emotional";
    }
    if (prompt.includes("nostalgic")) {
      return "When old songs move you";
    }
    if (prompt.includes("memories")) {
      return "That warm emotion about childhood";
    }
    return "Thinking of childhood summer days";
  } else if (prompt.includes("Given hint")) {
    if (prompt.includes("Thinking of childhood summer days")) {
      return "popsicle";
    }
    if (prompt.includes("When childhood cartoons make you emotional")) {
      return "nostalgic";
    }
    if (prompt.includes("When old songs move you")) {
      return "memories";
    }
    if (prompt.includes("That warm emotion about childhood")) {
      return "nostalgia";
    }
    return "unknown";
  }
  return "no response";
}

// Hinter agent that provides clues
class Hinter extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    if (shared.gameOver) {
      return null;
    }

    // Wait for a message in the hinter queue
    while (shared.hinterQueue.length === 0) {
      await new Promise((resolve) => setTimeout(resolve, 100));
      if (shared.gameOver) return null;
    }

    const guess = shared.hinterQueue.shift();
    if (guess === "GAME_OVER") {
      shared.gameOver = true;
      return null;
    }

    return {
      target: shared.targetWord,
      forbidden: shared.forbiddenWords,
      pastGuesses: shared.pastGuesses,
    };
  }

  async exec(inputs: any): Promise<string | null> {
    if (inputs === null) {
      return null;
    }

    const { target, forbidden, pastGuesses } = inputs;
    let prompt = `Generate hint for '${target}'\nForbidden words: ${forbidden.join(
      ", "
    )}`;

    if (pastGuesses && pastGuesses.length > 0) {
      prompt += `\nPrevious wrong guesses: ${pastGuesses.join(
        ", "
      )}\nMake hint more specific.`;
    }

    prompt += "\nUse at most 5 words.";

    const hint = callLLM(prompt);
    console.log(`\nHinter: Here's your hint - ${hint}`);
    return hint;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: string | null
  ): Promise<string> {
    if (execRes === null) {
      return "end";
    }

    // Send the hint to the guesser
    shared.guesserQueue.push(execRes);
    return "continue";
  }
}

// Guesser agent that tries to guess the word
class Guesser extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    if (shared.gameOver) {
      return null;
    }

    // Wait for a hint in the guesser queue
    while (shared.guesserQueue.length === 0) {
      await new Promise((resolve) => setTimeout(resolve, 100));
      if (shared.gameOver) return null;
    }

    const hint = shared.guesserQueue.shift();
    return {
      hint,
      pastGuesses: shared.pastGuesses,
    };
  }

  async exec(inputs: any): Promise<string | null> {
    if (inputs === null) {
      return null;
    }

    const { hint, pastGuesses } = inputs;
    const prompt = `Given hint: ${hint}, past wrong guesses: ${pastGuesses.join(
      ", "
    )}, make a new guess. Directly reply a single word:`;

    const guess = callLLM(prompt);
    console.log(`Guesser: I guess it's - ${guess}`);
    return guess;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: string | null
  ): Promise<string> {
    if (execRes === null) {
      return "end";
    }

    if (execRes.toLowerCase() === shared.targetWord.toLowerCase()) {
      console.log("Game Over - Correct guess!");
      shared.gameOver = true;
      shared.hinterQueue.push("GAME_OVER");
      return "end";
    }

    // Add to past guesses
    shared.pastGuesses.push(execRes);

    // Send the guess to the hinter
    shared.hinterQueue.push(execRes);
    return "continue";
  }
}

async function main() {
  // Set up game
  const shared: SharedStorage = {
    targetWord: "nostalgia",
    forbiddenWords: ["memory", "past", "remember", "feeling", "longing"],
    pastGuesses: [],
    hinterQueue: [],
    guesserQueue: [],
    gameOver: false,
  };

  console.log("Game starting!");
  console.log(`Target word: ${shared.targetWord}`);
  console.log(`Forbidden words: ${shared.forbiddenWords.join(", ")}`);

  // Initialize by sending empty guess to hinter
  shared.hinterQueue.push("");

  // Create agents
  const hinter = new Hinter();
  const guesser = new Guesser();

  // Set up flows
  hinter.on("continue", hinter);
  guesser.on("continue", guesser);

  const hinterFlow = new Flow(hinter);
  const guesserFlow = new Flow(guesser);

  // Run both agents
  await Promise.all([hinterFlow.run(shared), guesserFlow.run(shared)]);
}

main().catch(console.error);
```

The Output:

```
Game starting!
Target word: nostalgia
Forbidden words: memory, past, remember, feeling, longing

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Use at most 5 words.

Hinter: Here's your hint - Thinking of childhood summer days

[LLM PROMPT]: Given hint: Thinking of childhood summer days, past wrong guesses: , make a new guess. Directly reply a single word:
Guesser: I guess it's - popsicle

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Previous wrong guesses: popsicle
Make hint more specific.
Use at most 5 words.

Hinter: Here's your hint - When childhood cartoons make you emotional

[LLM PROMPT]: Given hint: When childhood cartoons make you emotional, past wrong guesses: popsicle, make a new guess. Directly reply a single word:
Guesser: I guess it's - nostalgic

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Previous wrong guesses: popsicle, nostalgic
Make hint more specific.
Use at most 5 words.

Hinter: Here's your hint - When old songs move you

[LLM PROMPT]: Given hint: When old songs move you, past wrong guesses: popsicle, nostalgic, make a new guess. Directly reply a single word:
Guesser: I guess it's - memories

[LLM PROMPT]: Generate hint for 'nostalgia'
Forbidden words: memory, past, remember, feeling, longing
Previous wrong guesses: popsicle, nostalgic, memories
Make hint more specific.
Use at most 5 words.

Hinter: Here's your hint - That warm emotion about childhood

[LLM PROMPT]: Given hint: That warm emotion about childhood, past wrong guesses: popsicle, nostalgic, memories, make a new guess. Directly reply a single word:
Guesser: I guess it's - nostalgia
Game Over - Correct guess!
```
</file>

<file path="docs/design_pattern/rag.md">
---
layout: default
title: "RAG"
parent: "Design Pattern"
nav_order: 3
---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---

## Stage 1: Offline Indexing

We create three Nodes:

1. `ChunkDocs` – [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` – [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` – stores embeddings into a [vector database](../utility_function/vector.md).

```typescript
type SharedStore = {
  files?: string[];
  allChunks?: string[];
  allEmbeds?: number[][];
  index?: any;
};

class ChunkDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.files || [];
  }

  async exec(filepath: string): Promise<string[]> {
    const text = fs.readFileSync(filepath, "utf-8");
    // Simplified chunking for example
    const chunks: string[] = [];
    const size = 100;
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.substring(i, i + size));
    }
    return chunks;
  }

  async post(
    shared: SharedStore,
    _: string[],
    chunks: string[][]
  ): Promise<undefined> {
    shared.allChunks = chunks.flat();
    return undefined;
  }
}

class EmbedDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.allChunks || [];
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk);
  }

  async post(
    shared: SharedStore,
    _: string[],
    embeddings: number[][]
  ): Promise<undefined> {
    shared.allEmbeds = embeddings;
    return undefined;
  }
}

class StoreIndex extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<number[][]> {
    return shared.allEmbeds || [];
  }

  async exec(allEmbeds: number[][]): Promise<unknown> {
    return await createIndex(allEmbeds);
  }

  async post(
    shared: SharedStore,
    _: number[][],
    index: unknown
  ): Promise<undefined> {
    shared.index = index;
    return undefined;
  }
}

// Create indexing flow
const chunkNode = new ChunkDocs();
const embedNode = new EmbedDocs();
const storeNode = new StoreIndex();

chunkNode.next(embedNode).next(storeNode);
const offlineFlow = new Flow(chunkNode);
```

---

## Stage 2: Online Query & Answer

We have 3 nodes:

1. `EmbedQuery` – embeds the user's question.
2. `RetrieveDocs` – retrieves top chunk from the index.
3. `GenerateAnswer` – calls the LLM with the question + chunk to produce the final answer.

```typescript
type OnlineStore = SharedStore & {
  question?: string;
  qEmb?: number[];
  retrievedChunk?: string;
  answer?: string;
};

class EmbedQuery extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<string> {
    return shared.question || "";
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question);
  }

  async post(
    shared: OnlineStore,
    _: string,
    qEmb: number[]
  ): Promise<undefined> {
    shared.qEmb = qEmb;
    return undefined;
  }
}

class RetrieveDocs extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[number[], any, string[]]> {
    return [shared.qEmb || [], shared.index, shared.allChunks || []];
  }

  async exec([qEmb, index, chunks]: [
    number[],
    any,
    string[]
  ]): Promise<string> {
    const [ids] = await searchIndex(index, qEmb, { topK: 1 });
    return chunks[ids[0][0]];
  }

  async post(
    shared: OnlineStore,
    _: [number[], any, string[]],
    chunk: string
  ): Promise<undefined> {
    shared.retrievedChunk = chunk;
    return undefined;
  }
}

class GenerateAnswer extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[string, string]> {
    return [shared.question || "", shared.retrievedChunk || ""];
  }

  async exec([question, chunk]: [string, string]): Promise<string> {
    return await callLlm(`Question: ${question}\nContext: ${chunk}\nAnswer:`);
  }

  async post(
    shared: OnlineStore,
    _: [string, string],
    answer: string
  ): Promise<undefined> {
    shared.answer = answer;
    return undefined;
  }
}

// Create query flow
const embedQNode = new EmbedQuery();
const retrieveNode = new RetrieveDocs();
const generateNode = new GenerateAnswer();

embedQNode.next(retrieveNode).next(generateNode);
const onlineFlow = new Flow(embedQNode);
```

Usage example:

```typescript
const shared = {
  files: ["doc1.txt", "doc2.txt"], // any text files
};
await offlineFlow.run(shared);
```
</file>

<file path="docs/design_pattern/structure.md">
---
layout: default
title: "Structured Output"
parent: "Design Pattern"
nav_order: 5
---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:

- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

## TypeScript Implementation

When using PocketFlow with structured output, follow these TypeScript patterns:

1. **Define Types** for your structured input/output
2. **Implement Validation** in your Node methods
3. **Use Type-Safe Operations** throughout your flow

### Example Text Summarization

````typescript
// Define types
type SummaryResult = {
  summary: string[];
};

type SharedStorage = {
  text?: string;
  result?: SummaryResult;
};

class SummarizeNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    return shared.text;
  }

  async exec(text: string | undefined): Promise<SummaryResult> {
    if (!text) return { summary: ["No text provided"] };

    const prompt = `
Please summarize the following text as YAML, with exactly 3 bullet points

${text}

Output:
\`\`\`yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
\`\`\``;

    // Simulated LLM call
    const response =
      "```yaml\nsummary:\n  - First point\n  - Second insight\n  - Final conclusion\n```";

    // Parse YAML response
    const yamlStr = response.split("```yaml")[1].split("```")[0].trim();

    // Extract bullet points
    const result: SummaryResult = {
      summary: yamlStr
        .split("\n")
        .filter((line) => line.trim().startsWith("- "))
        .map((line) => line.trim().substring(2)),
    };

    // Validate
    if (!result.summary || !Array.isArray(result.summary)) {
      throw new Error("Invalid summary structure");
    }

    return result;
  }

  async post(
    shared: SharedStorage,
    _: string | undefined,
    result: SummaryResult
  ): Promise<string | undefined> {
    shared.result = result;
    return "default";
  }
}
````

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

**In YAML**

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```
</file>

<file path="docs/design_pattern/workflow.md">
---
layout: default
title: "Workflow"
parent: "Design Pattern"
nav_order: 2
---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - You don't want to make each task **too coarse**, because it may be _too complex for one LLM call_.
> - You don't want to make each task **too granular**, because then _the LLM call doesn't have enough context_ and results are _not consistent across nodes_.
>
> You usually need multiple _iterations_ to find the _sweet spot_. If the task has too many _edge cases_, consider using [Agents](./agent.md).
> {: .best-practice }

### Example: Article Writing

```typescript
interface SharedState {
  topic?: string;
  outline?: string;
  draft?: string;
  final_article?: string;
}

// Helper function to simulate LLM call
async function callLLM(prompt: string): Promise<string> {
  return `Response to: ${prompt}`;
}

class GenerateOutline extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.topic || "";
  }

  async exec(topic: string): Promise<string> {
    return await callLLM(
      `Create a detailed outline for an article about ${topic}`
    );
  }

  async post(shared: SharedState, _: string, outline: string): Promise<string> {
    shared.outline = outline;
    return "default";
  }
}

class WriteSection extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.outline || "";
  }

  async exec(outline: string): Promise<string> {
    return await callLLM(`Write content based on this outline: ${outline}`);
  }

  async post(shared: SharedState, _: string, draft: string): Promise<string> {
    shared.draft = draft;
    return "default";
  }
}

class ReviewAndRefine extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.draft || "";
  }

  async exec(draft: string): Promise<string> {
    return await callLLM(`Review and improve this draft: ${draft}`);
  }

  async post(
    shared: SharedState,
    _: string,
    final: string
  ): Promise<undefined> {
    shared.final_article = final;
    return undefined;
  }
}

// Connect nodes in sequence
const outline = new GenerateOutline();
const write = new WriteSection();
const review = new ReviewAndRefine();

outline.next(write).next(review);

// Create and run flow
const writingFlow = new Flow(outline);
writingFlow.run({ topic: "AI Safety" });
```

For _dynamic cases_, consider using [Agents](./agent.md).
</file>

<file path="docs/utility_function/chunking.md">
---
layout: default
title: "Text Chunking"
parent: "Utility Function"
nav_order: 4
---

# Text Chunking

We recommend some implementations of commonly used text chunking approaches.

> Text Chunking is more a micro optimization, compared to the Flow Design.
>
> It's recommended to start with the Naive Chunking and optimize later.
> {: .best-practice }

---

## Example TypeScript Code Samples

### 1. Naive (Fixed-Size) Chunking

Splits text by a fixed number of characters, ignoring sentence or semantic boundaries.

```typescript
function fixedSizeChunk(text: string, chunkSize: number = 100): string[] {
  const chunks: string[] = [];
  for (let i = 0; i < text.length; i += chunkSize) {
    chunks.push(text.substring(i, i + chunkSize));
  }
  return chunks;
}
```

However, sentences are often cut awkwardly, losing coherence.

### 2. Sentence-Based Chunking

Using the popular [compromise](https://github.com/spencermountain/compromise) library (11.6k+ GitHub stars).

```typescript
import nlp from "compromise";

function sentenceBasedChunk(text: string, maxSentences: number = 2): string[] {
  // Parse the text into sentences
  const doc = nlp(text);
  const sentences = doc.sentences().out("array");

  // Group sentences into chunks
  const chunks: string[] = [];
  for (let i = 0; i < sentences.length; i += maxSentences) {
    chunks.push(sentences.slice(i, i + maxSentences).join(" "));
  }

  return chunks;
}
```

However, might not handle very long sentences or paragraphs well.

### 3. Other Chunking

- **Paragraph-Based**: Split text by paragraphs (e.g., newlines). Large paragraphs can create big chunks.

```typescript
function paragraphBasedChunk(text: string): string[] {
  // Split by double newlines (paragraphs)
  return text
    .split(/\n\s*\n/)
    .filter((paragraph) => paragraph.trim().length > 0);
}
```

- **Semantic**: Use embeddings or topic modeling to chunk by semantic boundaries.
- **Agentic**: Use an LLM to decide chunk boundaries based on context or meaning.
</file>

<file path="docs/utility_function/embedding.md">
---
layout: default
title: "Embedding"
parent: "Utility Function"
nav_order: 5
---

# Embedding

Below you will find an overview table of various text embedding APIs, along with example TypeScript code.

> Embedding is more a micro optimization, compared to the Flow Design.
>
> It's recommended to start with the most convenient one and optimize later.
> {: .best-practice }

| **API**              | **Free Tier**                           | **Pricing Model**                   | **Docs**                                                                                                                  |
| -------------------- | --------------------------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **OpenAI**           | ~$5 credit                              | ~$0.0001/1K tokens                  | [OpenAI Embeddings](https://platform.openai.com/docs/api-reference/embeddings)                                            |
| **Azure OpenAI**     | $200 credit                             | Same as OpenAI (~$0.0001/1K tokens) | [Azure OpenAI Embeddings](https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?tabs=portal) |
| **Google Vertex AI** | $300 credit                             | ~$0.025 / million chars             | [Vertex AI Embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)              |
| **AWS Bedrock**      | No free tier, but AWS credits may apply | ~$0.00002/1K tokens (Titan V2)      | [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/)                                                                    |
| **Cohere**           | Limited free tier                       | ~$0.0001/1K tokens                  | [Cohere Embeddings](https://docs.cohere.com/docs/cohere-embed)                                                            |
| **Hugging Face**     | ~$0.10 free compute monthly             | Pay per second of compute           | [HF Inference API](https://huggingface.co/docs/api-inference)                                                             |
| **Jina**             | 1M tokens free                          | Pay per token after                 | [Jina Embeddings](https://jina.ai/embeddings/)                                                                            |

## Example TypeScript Code

### 1. OpenAI

```typescript
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: "YOUR_API_KEY",
});

async function getEmbedding(text: string) {
  const response = await openai.embeddings.create({
    model: "text-embedding-ada-002",
    input: text,
  });

  // Extract the embedding vector from the response
  const embedding = response.data[0].embedding;
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 2. Azure OpenAI

```typescript
import { AzureOpenAI } from "openai";
import {
  getBearerTokenProvider,
  DefaultAzureCredential,
} from "@azure/identity";

// Using Azure credentials (recommended)
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Or using API key directly
const client = new AzureOpenAI({
  apiKey: "YOUR_AZURE_API_KEY",
  endpoint: "https://YOUR_RESOURCE_NAME.openai.azure.com",
  apiVersion: "2023-05-15", // Update to the latest version
});

async function getEmbedding(text: string) {
  const response = await client.embeddings.create({
    model: "text-embedding-ada-002", // Or your deployment name
    input: text,
  });

  const embedding = response.data[0].embedding;
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 3. Google Vertex AI

```typescript
import { VertexAI } from "@google-cloud/vertexai";

// Initialize Vertex with your Google Cloud project and location
const vertex = new VertexAI({
  project: "YOUR_GCP_PROJECT_ID",
  location: "us-central1",
});

// Access embeddings model
const model = vertex.preview.getTextEmbeddingModel("textembedding-gecko@001");

async function getEmbedding(text: string) {
  const response = await model.getEmbeddings({
    texts: [text],
  });

  const embedding = response.embeddings[0].values;
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 4. AWS Bedrock

```typescript
import {
  BedrockRuntimeClient,
  InvokeModelCommand,
} from "@aws-sdk/client-bedrock-runtime";

const client = new BedrockRuntimeClient({
  region: "us-east-1", // Use your AWS region
});

async function getEmbedding(text: string) {
  const modelId = "amazon.titan-embed-text-v2:0";
  const input = {
    inputText: text,
    dimensions: 1536, // Optional: specify embedding dimensions
    normalize: true, // Optional: normalize embeddings
  };

  const command = new InvokeModelCommand({
    modelId: modelId,
    contentType: "application/json",
    accept: "application/json",
    body: JSON.stringify(input),
  });

  const response = await client.send(command);
  const responseBody = JSON.parse(new TextDecoder().decode(response.body));
  const embedding = responseBody.embedding;

  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 5. Cohere

```typescript
import { CohereClient } from "cohere-ai";

const cohere = new CohereClient({
  token: "YOUR_API_KEY",
});

async function getEmbedding(text: string) {
  const response = await cohere.embed({
    texts: [text],
    model: "embed-english-v3.0", // Use the latest model
    inputType: "search_query",
  });

  const embedding = response.embeddings[0];
  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```

### 6. Hugging Face

```typescript
import { InferenceClient } from "@huggingface/inference";

const hf = new InferenceClient({
  apiToken: "YOUR_HF_TOKEN",
});

async function getEmbedding(text: string) {
  const model = "sentence-transformers/all-MiniLM-L6-v2";

  const response = await hf.featureExtraction({
    model: model,
    inputs: text,
  });

  console.log(response);
  return response;
}

// Usage
getEmbedding("Hello world");
```

### 7. Jina

```typescript
import axios from "axios";

async function getEmbedding(text: string) {
  const url = "https://api.jina.ai/v1/embeddings";
  const headers = {
    Authorization: `Bearer YOUR_JINA_TOKEN`,
    "Content-Type": "application/json",
  };

  const payload = {
    model: "jina-embeddings-v3",
    input: [text],
    normalized: true,
  };

  const response = await axios.post(url, payload, { headers });
  const embedding = response.data.data[0].embedding;

  console.log(embedding);
  return embedding;
}

// Usage
getEmbedding("Hello world");
```
</file>

<file path="docs/utility_function/index.md">
---
layout: default
title: "Utility Function"
nav_order: 4
has_children: true
---
</file>

<file path="docs/utility_function/llm.md">
---
layout: default
title: "LLM Wrapper"
parent: "Utility Function"
nav_order: 1
---

# LLM Wrappers

Check out popular libraries like [LangChain](https://github.com/langchain-ai/langchainjs) (13.8k+ GitHub stars), [ModelFusion](https://github.com/vercel/modelfusion) (1.2k+ GitHub stars), or [Firebase GenKit](https://firebase.google.com/docs/genkit) for unified LLM interfaces.
Here, we provide some minimal example implementations:

1. OpenAI

   ```typescript
   import { OpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
     const r = await client.chat.completions.create({
       model: "gpt-4o",
       messages: [{ role: "user", content: prompt }],
     });
     return r.choices[0].message.content || "";
   }

   // Example usage
   callLlm("How are you?").then(console.log);
   ```

   > Store the API key in an environment variable like OPENAI_API_KEY for security.
   > {: .best-practice }

2. Claude (Anthropic)

   ```typescript
   import Anthropic from "@anthropic-ai/sdk";

   async function callLlm(prompt: string): Promise<string> {
     const client = new Anthropic({
       apiKey: "YOUR_API_KEY_HERE",
     });
     const response = await client.messages.create({
       model: "claude-3-7-sonnet-20250219",
       max_tokens: 3000,
       messages: [{ role: "user", content: prompt }],
     });
     return response.content[0].text;
   }
   ```

3. Google (Vertex AI)

   ```typescript
   import { VertexAI } from "@google-cloud/vertexai";

   async function callLlm(prompt: string): Promise<string> {
     const vertexAI = new VertexAI({
       project: "YOUR_PROJECT_ID",
       location: "us-central1",
     });

     const generativeModel = vertexAI.getGenerativeModel({
       model: "gemini-1.5-flash",
     });

     const response = await generativeModel.generateContent({
       contents: [{ role: "user", parts: [{ text: prompt }] }],
     });

     return response.response.candidates[0].content.parts[0].text;
   }
   ```

4. Azure (Azure OpenAI)

   ```typescript
   import { AzureOpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new AzureOpenAI({
       apiKey: "YOUR_API_KEY_HERE",
       azure: {
         apiVersion: "2023-05-15",
         endpoint: "https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
       },
     });

     const r = await client.chat.completions.create({
       model: "<YOUR_DEPLOYMENT_NAME>",
       messages: [{ role: "user", content: prompt }],
     });

     return r.choices[0].message.content || "";
   }
   ```

5. Ollama (Local LLM)

   ```typescript
   import ollama from "ollama";

   async function callLlm(prompt: string): Promise<string> {
     const response = await ollama.chat({
       model: "llama2",
       messages: [{ role: "user", content: prompt }],
     });
     return response.message.content;
   }
   ```

## Improvements

Feel free to enhance your `callLlm` function as needed. Here are examples:

- Handle chat history:

```typescript
interface Message {
  role: "user" | "assistant" | "system";
  content: string;
}

async function callLlm(messages: Message[]): Promise<string> {
  const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
  const r = await client.chat.completions.create({
    model: "gpt-4o",
    messages: messages,
  });
  return r.choices[0].message.content || "";
}
```

- Add in-memory caching

```typescript
import { memoize } from "lodash";

const callLlmMemoized = memoize(async (prompt: string): Promise<string> => {
  // Your implementation here
  return "";
});

async function callLlm(prompt: string, useCache = true): Promise<string> {
  if (useCache) {
    return callLlmMemoized(prompt);
  }
  // Call the underlying function directly
  return callLlmInternal(prompt);
}

class SummarizeNode {
  private curRetry = 0;

  async exec(text: string): Promise<string> {
    return callLlm(`Summarize: ${text}`, this.curRetry === 0);
  }
}
```

- Enable logging:

```typescript
async function callLlm(prompt: string): Promise<string> {
  console.info(`Prompt: ${prompt}`);
  // Your implementation here
  const response = ""; // Response from your implementation
  console.info(`Response: ${response}`);
  return response;
}
```
</file>

<file path="docs/utility_function/text_to_speech.md">
---
layout: default
title: "Text-to-Speech"
parent: "Utility Function"
nav_order: 7
---

# Text-to-Speech

| **Service**          | **Free Tier**       | **Pricing Model**                                            | **Docs**                                                                                  |
| -------------------- | ------------------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------- |
| **Amazon Polly**     | 5M std + 1M neural  | ~$4 /M (std), ~$16 /M (neural) after free tier               | [Polly Docs](https://aws.amazon.com/polly/)                                               |
| **Google Cloud TTS** | 4M std + 1M WaveNet | ~$4 /M (std), ~$16 /M (WaveNet) pay-as-you-go                | [Cloud TTS Docs](https://cloud.google.com/text-to-speech)                                 |
| **Azure TTS**        | 500K neural ongoing | ~$15 /M (neural), discount at higher volumes                 | [Azure TTS Docs](https://azure.microsoft.com/products/cognitive-services/text-to-speech/) |
| **IBM Watson TTS**   | 10K chars Lite plan | ~$0.02 /1K (i.e. ~$20 /M). Enterprise options available      | [IBM Watson Docs](https://www.ibm.com/cloud/watson-text-to-speech)                        |
| **ElevenLabs**       | 10K chars monthly   | From ~$5/mo (30K chars) up to $330/mo (2M chars). Enterprise | [ElevenLabs Docs](https://elevenlabs.io)                                                  |

## Example TypeScript Code

### Amazon Polly

```typescript
import { PollyClient, SynthesizeSpeechCommand } from "@aws-sdk/client-polly";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Create a Polly client
  const polly = new PollyClient({
    region: "us-east-1",
    credentials: {
      accessKeyId: "YOUR_AWS_ACCESS_KEY_ID",
      secretAccessKey: "YOUR_AWS_SECRET_ACCESS_KEY",
    },
  });

  // Set the parameters
  const params = {
    Text: "Hello from Polly!",
    OutputFormat: "mp3",
    VoiceId: "Joanna",
  };

  try {
    // Synthesize speech
    const command = new SynthesizeSpeechCommand(params);
    const response = await polly.send(command);

    // Convert AudioStream to Buffer and save to file
    if (response.AudioStream) {
      const audioBuffer = Buffer.from(
        await response.AudioStream.transformToByteArray()
      );
      writeFileSync("polly.mp3", audioBuffer);
      console.log("Audio content written to file: polly.mp3");
    }
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### Google Cloud TTS

```typescript
import { TextToSpeechClient } from "@google-cloud/text-to-speech";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Creates a client
  const client = new TextToSpeechClient();

  // The text to synthesize
  const text = "Hello from Google Cloud TTS!";

  // Construct the request
  const request = {
    input: { text: text },
    // Select the language and SSML voice gender
    voice: { languageCode: "en-US", ssmlGender: "NEUTRAL" },
    // Select the type of audio encoding
    audioConfig: { audioEncoding: "MP3" },
  };

  try {
    // Performs the text-to-speech request
    const [response] = await client.synthesizeSpeech(request);
    // Write the binary audio content to a local file
    writeFileSync("gcloud_tts.mp3", response.audioContent as Buffer);
    console.log("Audio content written to file: gcloud_tts.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### Azure TTS

```typescript
import {
  SpeechConfig,
  AudioConfig,
  SpeechSynthesizer,
} from "microsoft-cognitiveservices-speech-sdk";

async function synthesizeText() {
  // Create a speech configuration with subscription information
  const speechConfig = SpeechConfig.fromSubscription(
    "AZURE_KEY",
    "AZURE_REGION"
  );

  // Set speech synthesis output format
  speechConfig.speechSynthesisOutputFormat = 1; // 1 corresponds to Audio16Khz128KBitRateMonoMp3

  // Create an audio configuration for file output
  const audioConfig = AudioConfig.fromAudioFileOutput("azure_tts.mp3");

  // Create a speech synthesizer with the given configurations
  const synthesizer = new SpeechSynthesizer(speechConfig, audioConfig);

  try {
    // Synthesize text to speech
    const result = await new Promise((resolve, reject) => {
      synthesizer.speakTextAsync(
        "Hello from Azure TTS!",
        (result) => {
          synthesizer.close();
          resolve(result);
        },
        (error) => {
          synthesizer.close();
          reject(error);
        }
      );
    });

    console.log("Audio content written to file: azure_tts.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### IBM Watson TTS

```typescript
import { TextToSpeechV1 } from "ibm-watson/text-to-speech/v1";
import { IamAuthenticator } from "ibm-watson/auth";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Create a TextToSpeech client with authentication
  const textToSpeech = new TextToSpeechV1({
    authenticator: new IamAuthenticator({ apikey: "IBM_API_KEY" }),
    serviceUrl: "IBM_SERVICE_URL",
  });

  try {
    // Synthesize speech
    const params = {
      text: "Hello from IBM Watson!",
      voice: "en-US_AllisonV3Voice",
      accept: "audio/mp3",
    };

    const response = await textToSpeech.synthesize(params);
    const audio = response.result;

    // The wav header requires a file length, but this is unknown until after the header is already generated
    const repairedAudio = await textToSpeech.repairWavHeaderStream(audio);

    // Write audio to file
    writeFileSync("ibm_tts.mp3", repairedAudio);
    console.log("Audio content written to file: ibm_tts.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```

### ElevenLabs

```typescript
import { ElevenLabs } from "elevenlabs";
import { writeFileSync } from "fs";

async function synthesizeText() {
  // Initialize the ElevenLabs client
  const eleven = new ElevenLabs({
    apiKey: "ELEVENLABS_KEY",
  });

  try {
    // Generate speech
    const voiceId = "ELEVENLABS_VOICE_ID";
    const text = "Hello from ElevenLabs!";

    // Generate audio
    const audioResponse = await eleven.generate({
      voice: voiceId,
      text: text,
      model_id: "eleven_monolingual_v1",
      voice_settings: {
        stability: 0.75,
        similarity_boost: 0.75,
      },
    });

    // Convert to buffer and save to file
    const audioBuffer = Buffer.from(await audioResponse.arrayBuffer());
    writeFileSync("elevenlabs.mp3", audioBuffer);
    console.log("Audio content written to file: elevenlabs.mp3");
  } catch (error) {
    console.error("Error:", error);
  }
}

synthesizeText();
```
</file>

<file path="docs/utility_function/vector.md">
---
layout: default
title: "Vector Databases"
parent: "Utility Function"
nav_order: 6
---

# Vector Databases

Below is a table of the popular vector search solutions:

| **Tool**     | **Free Tier**  | **Pricing Model**        | **Docs**                               |
| ------------ | -------------- | ------------------------ | -------------------------------------- |
| **FAISS**    | N/A, self-host | Open-source              | [Faiss.ai](https://faiss.ai)           |
| **Pinecone** | 2GB free       | From $25/mo              | [pinecone.io](https://pinecone.io)     |
| **Qdrant**   | 1GB free cloud | Pay-as-you-go            | [qdrant.tech](https://qdrant.tech)     |
| **Weaviate** | 14-day sandbox | From $25/mo              | [weaviate.io](https://weaviate.io)     |
| **Milvus**   | 5GB free cloud | PAYG or $99/mo dedicated | [milvus.io](https://milvus.io)         |
| **Chroma**   | N/A, self-host | Free (Apache 2.0)        | [trychroma.com](https://trychroma.com) |
| **Redis**    | 30MB free      | From $5/mo               | [redis.io](https://redis.io)           |

---

## Example TypeScript Code

Below are basic usage snippets for each tool.

### FAISS

```typescript
import { IndexFlatL2 } from "faiss-node";

// Dimensionality of embeddings
const dimension = 128;

// Create a flat L2 index
const index = new IndexFlatL2(dimension);

// Create random vectors (using standard JS arrays)
const data: number[] = [];
for (let i = 0; i < 1000; i++) {
  for (let j = 0; j < dimension; j++) {
    data.push(Math.random());
  }
}

// Add vectors to the index
for (let i = 0; i < 1000; i++) {
  const vector = data.slice(i * dimension, (i + 1) * dimension);
  index.add(vector);
}

// Query
const query = Array(dimension)
  .fill(0)
  .map(() => Math.random());
const results = index.search(query, 5);

console.log("Distances:", results.distances);
console.log("Neighbors:", results.labels);
```

### Pinecone

```typescript
import { PineconeClient } from "@pinecone-database/pinecone";

// Define interface for your metadata (optional)
interface Metadata {
  type: string;
}

const init = async () => {
  // Initialize the client
  const pinecone = new PineconeClient();
  await pinecone.init({
    apiKey: "YOUR_API_KEY",
    environment: "YOUR_ENVIRONMENT",
  });

  const indexName = "my-index";

  // List indexes to check if it exists
  const indexes = await pinecone.listIndexes();

  // Create index if it doesn't exist
  if (!indexes.includes(indexName)) {
    await pinecone.createIndex({
      name: indexName,
      dimension: 128,
      metric: "cosine",
    });
  }

  // Connect to the index
  const index = pinecone.Index(indexName);

  // Upsert vectors
  await index.upsert({
    upsertRequest: {
      vectors: [
        {
          id: "id1",
          values: Array(128).fill(0.1),
          metadata: { type: "doc1" } as Metadata,
        },
        {
          id: "id2",
          values: Array(128).fill(0.2),
          metadata: { type: "doc2" } as Metadata,
        },
      ],
      namespace: "example-namespace",
    },
  });

  // Query
  const queryResult = await index.query({
    queryRequest: {
      vector: Array(128).fill(0.15),
      topK: 3,
      includeMetadata: true,
      namespace: "example-namespace",
    },
  });

  console.log(queryResult);
};

init();
```

### Qdrant

```typescript
import { QdrantClient } from "@qdrant/js-client-rest";

const init = async () => {
  const client = new QdrantClient({
    url: "https://YOUR-QDRANT-CLOUD-ENDPOINT",
    apiKey: "YOUR_API_KEY",
  });

  const collectionName = "my_collection";

  // Create or recreate collection
  await client.recreateCollection(collectionName, {
    vectors: {
      size: 128,
      distance: "Cosine",
    },
  });

  // Insert points
  await client.upsert(collectionName, {
    wait: true,
    points: [
      {
        id: "1",
        vector: Array(128).fill(0.1),
        payload: { type: "doc1" },
      },
      {
        id: "2",
        vector: Array(128).fill(0.2),
        payload: { type: "doc2" },
      },
    ],
  });

  // Search
  const searchResult = await client.search(collectionName, {
    vector: Array(128).fill(0.15),
    limit: 2,
  });

  console.log(searchResult);
};

init();
```

### Weaviate

```typescript
import weaviate from "weaviate-client";

const init = async () => {
  // Connect to Weaviate
  const client = weaviate.client({
    scheme: "https",
    host: "YOUR-WEAVIATE-CLOUD-ENDPOINT",
  });

  // Create schema
  const schema = {
    classes: [
      {
        class: "Article",
        vectorizer: "none",
      },
    ],
  };

  await client.schema.create(schema);

  // Add data
  await client.data
    .creator()
    .withClassName("Article")
    .withProperties({
      title: "Hello World",
      content: "Weaviate vector search",
    })
    .withVector(Array(128).fill(0.1))
    .do();

  // Query
  const result = await client.graphql
    .get()
    .withClassName("Article")
    .withFields(["title", "content"])
    .withNearVector({
      vector: Array(128).fill(0.15),
    })
    .withLimit(3)
    .do();

  console.log(result);
};

init();
```

### Milvus

```typescript
import { MilvusClient, DataType } from "@zilliz/milvus2-sdk-node";

const init = async () => {
  // Connect to Milvus
  const client = new MilvusClient("localhost:19530");

  // Wait for connection to be established
  await client.connectPromise;

  const collectionName = "MyCollection";

  // Create collection
  await client.createCollection({
    collection_name: collectionName,
    fields: [
      {
        name: "id",
        data_type: DataType.Int64,
        is_primary_key: true,
        autoID: true,
      },
      {
        name: "embedding",
        data_type: DataType.FloatVector,
        dim: 128,
      },
    ],
  });

  // Create random vectors
  const vectors = [];
  for (let i = 0; i < 10; i++) {
    vectors.push(
      Array(128)
        .fill(0)
        .map(() => Math.random())
    );
  }

  // Insert data
  await client.insert({
    collection_name: collectionName,
    fields_data: vectors.map((vector) => ({
      embedding: vector,
    })),
  });

  // Create index
  await client.createIndex({
    collection_name: collectionName,
    field_name: "embedding",
    extra_params: {
      index_type: "IVF_FLAT",
      metric_type: "L2",
      params: JSON.stringify({ nlist: 128 }),
    },
  });

  // Load collection to memory
  await client.loadCollection({
    collection_name: collectionName,
  });

  // Search
  const searchResult = await client.search({
    collection_name: collectionName,
    vector: Array(128)
      .fill(0)
      .map(() => Math.random()),
    search_params: {
      anns_field: "embedding",
      topk: 3,
      metric_type: "L2",
      params: JSON.stringify({ nprobe: 10 }),
    },
  });

  console.log(searchResult);
};

init();
```

### Chroma

```typescript
import { ChromaClient, Collection } from "chromadb";

const init = async () => {
  const client = new ChromaClient({
    path: "http://localhost:8000", // Default path if using chroma server
  });

  // Create or get collection
  const collection: Collection = await client.createCollection({
    name: "my_collection",
    // Optional metadata
    metadata: { description: "My test collection" },
  });

  // Add vectors
  await collection.add({
    ids: ["id1", "id2"],
    embeddings: [
      [0.1, 0.2, 0.3],
      [0.2, 0.2, 0.2],
    ],
    metadatas: [{ doc: "text1" }, { doc: "text2" }],
  });

  // Query
  const result = await collection.query({
    queryEmbeddings: [[0.15, 0.25, 0.3]],
    nResults: 2,
  });

  console.log(result);
};

init();
```

### Redis

```typescript
import { createClient } from "redis";
import { SchemaFieldTypes, VectorAlgorithms } from "@redis/search";

const init = async () => {
  // Connect to Redis
  const client = createClient({
    url: "redis://localhost:6379",
  });

  await client.connect();

  const indexName = "my_idx";

  // Create index for vector search
  try {
    await client.ft.create(
      indexName,
      {
        embedding: {
          type: SchemaFieldTypes.VECTOR,
          ALGORITHM: VectorAlgorithms.FLAT,
          TYPE: "FLOAT32",
          DIM: 128,
          DISTANCE_METRIC: "L2",
        },
      },
      {
        ON: "HASH",
      }
    );
  } catch (e) {
    console.log("Index might exist already");
  }

  // Create a Float32Array for the vectors
  const createVector = (value: number): Buffer => {
    const vector = new Float32Array(128).fill(value);
    return Buffer.from(vector.buffer);
  };

  // Insert
  await client.hSet("doc1", {
    embedding: createVector(0.1),
  });

  // Search
  const searchResults = await client.ft.search(
    indexName,
    "*=>[KNN 3 @embedding $BLOB AS dist]",
    {
      PARAMS: {
        BLOB: createVector(0.15),
      },
      RETURN: ["dist"],
      DIALECT: 2,
    }
  );

  console.log(searchResults);

  await client.quit();
};

init();
```
</file>

<file path="docs/utility_function/viz.md">
---
layout: default
title: "Viz and Debug"
parent: "Utility Function"
nav_order: 2
---

# Visualization and Debugging

Similar to LLM wrappers, we **don't** provide built-in visualization and debugging. Here, we recommend some _minimal_ (and incomplete) implementations These examples can serve as a starting point for your own tooling.

## 1. Visualization with Mermaid

### Using the Official Mermaid Library (Recommended)

The [mermaid library](https://github.com/mermaid-js/mermaid) is a very popular (76K+ stars) TypeScript library for creating diagrams from text. We recommend using it for visualization in production projects.

First, install the library:

```bash
npm install mermaid
# or
yarn add mermaid
# or
pnpm add mermaid
```

Then, you can use it to generate diagrams from your flow structure:

```typescript
import { BaseNode, Flow } from "../src/index";
import mermaid from "mermaid";

function generateFlowDiagram(start: BaseNode): string {
  // Configure mermaid
  mermaid.initialize({
    startOnLoad: false,
    theme: "default",
  });

  // Generate mermaid diagram definition
  const diagram = buildMermaidDefinition(start);

  // You can then render this diagram in a web context
  // or export it to SVG/PNG using mermaid's rendering capabilities
  return diagram;
}

// Helper function to build mermaid definition
function buildMermaidDefinition(start: BaseNode): string {
  // This is where we'll put our custom code to traverse the flow
  // and build a mermaid diagram definition
  // The implementation is similar to our custom approach below
  // [Implementation of custom traversal to generate mermaid diagram definition]
  // For a simple implementation example, see the custom approach below
  // The function returns a string containing the mermaid diagram definition
  // which can be rendered by the mermaid library
}
```

For rendering in a browser environment:

```typescript
// In a web context
const element = document.querySelector("#diagram");
mermaid.render("diagram-id", diagramDefinition, (svgCode) => {
  element.innerHTML = svgCode;
});
```

### Custom Implementation Alternative

If you prefer a custom lightweight implementation, this code recursively traverses the nested graph, assigns unique IDs to each node, and treats Flow nodes as subgraphs to generate Mermaid syntax for a hierarchical visualization:

{% raw %}

```typescript
import { BaseNode, Flow } from "../src/index";

type NodeIds = Map<BaseNode, string>;

function buildMermaid(start: BaseNode): string {
  const ids: NodeIds = new Map();
  const visited: Set<BaseNode> = new Set();
  const lines: string[] = ["graph LR"];
  let ctr = 1;

  function getId(node: BaseNode): string {
    if (ids.has(node)) {
      return ids.get(node)!;
    }
    const id = `N${ctr++}`;
    ids.set(node, id);
    return id;
  }

  function link(a: string, b: string): void {
    lines.push(`    ${a} --> ${b}`);
  }

  function walk(node: BaseNode, parent?: string): void {
    if (visited.has(node)) {
      if (parent) link(parent, getId(node));
      return;
    }

    visited.add(node);

    if (node instanceof Flow) {
      if (node.start && parent) {
        link(parent, getId(node.start));
      }

      lines.push(
        `\n    subgraph sub_flow_${getId(node)}[${node.constructor.name}]`
      );

      if (node.start) {
        walk(node.start);
      }

      // Get all successors from the node's internal map
      const successors = Array.from((node as any)._successors.entries());

      for (const [_, nextNode] of successors) {
        if (node.start) {
          walk(nextNode, getId(node.start));
        } else if (parent) {
          link(parent, getId(nextNode));
        } else {
          walk(nextNode);
        }
      }

      lines.push("    end\n");
    } else {
      const nodeId = getId(node);
      lines.push(`    ${nodeId}['${node.constructor.name}']`);

      if (parent) {
        link(parent, nodeId);
      }

      // Get all successors from the node's internal map
      const successors = Array.from((node as any)._successors.entries());

      for (const [_, nextNode] of successors) {
        walk(nextNode, nodeId);
      }
    }
  }

  walk(start);
  return lines.join("\n");
}
```

{% endraw %}

For example, suppose we have a complex Flow for data science:

```typescript
import { BaseNode, BatchNode, Node, Flow } from "../src/index";

class DataPrepBatchNode extends BatchNode {
  prep(shared: any): any[] {
    return [];
  }
}
class ValidateDataNode extends Node {}
class FeatureExtractionNode extends Node {}
class TrainModelNode extends Node {}
class EvaluateModelNode extends Node {}
class ModelFlow extends Flow {}
class DataScienceFlow extends Flow {}

const featureNode = new FeatureExtractionNode();
const trainNode = new TrainModelNode();
const evaluateNode = new EvaluateModelNode();
featureNode.next(trainNode);
trainNode.next(evaluateNode);
const modelFlow = new ModelFlow(featureNode);

const dataPrepNode = new DataPrepBatchNode();
const validateNode = new ValidateDataNode();
dataPrepNode.next(validateNode);
validateNode.next(modelFlow);
const dataScienceFlow = new DataScienceFlow(dataPrepNode);

const result = buildMermaid(dataScienceFlow);
```

The code generates a Mermaid diagram:

```mermaid
graph LR
    subgraph sub_flow_N1[DataScienceFlow]
    N2['DataPrepBatchNode']
    N3['ValidateDataNode']
    N2 --> N3
    N3 --> N4

    subgraph sub_flow_N5[ModelFlow]
    N4['FeatureExtractionNode']
    N6['TrainModelNode']
    N4 --> N6
    N7['EvaluateModelNode']
    N6 --> N7
    end

    end
```

## 2. Call Stack Debugging

There are several approaches to debug and trace execution in your flow application. Here are some recommended methods:

### Method 1: Using Decorators for Tracing (Recommended)

TypeScript decorators provide a clean way to add tracing functionality to your methods. This approach is more maintainable and flexible than manipulating the stack trace:

```typescript
import { BaseNode } from "../src/index";

// A simple tracing decorator that logs method execution
function trace(
  target: any,
  propertyKey: string,
  descriptor: PropertyDescriptor
) {
  const originalMethod = descriptor.value;

  descriptor.value = function (...args: any[]) {
    const nodeName = this.constructor.name;
    console.log(`Entering ${nodeName}.${propertyKey}`);

    try {
      const result = originalMethod.apply(this, args);
      console.log(`Exiting ${nodeName}.${propertyKey}`);
      return result;
    } catch (error) {
      console.error(`Error in ${nodeName}.${propertyKey}:`, error);
      throw error;
    }
  };

  return descriptor;
}

// Usage example
class MyNode extends BaseNode {
  @trace
  async prep(shared: any): Promise<any> {
    // Implementation
    return {};
  }

  @trace
  async exec(prepRes: any): Promise<any> {
    // Implementation
    return {};
  }

  @trace
  async post(
    shared: any,
    prepRes: any,
    execRes: any
  ): Promise<string | undefined> {
    // Implementation
    return "default";
  }
}
```

### Method 2: Stack Inspection

If you need to examine the call stack during runtime, you can implement a utility function that parses the Error stack trace:

```typescript
import { BaseNode } from "../src/index";

function getNodeCallStack(): string[] {
  const stack = new Error().stack || "";
  const stackLines = stack.split("\n").slice(1); // Skip the Error constructor line
  const nodeNames: string[] = [];
  const seenNodeIds = new Set<number>();

  // Regular expression to match the function names
  const functionNameRegex = /at\s+(\w+)\.(\w+)/;

  for (const line of stackLines) {
    const match = line.match(functionNameRegex);
    if (match) {
      // Try to find this.constructor.name in the stack frames
      // This is less precise than Python's inspect module, but can work
      // for basic stack tracing
      try {
        const instance = Function("return this")();
        if (instance instanceof BaseNode && !seenNodeIds.has(instance.nodeId)) {
          seenNodeIds.add(instance.nodeId);
          nodeNames.push(instance.constructor.name);
        }
      } catch (e) {
        // Unable to get instance from this context, continue
      }
    }
  }

  return nodeNames;
}
```

### Method 3: Using Existing Debugging Libraries

For production applications, consider using established debugging and tracing libraries:

```typescript
import debug from "debug"; // npm install debug
import { BaseNode } from "../src/index";

// Create debuggers for different aspects of your application
const prepDebug = debug("flow:prep");
const execDebug = debug("flow:exec");
const postDebug = debug("flow:post");

class DebuggableNode extends BaseNode {
  async prep(shared: any): Promise<any> {
    prepDebug(`${this.constructor.name} prep started with shared:`, shared);
    const result = await super.prep(shared);
    prepDebug(`${this.constructor.name} prep completed with result:`, result);
    return result;
  }

  async exec(prepRes: any): Promise<any> {
    execDebug(`${this.constructor.name} exec started with:`, prepRes);
    const result = await super.exec(prepRes);
    execDebug(`${this.constructor.name} exec completed with:`, result);
    return result;
  }

  async post(
    shared: any,
    prepRes: any,
    execRes: any
  ): Promise<string | undefined> {
    postDebug(`${this.constructor.name} post started`);
    const result = await super.post(shared, prepRes, execRes);
    postDebug(`${this.constructor.name} post completed with action:`, result);
    return result;
  }
}
```

To enable debugging, run your application with the DEBUG environment variable:

```bash
# Enable all flow debugging
DEBUG=flow:* node your-app.js

# Enable only specific parts
DEBUG=flow:prep,flow:post node your-app.js
```

### Example with Custom EvaluateModelNode

Here's a complete example using our original approach:

```typescript
import { BaseNode, BatchNode, Node, Flow } from "../src/index";

class DataPrepBatchNode extends BatchNode {
  prep(shared: any): any[] {
    return [];
  }
}
class ValidateDataNode extends Node {}
class FeatureExtractionNode extends Node {}
class TrainModelNode extends Node {}
class EvaluateModelNode extends Node {
  prep(shared: any): void {
    const stack = getNodeCallStack();
    console.log("Call stack:", stack);
  }
}
class ModelFlow extends Flow {}
class DataScienceFlow extends Flow {}

const featureNode = new FeatureExtractionNode();
const trainNode = new TrainModelNode();
const evaluateNode = new EvaluateModelNode();
featureNode.next(trainNode);
trainNode.next(evaluateNode);
const modelFlow = new ModelFlow(featureNode);

const dataPrepNode = new DataPrepBatchNode();
const validateNode = new ValidateDataNode();
dataPrepNode.next(validateNode);
validateNode.next(modelFlow);
const dataScienceFlow = new DataScienceFlow(dataPrepNode);

dataScienceFlow.run({});
```

The output would be: `Call stack: ['EvaluateModelNode', 'ModelFlow', 'DataScienceFlow']`

> Note: Each debugging approach has its strengths and limitations. The decorator-based approach offers the cleanest integration but requires TypeScript configuration with decorator support. The debug library approach is ideal for production systems with configurable logging levels.
</file>

<file path="docs/utility_function/websearch.md">
---
layout: default
title: "Web Search"
parent: "Utility Function"
nav_order: 3
---

# Web Search

We recommend some implementations of commonly used web search tools.

| **API**                           | **Free Tier**                                       | **Pricing Model**                                   | **Docs**                                                                    |
| --------------------------------- | --------------------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------- |
| **Google Custom Search JSON API** | 100 queries/day free                                | $5 per 1000 queries.                                | [Link](https://developers.google.com/custom-search/v1/overview)             |
| **Bing Web Search API**           | 1,000 queries/month                                 | $15–$25 per 1,000 queries.                          | [Link](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/) |
| **DuckDuckGo Instant Answer**     | Completely free (Instant Answers only, **no URLs**) | No paid plans; usage unlimited, but data is limited | [Link](https://duckduckgo.com/api)                                          |
| **Brave Search API**              | 2,000 queries/month free                            | $3 per 1k queries for Base, $5 per 1k for Pro       | [Link](https://brave.com/search/api/)                                       |
| **SerpApi**                       | 100 searches/month free                             | Start at $75/month for 5,000 searches               | [Link](https://serpapi.com/)                                                |

## Example TypeScript Code

### 1. Google Custom Search JSON API

```typescript
// Google doesn't provide an official SDK for JavaScript/TypeScript
// This example uses the googleapis package which provides a more SDK-like experience

import { google } from "googleapis";

interface GoogleSearchResult {
  kind: string;
  url: {
    type: string;
    template: string;
  };
  queries: {
    request: any[];
    nextPage?: any[];
  };
  context: {
    title: string;
  };
  searchInformation: {
    searchTime: number;
    formattedSearchTime: string;
    totalResults: string;
    formattedTotalResults: string;
  };
  items: Array<{
    kind: string;
    title: string;
    htmlTitle: string;
    link: string;
    displayLink: string;
    snippet: string;
    htmlSnippet: string;
    formattedUrl: string;
    htmlFormattedUrl: string;
    pagemap?: Record<string, any>;
  }>;
}

/**
 * Perform a Google Custom Search using the Google APIs client library
 */
async function googleSearch(
  query: string,
  apiKey: string,
  cxId: string,
  start: number = 1
): Promise<GoogleSearchResult> {
  // Initialize the Custom Search API service
  const customsearch = google.customsearch("v1");

  try {
    // Make the search request
    const response = await customsearch.cse.list({
      auth: apiKey,
      cx: cxId,
      q: query,
      start,
    });

    return response.data as GoogleSearchResult;
  } catch (error) {
    console.error("Error performing Google Custom Search:", error);
    throw error;
  }
}

// Example usage
const API_KEY = "YOUR_API_KEY";
const CX_ID = "YOUR_CX_ID";

googleSearch("example query", API_KEY, CX_ID)
  .then((results) => {
    console.log(`Total Results: ${results.searchInformation.totalResults}`);
    results.items.forEach((item) => {
      console.log(`Title: ${item.title}`);
      console.log(`Link: ${item.link}`);
      console.log(`Snippet: ${item.snippet}`);
      console.log("---");
    });
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 2. Bing Web Search API

```typescript
// Using the official Microsoft Bing Web Search SDK
import * as BingWebSearchAPI from "@azure/cognitiveservices-websearch";
import { CognitiveServicesCredentials } from "@azure/ms-rest-azure-js";

/**
 * Perform a Bing Web Search using the official Microsoft SDK
 */
async function bingSearch(
  query: string,
  subscriptionKey: string,
  count: number = 10,
  offset: number = 0,
  market: string = "en-US"
) {
  // Create a Cognitive Services credentials instance with your subscription key
  const credentials = new CognitiveServicesCredentials(subscriptionKey);

  // Create the search client
  const client = new BingWebSearchAPI.WebSearchClient(credentials);

  try {
    // Make the search request
    const searchResponse = await client.web.search(query, {
      count,
      offset,
      market,
      responseFilter: ["Webpages", "News", "Videos", "RelatedSearches"],
    });

    return searchResponse;
  } catch (error) {
    console.error("Error performing Bing Web Search:", error);
    throw error;
  }
}

// Example usage
const SUBSCRIPTION_KEY = "YOUR_BING_API_KEY";

bingSearch("example query", SUBSCRIPTION_KEY)
  .then((results) => {
    if (results.webPages) {
      console.log(`Total Results: ${results.webPages.totalEstimatedMatches}`);
      results.webPages.value.forEach((item) => {
        console.log(`Title: ${item.name}`);
        console.log(`URL: ${item.url}`);
        console.log(`Snippet: ${item.snippet}`);
        console.log("---");
      });
    } else {
      console.log("No web results found");
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 3. DuckDuckGo Instant Answer API

```typescript
// Using duck-duck-scrape, a community SDK for DuckDuckGo
import { DuckDuckGo } from "duck-duck-scrape";

interface DuckDuckGoResultItem {
  title: string;
  url: string;
  description: string;
}

/**
 * Search DuckDuckGo using the duck-duck-scrape library
 * This provides more functionality than the Instant Answer API alone
 */
async function duckDuckGoSearch(query: string, maxResults: number = 10) {
  const ddg = new DuckDuckGo();

  try {
    // Get instant answers and search results
    const response = await ddg.search(query, {
      safeSearch: "moderate",
      time: "y", // past year
      region: "us-en",
    });

    return {
      // Get instant answer if available
      abstractText: response.abstractText || "",
      abstractSource: response.abstractSource || "",
      abstractUrl: response.abstractUrl || "",

      // Get search results
      results: response.results.slice(0, maxResults).map((result) => ({
        title: result.title,
        url: result.url,
        description: result.description,
      })),

      // Get related topics
      relatedTopics: (response.relatedTopics || [])
        .slice(0, 5)
        .map((topic) => ({
          text: topic.text,
          url: topic.url,
        })),
    };
  } catch (error) {
    console.error("Error performing DuckDuckGo search:", error);
    throw error;
  }
}

// Example usage
duckDuckGoSearch("climate change")
  .then((results) => {
    if (results.abstractText) {
      console.log(`Abstract: ${results.abstractText}`);
      console.log(`Source: ${results.abstractSource} (${results.abstractUrl})`);
    }

    if (results.results.length > 0) {
      console.log("\nResults:");
      results.results.forEach((result, index) => {
        console.log(`${index + 1}. ${result.title}`);
        console.log(`   URL: ${result.url}`);
        console.log(`   ${result.description}`);
        console.log("---");
      });
    }

    if (results.relatedTopics.length > 0) {
      console.log("\nRelated Topics:");
      results.relatedTopics.forEach((topic) => {
        console.log(`- ${topic.text} (${topic.url})`);
      });
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 4. Brave Search API

```typescript
// Using brave-search TypeScript SDK
import { BraveSearch } from "brave-search";

/**
 * Perform a Brave Search using the brave-search SDK
 */
async function braveSearchWithSDK(
  query: string,
  apiKey: string,
  count: number = 10,
  offset: number = 0
) {
  // Initialize the Brave Search client
  const braveClient = new BraveSearch(apiKey);

  try {
    // Make the search request
    const results = await braveClient.search({
      q: query,
      count,
      offset,
    });

    return results;
  } catch (error) {
    console.error("Error performing Brave search:", error);
    throw error;
  }
}

// Example usage
const BRAVE_API_TOKEN = "YOUR_BRAVE_API_TOKEN";

braveSearchWithSDK("renewable energy", BRAVE_API_TOKEN)
  .then((results) => {
    console.log("Search Results:");

    // Access the web results
    if (results.web && results.web.results) {
      results.web.results.forEach((result, index) => {
        console.log(`${index + 1}. ${result.title}`);
        console.log(`   URL: ${result.url}`);
        console.log(`   ${result.description}`);
        console.log("---");
      });
    }

    // Check if automatic summarization is available (Pro plan feature)
    if (results.goggles) {
      console.log("Automatic Summarization:");
      console.log(results.goggles.content);
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```

### 5. SerpApi

```typescript
// Using the official SerpApi SDK
import { getJson } from "serpapi";

interface SerpApiResult {
  search_metadata: {
    id: string;
    status: string;
    json_endpoint: string;
    created_at: string;
    processed_at: string;
    google_url: string;
    raw_html_file: string;
    total_time_taken: number;
  };
  search_parameters: {
    engine: string;
    q: string;
    location_requested: string;
    location_used: string;
    google_domain: string;
    hl: string;
    gl: string;
    device: string;
  };
  search_information: {
    organic_results_state: string;
    query_displayed: string;
    total_results: number;
    time_taken_displayed: number;
  };
  organic_results: Array<{
    position: number;
    title: string;
    link: string;
    displayed_link: string;
    snippet: string;
    snippet_highlighted_words: string[];
    sitelinks?: any;
    about_this_result?: any;
    cached_page_link?: string;
    related_pages_link?: string;
  }>;
  related_questions?: any[];
  related_searches?: any[];
  pagination?: any;
  knowledge_graph?: any;
  answer_box?: any;
}

/**
 * Perform a search using the SerpApi SDK
 */
async function serpApiSearch(
  query: string,
  apiKey: string,
  location: string = "United States",
  num: number = 10,
  start: number = 0
): Promise<SerpApiResult> {
  try {
    // Make the search request using the SDK
    return (await getJson({
      engine: "google",
      api_key: apiKey,
      q: query,
      location,
      num,
      start,
    })) as SerpApiResult;
  } catch (error) {
    console.error("Error performing SerpApi search:", error);
    throw error;
  }
}

// Example usage
const SERPAPI_KEY = "YOUR_SERPAPI_KEY";

serpApiSearch("electric vehicles", SERPAPI_KEY)
  .then((response) => {
    console.log(`Total Results: ${response.search_information.total_results}`);
    console.log("Organic Results:");

    response.organic_results.forEach((result, index) => {
      console.log(`${index + 1}. ${result.title}`);
      console.log(`   URL: ${result.link}`);
      console.log(`   ${result.snippet}`);
      console.log("---");
    });

    if (response.pagination) {
      console.log("Pagination available for more results");
    }
  })
  .catch((error) => {
    console.error("Search failed:", error);
  });
```
</file>

<file path="docs/_config.yml">
# Basic site settings
title: PocketFlow.js
tagline: A 100-line LLM framework
description: Minimalist LLM Framework in 100 Lines, Enabling LLMs to Program Themselves

# Theme settings
remote_theme: just-the-docs/just-the-docs

# Navigation
nav_sort: case_sensitive

# Aux links (shown in upper right)
aux_links:
  "View on GitHub":
    - "//github.com/the-pocket/PocketFlow"

# Color scheme
color_scheme: light

# Author settings
author:
  name: Zachary Huang
  url: https://www.columbia.edu/~zh2408/
  twitter: ZacharyHuang12

# Mermaid settings
mermaid:
  version: "9.1.3" # Pick the version you want
  # Default configuration
  config: |
    directionLR

# Callouts settings
callouts:
  warning:
    title: Warning
    color: red
  note:
    title: Note
    color: blue
  best-practice:
    title: Best Practice
    color: green

# The custom navigation
nav:
  - Home: index.md # Link to your main docs index
  - GitHub: "https://github.com/the-pocket/PocketFlow"
  - Discord: "https://discord.gg/hUHHE9Sa6T"
</file>

<file path="docs/guide.md">
---
layout: default
title: "Agentic Coding"
---

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agents involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
> {: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps             |   Human    |     AI     | Comment                                                                                        |
| :---------------- | :--------: | :--------: | :--------------------------------------------------------------------------------------------- |
| 1. Requirements   |  ★★★ High  |  ★☆☆ Low   | Humans understand the requirements and context.                                                |
| 2. Flow           | ★★☆ Medium | ★★☆ Medium | Humans specify the high-level design, and the AI fills in the details.                         |
| 3. Utilities      | ★★☆ Medium | ★★☆ Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Node           |  ★☆☆ Low   |  ★★★ High  | The AI helps design the node types and data handling based on the flow.                        |
| 5. Implementation |  ★☆☆ Low   |  ★★★ High  | The AI implements the flow based on the design.                                                |
| 6. Optimization   | ★★☆ Medium | ★★☆ Medium | Humans evaluate the results, and the AI helps optimize.                                        |
| 7. Reliability    |  ★☆☆ Low   |  ★★★ High  | The AI writes test cases and addresses corner cases.                                           |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit.

   - Understand AI systems' strengths and limitations:
     - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
     - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
     - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
   - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
   - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.

   - Identify applicable design patterns (e.g., [Map Reduce](./design_pattern/mapreduce.md), [Agent](./design_pattern/agent.md), [RAG](./design_pattern/rag.md)).
     - For each node in the flow, start with a high-level one-line description of what it does.
     - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
     - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
     - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
   - Outline the flow and draw it in a mermaid diagram. For example:

     ```mermaid
     flowchart LR
         start[Start] --> batch[Batch]
         batch --> check[Check]
         check -->|OK| process
         check -->|Error| fix[Fix]
         fix --> check

         subgraph process[Process]
           step1[Step 1] --> step2[Step 2]
         end

         process --> endNode[End]
     ```

   - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
     > {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.

   - Think of your AI system as the brain. It needs a body—these _external utility functions_—to interact with the real world:
       <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

     - Reading inputs (e.g., retrieving Slack messages, reading emails)
     - Writing outputs (e.g., generating reports, sending emails)
     - Using external tools (e.g., calling LLMs, searching the web)
     - **NOTE**: _LLM-based tasks_ (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are _core functions_ internal in the AI system.

   - For each utility function, implement it and write a simple test.
   - Document their input/output, as well as why they are necessary. For example:
     - `name`: `getEmbedding` (`src/utils/getEmbedding.ts`)
     - `input`: `string`
     - `output`: a vector of 3072 numbers
     - `necessity`: Used by the second node to embed text
   - Example utility implementation:

     ```typescript
     // src/utils/callLlm.ts
     import { OpenAI } from "openai";

     export async function callLlm(prompt: string): Promise<string> {
       const client = new OpenAI({
         apiKey: process.env.OPENAI_API_KEY,
       });

       const response = await client.chat.completions.create({
         model: "gpt-4o",
         messages: [{ role: "user", content: prompt }],
       });

       return response.choices[0].message.content || "";
     }
     ```

   - > **Sometimes, design Utilies before Flow:** For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
     > {: .best-practice }

4. **Node Design**: Plan how each node will read and write data, and use utility functions.

   - One core design principle for PocketFlow is to use a [shared store](./core_abstraction/communication.md), so start with a shared store design:

     - For simple systems, use an in-memory object.
     - For more complex systems or when persistence is required, use a database.
     - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
     - Example shared store design:

       ```typescript
       interface SharedStore {
         user: {
           id: string;
           context: {
             weather: { temp: number; condition: string };
             location: string;
           };
         };
         results: Record<string, unknown>;
       }

       const shared: SharedStore = {
         user: {
           id: "user123",
           context: {
             weather: { temp: 72, condition: "sunny" },
             location: "San Francisco",
           },
         },
         results: {}, // Empty object to store outputs
       };
       ```

   - For each [Node](./core_abstraction/node.md), describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Node (or BatchNode, or ParallelBatchNode)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function
     - `post`: Write "embedding" to the shared store

5. **Implementation**: Implement the initial nodes and flows based on the design.

   - 🎉 If you've reached this step, humans have finished the design. Now _Agentic Coding_ begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Avoid `try` logic so you can quickly identify any weak points in the system.
   - Add logging throughout the code to facilitate debugging.

6. **Optimization**:

   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:

     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3–6 hundreds of times.
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     > {: .best-practice }

7. **Reliability**
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `maxRetries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my-project/
├── src/
│   ├── index.ts
│   ├── nodes.ts
│   ├── flow.ts
│   ├── types.ts
│   └── utils/
│       ├── callLlm.ts
│       └── searchWeb.ts
├── docs/
│   └── design.md
├── package.json
└── tsconfig.json
```

- **`docs/design.md`**: Contains project documentation for each step above. This should be _high-level_ and _no-code_.
- **`src/types.ts`**: Contains shared type definitions and interfaces used throughout the project.
- **`src/utils/`**: Contains all utility functions.
  - It's recommended to dedicate one TypeScript file to each API call, for example `callLlm.ts` or `searchWeb.ts`.
  - Each file should export functions that can be imported elsewhere in the project
  - Include test cases for each utility function using `utilityFunctionName.test.ts`
- **`src/nodes.ts`**: Contains all the node definitions.

  ```typescript
  // src/types.ts
  export interface QASharedStore {
    question?: string;
    answer?: string;
  }
  ```

  ```typescript
  // src/nodes.ts
  import { Node } from "pocketflow";
  import { callLlm } from "./utils/callLlm";
  import { QASharedStore } from "./types";
  import PromptSync from "prompt-sync";

  const prompt = PromptSync();

  export class GetQuestionNode extends Node<QASharedStore> {
    async exec(): Promise<string> {
      // Get question directly from user input
      const userQuestion = prompt("Enter your question: ") || "";
      return userQuestion;
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the user's question
      shared.question = execRes;
      return "default"; // Go to the next node
    }
  }

  export class AnswerNode extends Node<QASharedStore> {
    async prep(shared: QASharedStore): Promise<string> {
      // Read question from shared
      return shared.question || "";
    }

    async exec(question: string): Promise<string> {
      // Call LLM to get the answer
      return await callLlm(question);
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the answer in shared
      shared.answer = execRes;
      return undefined;
    }
  }
  ```

- **`src/flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.

  ```typescript
  // src/flow.ts
  import { Flow } from "pocketflow";
  import { GetQuestionNode, AnswerNode } from "./nodes";
  import { QASharedStore } from "./types";

  export function createQaFlow(): Flow {
    // Create nodes
    const getQuestionNode = new GetQuestionNode();
    const answerNode = new AnswerNode();

    // Connect nodes in sequence
    getQuestionNode.next(answerNode);

    // Create flow starting with input node
    return new Flow<QASharedStore>(getQuestionNode);
  }
  ```

- **`src/index.ts`**: Serves as the project's entry point.

  ```typescript
  // src/index.ts
  import { createQaFlow } from "./flow";
  import { QASharedStore } from "./types";

  // Example main function
  async function main(): Promise<void> {
    const shared: QASharedStore = {
      question: undefined, // Will be populated by GetQuestionNode from user input
      answer: undefined, // Will be populated by AnswerNode
    };

    // Create the flow and run it
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    console.log(`Question: ${shared.question}`);
    console.log(`Answer: ${shared.answer}`);
  }

  // Run the main function
  main().catch(console.error);
  ```

- **`package.json`**: Contains project metadata and dependencies.

- **`tsconfig.json`**: Contains TypeScript compiler configuration.
</file>

<file path="docs/index.md">
---
layout: default
title: "Home"
nav_order: 1
---

# PocketFlow.js

A [100-line](https://github.com/The-Pocket/PocketFlow-Typescript/blob/main/src/index.ts) minimalist LLM framework for _Agents, Task Decomposition, RAG, etc_.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworks—([Multi-](./design_pattern/multi_agent.html))[Agents](./design_pattern/agent.html), [Workflow](./design_pattern/workflow.html), [RAG](./design_pattern/rag.html), and more.
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" width="400"/>
</div>

## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node](./core_abstraction/node.md) handles simple (LLM) tasks.
- [Flow](./core_abstraction/flow.md) connects nodes through **Actions** (labeled edges).
- [Shared Store](./core_abstraction/communication.md) enables communication between nodes within flows.
- [Batch](./core_abstraction/batch.md) nodes/flows allow for data-intensive tasks.
- [(Advanced) Parallel](./core_abstraction/parallel.md) nodes/flows handle I/O-bound tasks.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" width="700"/>
</div>

## Design Pattern

From there, it’s easy to implement popular design patterns:

- [Agent](./design_pattern/agent.md) autonomously makes decisions.
- [Workflow](./design_pattern/workflow.md) chains multiple tasks into pipelines.
- [RAG](./design_pattern/rag.md) integrates data retrieval with generation.
- [Map Reduce](./design_pattern/mapreduce.md) splits data tasks into Map and Reduce steps.
- [Structured Output](./design_pattern/structure.md) formats outputs consistently.
- [(Advanced) Multi-Agents](./design_pattern/multi_agent.md) coordinate multiple agents.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" width="700"/>
</div>

## Utility Function

We **do not** provide built-in utilities. Instead, we offer _examples_—please _implement your own_:

- [LLM Wrapper](./utility_function/llm.md)
- [Viz and Debug](./utility_function/viz.md)
- [Web Search](./utility_function/websearch.md)
- [Chunking](./utility_function/chunking.md)
- [Embedding](./utility_function/embedding.md)
- [Vector Databases](./utility_function/vector.md)
- [Text-to-Speech](./utility_function/text_to_speech.md)

**Why not built-in?**: I believe it's a _bad practice_ for vendor-specific APIs in a general framework:

- _API Volatility_: Frequent changes lead to heavy maintenance for hardcoded APIs.
- _Flexibility_: You may want to switch vendors, use fine-tuned models, or run them locally.
- _Optimizations_: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps?

Check out [Agentic Coding Guidance](./guide.md), the fastest way to develop LLM projects with PocketFlow.js!
</file>

<file path="src/index.ts">
type NonIterableObject = Partial<Record<string, unknown>> & { [Symbol.iterator]?: never }; type Action = string;
class BaseNode<S = unknown, P extends NonIterableObject = NonIterableObject> {
  protected _params: P = {} as P; protected _successors: Map<Action, BaseNode> = new Map();
  protected async _exec(prepRes: unknown): Promise<unknown> { return await this.exec(prepRes); }
  async prep(shared: S): Promise<unknown> { return undefined; }
  async exec(prepRes: unknown): Promise<unknown> { return undefined; }
  async post(shared: S, prepRes: unknown, execRes: unknown): Promise<Action | undefined> { return undefined; }
  async _run(shared: S): Promise<Action | undefined> {
    const p = await this.prep(shared), e = await this._exec(p); return await this.post(shared, p, e);
  }
  async run(shared: S): Promise<Action | undefined> {
    if (this._successors.size > 0) console.warn("Node won't run successors. Use Flow.");
    return await this._run(shared);
  }
  setParams(params: P): this { this._params = params; return this; }
  next<T extends BaseNode>(node: T): T { this.on("default", node); return node; }
  on(action: Action, node: BaseNode): this {
    if (this._successors.has(action)) console.warn(`Overwriting successor for action '${action}'`);
    this._successors.set(action, node); return this;
  }
  getNextNode(action: Action = "default"): BaseNode | undefined {
    const nextAction = action || 'default', next = this._successors.get(nextAction)
    if (!next && this._successors.size > 0)
      console.warn(`Flow ends: '${nextAction}' not found in [${Array.from(this._successors.keys())}]`)
    return next
  }
  clone(): this {
    const clonedNode = Object.create(Object.getPrototypeOf(this)); Object.assign(clonedNode, this);
    clonedNode._params = { ...this._params }; clonedNode._successors = new Map(this._successors);
    return clonedNode;
  }
}
class Node<S = unknown, P extends NonIterableObject = NonIterableObject> extends BaseNode<S, P> {
  maxRetries: number; wait: number; currentRetry: number = 0;
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(); this.maxRetries = maxRetries; this.wait = wait;
  }
  async execFallback(prepRes: unknown, error: Error): Promise<unknown> { throw error; }
  async _exec(prepRes: unknown): Promise<unknown> {
    for (this.currentRetry = 0; this.currentRetry < this.maxRetries; this.currentRetry++) {
      try { return await this.exec(prepRes); } 
      catch (e) {
        if (this.currentRetry === this.maxRetries - 1) return await this.execFallback(prepRes, e as Error);
        if (this.wait > 0) await new Promise(resolve => setTimeout(resolve, this.wait * 1000));
      }
    }
    return undefined;
  }
}
class BatchNode<S = unknown, P extends NonIterableObject = NonIterableObject> extends Node<S, P> {
  async _exec(items: unknown[]): Promise<unknown[]> {
    if (!items || !Array.isArray(items)) return [];
    const results = []; for (const item of items) results.push(await super._exec(item)); return results;
  }
}
class ParallelBatchNode<S = unknown, P extends NonIterableObject = NonIterableObject> extends Node<S, P> {
  async _exec(items: unknown[]): Promise<unknown[]> {
    if (!items || !Array.isArray(items)) return []
    return Promise.all(items.map((item) => super._exec(item)))
  }
}
class Flow<S = unknown, P extends NonIterableObject = NonIterableObject> extends BaseNode<S, P> {
  start: BaseNode;
  constructor(start: BaseNode) { super(); this.start = start; }
  protected async _orchestrate(shared: S, params?: P): Promise<void> {
    let current: BaseNode | undefined = this.start.clone();
    const p = params || this._params;
    while (current) {
      current.setParams(p); const action = await current._run(shared);
      current = current.getNextNode(action); current = current?.clone();
    }
  }
  async _run(shared: S): Promise<Action | undefined> {
    const pr = await this.prep(shared); await this._orchestrate(shared);
    return await this.post(shared, pr, undefined);
  }
  async exec(prepRes: unknown): Promise<unknown> { throw new Error("Flow can't exec."); }
}
class BatchFlow<S = unknown, P extends NonIterableObject = NonIterableObject, NP extends NonIterableObject[] = NonIterableObject[]> extends Flow<S, P> {
  async _run(shared: S): Promise<Action | undefined> {
    const batchParams = await this.prep(shared);
    for (const bp of batchParams) {
      const mergedParams = { ...this._params, ...bp };
      await this._orchestrate(shared, mergedParams);
    }
    return await this.post(shared, batchParams, undefined);
  }
  async prep(shared: S): Promise<NP> { const empty: readonly NonIterableObject[] = []; return empty as NP; }
}
class ParallelBatchFlow<S = unknown, P extends NonIterableObject = NonIterableObject, NP extends NonIterableObject[] = NonIterableObject[]> extends BatchFlow<S, P, NP> {
  async _run(shared: S): Promise<Action | undefined> {
    const batchParams = await this.prep(shared);
    await Promise.all(batchParams.map(bp => {
      const mergedParams = { ...this._params, ...bp };
      return this._orchestrate(shared, mergedParams);
    }));
    return await this.post(shared, batchParams, undefined);
  }
}
export { BaseNode, Node, BatchNode, ParallelBatchNode, Flow, BatchFlow, ParallelBatchFlow };
</file>

<file path="tests/batch-flow.test.ts">
// tests/async-batch-flow.test.ts
import { Node, BatchFlow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  inputData?: Record<string, number>;
  results?: Record<string, number>;
  intermediateResults?: Record<string, number>;
};

// Parameters type
type BatchParams = {
  key: string;
  multiplier?: number;
};

class AsyncDataProcessNode extends Node<SharedStorage, BatchParams> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number> {
    const key = this._params.key;
    const data = shared.inputData?.[key] ?? 0;

    if (!shared.results) {
      shared.results = {};
    }

    shared.results[key] = data;
    return data;
  }

  async exec(prepRes: number): Promise<number> {
    return prepRes; // Just return the prep result as-is
  }

  async post(
    shared: SharedStorage,
    prepRes: number,
    execRes: number
  ): Promise<string | undefined> {
    await new Promise((resolve) => setTimeout(resolve, 10)); // Simulate async work
    const key = this._params.key;

    if (!shared.results) {
      shared.results = {};
    }

    shared.results[key] = execRes * 2; // Double the value
    return 'processed';
  }
}

class AsyncErrorNode extends Node<SharedStorage, BatchParams> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<any> {
    return undefined;
  }

  async exec(prepRes: any): Promise<any> {
    return undefined;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: any
  ): Promise<string | undefined> {
    const key = this._params.key;
    if (key === 'errorKey') {
      throw new Error(`Async error processing key: ${key}`);
    }
    return 'processed';
  }
}

describe('BatchFlow Tests', () => {
  let processNode: AsyncDataProcessNode;

  beforeEach(() => {
    processNode = new AsyncDataProcessNode();
  });

  test('basic async batch processing', async () => {
    class SimpleTestBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }
    }

    const shared: SharedStorage = {
      inputData: {
        a: 1,
        b: 2,
        c: 3,
      },
    };

    const flow = new SimpleTestBatchFlow(processNode);
    await flow.run(shared);

    expect(shared.results).toEqual({
      a: 2, // 1 * 2
      b: 4, // 2 * 2
      c: 6, // 3 * 2
    });
  });

  test('empty async batch', async () => {
    class EmptyTestBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        // Initialize results as an empty object
        if (!shared.results) {
          shared.results = {};
        }
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }

      // Ensure post is called even if batch is empty
      async post(
        shared: SharedStorage,
        prepRes: BatchParams[],
        execRes: any
      ): Promise<string | undefined> {
        if (!shared.results) {
          shared.results = {};
        }
        return undefined;
      }
    }

    const shared: SharedStorage = {
      inputData: {},
    };

    const flow = new EmptyTestBatchFlow(processNode);
    await flow.run(shared);

    expect(shared.results).toEqual({});
  });

  test('async error handling', async () => {
    class ErrorTestBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }
    }

    const shared: SharedStorage = {
      inputData: {
        normalKey: 1,
        errorKey: 2,
        anotherKey: 3,
      },
    };

    const flow = new ErrorTestBatchFlow(new AsyncErrorNode());

    await expect(async () => {
      await flow.run(shared);
    }).rejects.toThrow('Async error processing key: errorKey');
  });

  test('nested async flow', async () => {
    class AsyncInnerNode extends Node<SharedStorage, BatchParams> {
      async prep(shared: SharedStorage): Promise<any> {
        return undefined;
      }

      async exec(prepRes: any): Promise<any> {
        return undefined;
      }

      async post(
        shared: SharedStorage,
        prepRes: any,
        execRes: any
      ): Promise<string | undefined> {
        const key = this._params.key;

        if (!shared.intermediateResults) {
          shared.intermediateResults = {};
        }

        // Safely access inputData
        const inputValue = shared.inputData?.[key] ?? 0;
        shared.intermediateResults[key] = inputValue + 1;

        await new Promise((resolve) => setTimeout(resolve, 10));
        return 'next';
      }
    }

    class AsyncOuterNode extends Node<SharedStorage, BatchParams> {
      async prep(shared: SharedStorage): Promise<any> {
        return undefined;
      }

      async exec(prepRes: any): Promise<any> {
        return undefined;
      }

      async post(
        shared: SharedStorage,
        prepRes: any,
        execRes: any
      ): Promise<string | undefined> {
        const key = this._params.key;

        if (!shared.results) {
          shared.results = {};
        }

        if (!shared.intermediateResults) {
          shared.intermediateResults = {};
        }

        shared.results[key] = shared.intermediateResults[key] * 2;
        await new Promise((resolve) => setTimeout(resolve, 10));
        return 'done';
      }
    }

    class NestedBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k) => ({ key: k }));
      }
    }

    // Create inner flow
    const innerNode = new AsyncInnerNode();
    const outerNode = new AsyncOuterNode();
    innerNode.on('next', outerNode);

    const shared: SharedStorage = {
      inputData: {
        x: 1,
        y: 2,
      },
    };

    const flow = new NestedBatchFlow(innerNode);
    await flow.run(shared);

    expect(shared.results).toEqual({
      x: 4, // (1 + 1) * 2
      y: 6, // (2 + 1) * 2
    });
  });

  test('custom async parameters', async () => {
    class CustomParamNode extends Node<SharedStorage, BatchParams> {
      async prep(shared: SharedStorage): Promise<any> {
        return undefined;
      }

      async exec(prepRes: any): Promise<any> {
        return undefined;
      }

      async post(
        shared: SharedStorage,
        prepRes: any,
        execRes: any
      ): Promise<string | undefined> {
        const key = this._params.key;
        const multiplier = this._params.multiplier || 1;

        await new Promise((resolve) => setTimeout(resolve, 10));

        if (!shared.results) {
          shared.results = {};
        }

        // Safely access inputData with default value
        const inputValue = shared.inputData?.[key] ?? 0;
        shared.results[key] = inputValue * multiplier;

        return 'done';
      }
    }

    class CustomParamBatchFlow extends BatchFlow<SharedStorage> {
      async prep(shared: SharedStorage): Promise<BatchParams[]> {
        return Object.keys(shared.inputData || {}).map((k, i) => ({
          key: k,
          multiplier: i + 1,
        }));
      }
    }

    const shared: SharedStorage = {
      inputData: {
        a: 1,
        b: 2,
        c: 3,
      },
    };

    const flow = new CustomParamBatchFlow(new CustomParamNode());
    await flow.run(shared);

    expect(shared.results).toEqual({
      a: 1 * 1, // first item, multiplier = 1
      b: 2 * 2, // second item, multiplier = 2
      c: 3 * 3, // third item, multiplier = 3
    });
  });
});
</file>

<file path="tests/batch-node.test.ts">
// tests/batch-node.test.ts
import { Node, BatchNode, Flow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  inputArray?: number[];
  chunkResults?: number[];
  total?: number;
  result?: string;
  parallelResults?: number[];
  completionOrder?: number[];
};

class AsyncArrayChunkNode extends BatchNode<SharedStorage> {
  private chunkSize: number;

  constructor(
    chunkSize: number = 10,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
    this.chunkSize = chunkSize;
  }

  async prep(shared: SharedStorage): Promise<number[][]> {
    // Get array from shared storage and split into chunks
    const array = shared.inputArray || [];
    const chunks: number[][] = [];

    for (let start = 0; start < array.length; start += this.chunkSize) {
      const end = Math.min(start + this.chunkSize, array.length);
      chunks.push(array.slice(start, end));
    }

    return chunks;
  }

  async exec(chunk: number[]): Promise<number> {
    // Simulate async processing of each chunk
    await new Promise((resolve) => setTimeout(resolve, 10));
    return chunk.reduce((sum, value) => sum + value, 0);
  }

  async post(
    shared: SharedStorage,
    prepRes: number[][],
    execRes: number[]
  ): Promise<string | undefined> {
    // Store chunk results in shared storage
    shared.chunkResults = execRes;
    return 'processed';
  }
}

class AsyncSumReduceNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    // Get chunk results from shared storage
    return shared.chunkResults || [];
  }

  async exec(chunkResults: number[]): Promise<number> {
    // Simulate async processing
    await new Promise((resolve) => setTimeout(resolve, 10));
    return chunkResults.reduce((sum, value) => sum + value, 0);
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number
  ): Promise<string | undefined> {
    // Store the total in shared storage
    shared.total = execRes;
    return 'reduced';
  }
}

// Create an error-throwing node for testing error handling
class ErrorBatchNode extends BatchNode<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    return shared.inputArray || [];
  }

  async exec(item: number): Promise<number> {
    if (item === 2) {
      throw new Error('Error processing item 2');
    }
    return item;
  }
}

describe('BatchNode Tests', () => {
  test('array chunking', async () => {
    // Test that the array is correctly split into chunks and processed asynchronously
    const shared: SharedStorage = {
      inputArray: Array.from({ length: 25 }, (_, i) => i), // [0,1,2,...,24]
    };

    const chunkNode = new AsyncArrayChunkNode(10);
    await chunkNode.run(shared);

    expect(shared.chunkResults).toEqual([45, 145, 110]); // Sum of chunks [0-9], [10-19], [20-24]
  });

  test('async map-reduce sum', async () => {
    // Test a complete async map-reduce pipeline that sums a large array
    const array = Array.from({ length: 100 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0); // 4950

    const shared: SharedStorage = {
      inputArray: array,
    };

    // Create nodes
    const chunkNode = new AsyncArrayChunkNode(10);
    const reduceNode = new AsyncSumReduceNode();

    // Connect nodes
    chunkNode.on('processed', reduceNode);

    // Create and run pipeline
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('uneven chunks', async () => {
    // Test that the async map-reduce works correctly with array lengths
    // that don't divide evenly by chunkSize
    const array = Array.from({ length: 25 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0); // 300

    const shared: SharedStorage = {
      inputArray: array,
    };

    const chunkNode = new AsyncArrayChunkNode(10);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('custom chunk size', async () => {
    // Test that the async map-reduce works with different chunk sizes
    const array = Array.from({ length: 100 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0);

    const shared: SharedStorage = {
      inputArray: array,
    };

    // Use chunkSize=15 instead of default 10
    const chunkNode = new AsyncArrayChunkNode(15);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('single element chunks', async () => {
    // Test extreme case where chunkSize=1
    const array = Array.from({ length: 5 }, (_, i) => i);
    const expectedSum = array.reduce((sum, val) => sum + val, 0);

    const shared: SharedStorage = {
      inputArray: array,
    };

    const chunkNode = new AsyncArrayChunkNode(1);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(expectedSum);
  });

  test('empty array', async () => {
    // Test edge case of empty input array
    const shared: SharedStorage = {
      inputArray: [],
    };

    const chunkNode = new AsyncArrayChunkNode(10);
    const reduceNode = new AsyncSumReduceNode();

    chunkNode.on('processed', reduceNode);
    const pipeline = new Flow(chunkNode);
    await pipeline.run(shared);

    expect(shared.total).toBe(0);
  });

  test('error handling', async () => {
    // Test error handling in async batch processing
    const shared: SharedStorage = {
      inputArray: [1, 2, 3],
    };

    const errorNode = new ErrorBatchNode();

    await expect(async () => {
      await errorNode.run(shared);
    }).rejects.toThrow('Error processing item 2');
  });

  test('retry mechanism', async () => {
    // Test the retry mechanism with a node that fails intermittently
    let attempts = 0;

    class RetryTestNode extends BatchNode<SharedStorage> {
      constructor() {
        super(3, 0.01); // 3 retries with 10ms wait time
      }

      async prep(shared: SharedStorage): Promise<number[]> {
        return [1];
      }

      async exec(item: number): Promise<string> {
        attempts++;
        if (attempts < 3) {
          throw new Error(`Failure on attempt ${attempts}`);
        }
        return `Success on attempt ${attempts}`;
      }

      async post(
        shared: SharedStorage,
        prepRes: number[],
        execRes: string[]
      ): Promise<string | undefined> {
        shared.result = execRes[0];
        return undefined;
      }
    }

    const shared: SharedStorage = {};
    const retryNode = new RetryTestNode();

    await retryNode.run(shared);

    expect(attempts).toBe(3);
    expect(shared.result).toBe('Success on attempt 3');
  });

  test('parallel batch processing', async () => {
    // Test that ParallelBatchNode processes items in parallel
    let completed: number[] = [];

    // Import ParallelBatchNode
    const { ParallelBatchNode } = require('../src/index');

    class ParallelProcessingNode extends ParallelBatchNode<SharedStorage> {
      constructor() {
        super(1, 0);
      }

      async prep(shared: SharedStorage): Promise<number[]> {
        return [1, 2, 3, 4, 5];
      }

      async exec(item: number): Promise<number> {
        // Items with higher values will complete first
        const delay = 100 - item * 20;
        await new Promise((resolve) => setTimeout(resolve, delay));
        completed.push(item);
        return item;
      }

      async post(
        shared: SharedStorage,
        prepRes: number[],
        execRes: number[]
      ): Promise<string | undefined> {
        shared.parallelResults = execRes;
        shared.completionOrder = [...completed];
        return undefined;
      }
    }

    const shared: SharedStorage = {};
    const parallelNode = new ParallelProcessingNode();

    await parallelNode.run(shared);

    // Check that results contain all processed items
    expect(shared.parallelResults?.sort()).toEqual([1, 2, 3, 4, 5]);

    // Check that items were not necessarily processed in order
    // Higher numbered items should complete before lower ones due to the delay logic
    expect(shared.completionOrder).not.toEqual([1, 2, 3, 4, 5]);
  });
});
</file>

<file path="tests/core-abstraction-examples.test.ts">
// tests/core_abstraction_examples.test.ts
import { BaseNode, Node, BatchNode, ParallelBatchNode, Flow, BatchFlow, ParallelBatchFlow } from '../src/index';

// Define shared storage types
type SharedStorage = {
  data?: string;
  summary?: string;
  files?: string[];
  results?: any[];
  current?: number;
  expenses?: Array<{id: string; amount: number; status?: string}>;
  // For nested flows example
  payments?: Array<{id: string; status: string}>;
  inventory?: Array<{id: string; status: string}>;
  shipping?: Array<{id: string; status: string}>;
  orderComplete?: boolean;
};

// 1. Simple Node Example (SummarizeFile from node.md)
class SummarizeFile extends Node<SharedStorage, FileParams> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    return shared.data;
  }

  async exec(prepRes: string | undefined): Promise<string> {
    if (!prepRes) {
      return "Empty file content";
    }
    // Simulate LLM call
    return `Summary: ${prepRes.substring(0, 10)}...`;
  }

  async execFallback(prepRes: string | undefined, error: Error): Promise<string> {
    // Provide a simple fallback
    return "There was an error processing your request.";
  }

  async post(shared: SharedStorage, prepRes: string | undefined, execRes: string): Promise<string | undefined> {
    shared.summary = execRes;
    return undefined; // "default" action
  }
}

// 2. Expense Approval Flow example (from flow.md)
class ReviewExpense extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    // Select the first pending expense
    return shared.expenses?.find(e => !e.status);
  }

  async post(shared: SharedStorage, prepRes: any): Promise<string> {
    if (!prepRes) return "finished";
    
    // Based on amount, determine approval path
    const expense = prepRes;
    if (expense.amount <= 100) {
      expense.status = "approved";
      return "approved";
    } else if (expense.amount > 500) {
      expense.status = "rejected";
      return "rejected";
    } else {
      expense.status = "needs_revision";
      return "needs_revision";
    }
  }
}

class ReviseExpense extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    // Find expense that needs revision
    return shared.expenses?.find(e => e.status === "needs_revision");
  }

  async post(shared: SharedStorage, prepRes: any): Promise<string> {
    if (!prepRes) return "finished";
    
    // Reduce the expense amount to make it more likely to be approved
    const expense = prepRes;
    expense.amount = 75; // Revised to a lower amount
    expense.status = undefined; // Reset status for re-review
    
    return "default"; // Go back to review
  }
}

class ProcessPayment extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<any> {
    // Find approved expenses to process
    return shared.expenses?.find(e => e.status === "approved");
  }

  async post(shared: SharedStorage, prepRes: any): Promise<string> {
    if (!prepRes) return "finished";
    
    // Mark expense as paid
    const expense = prepRes;
    expense.status = "paid";
    
    return "default"; // Continue to finish
  }
}

class FinishProcess extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    // Simply mark the process as done and return
    return "finished";
  }
}

// 3. BatchNode example (MapSummaries from batch.md)
class MapSummaries extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // Suppose we have a big file; chunk it
    const content = shared.data || "";
    const chunks: string[] = [];
    const chunkSize = 10;

    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize));
    }

    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    // Process each chunk
    return `Chunk summary: ${chunk.substring(0, 3)}...`;
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    // Combine summaries
    shared.summary = execRes.join("\n");
    return "default";
  }
}

// 4. BatchFlow example (SummarizeAllFiles from batch.md)
type FileParams = {
  filename: string;
};

class LoadFile extends Node<SharedStorage, FileParams> {
  async prep(shared: SharedStorage): Promise<string> {
    // Simulate loading a file
    const filename = this._params.filename;
    return `Content of ${filename}`;
  }

  async post(shared: SharedStorage, prepRes: string): Promise<string | undefined> {
    shared.data = prepRes;
    return undefined;
  }
}

class SummarizeAllFiles extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    return (shared.files || []).map((filename) => ({ filename }));
  }
}

// 5. ParallelBatchNode example (TextSummarizer from parallel.md)
class TextSummarizer extends ParallelBatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // For testing, we'll create an array of texts
    return shared.data?.split('\n') || [];
  }

  async exec(text: string): Promise<string> {
    // Simulate LLM call
    return `Summary of: ${text.substring(0, 5)}...`;
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.results = execRes;
    return "default";
  }
}

// 6. Nested Flow example (Order Processing Pipeline from flow.md)
// Payment Flow nodes
class ValidatePayment extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (!shared.payments) shared.payments = [];
    shared.payments.push({ id: "payment1", status: "validated" });
    return "default";
  }
}

class ProcessPaymentFlow extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.payments) {
      shared.payments.forEach(p => {
        if (p.status === "validated") p.status = "processed";
      });
    }
    return "default";
  }
}

class PaymentConfirmation extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.payments) {
      shared.payments.forEach(p => {
        if (p.status === "processed") p.status = "confirmed";
      });
    }
    return "default";
  }
}

// Inventory Flow nodes
class CheckStock extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (!shared.inventory) shared.inventory = [];
    shared.inventory.push({ id: "inventory1", status: "checked" });
    return "default";
  }
}

class ReserveItems extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.inventory) {
      shared.inventory.forEach(i => {
        if (i.status === "checked") i.status = "reserved";
      });
    }
    return "default";
  }
}

class UpdateInventory extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.inventory) {
      shared.inventory.forEach(i => {
        if (i.status === "reserved") i.status = "updated";
      });
    }
    return "default";
  }
}

// Shipping Flow nodes
class CreateLabel extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (!shared.shipping) shared.shipping = [];
    shared.shipping.push({ id: "shipping1", status: "labeled" });
    return "default";
  }
}

class AssignCarrier extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.shipping) {
      shared.shipping.forEach(s => {
        if (s.status === "labeled") s.status = "assigned";
      });
    }
    return "default";
  }
}

class SchedulePickup extends Node<SharedStorage> {
  async post(shared: SharedStorage): Promise<string> {
    if (shared.shipping) {
      shared.shipping.forEach(s => {
        if (s.status === "assigned") s.status = "scheduled";
      });
    }
    
    // Order complete
    shared.orderComplete = true;
    return "default";
  }
}

describe('Core Abstraction Examples', () => {
  // 1. Basic Node tests
  describe('Node Example: SummarizeFile', () => {
    test('should summarize file content', async () => {
      const shared: SharedStorage = { data: "This is a test document that needs to be summarized." };
      const summarizeNode = new SummarizeFile(3); // maxRetries = 3
      await summarizeNode.run(shared);
      
      expect(shared.summary).toBe("Summary: This is a ...");
    });
    
    test('should handle empty content', async () => {
      const shared: SharedStorage = { data: "" };
      const summarizeNode = new SummarizeFile();
      await summarizeNode.run(shared);
      
      expect(shared.summary).toBe("Empty file content");
    });
  });
  
  // 2. Flow and branching tests
  describe('Flow Example: Expense Approval', () => {
    test('should approve small expenses', async () => {
      const shared: SharedStorage = {
        expenses: [{ id: "exp1", amount: 50 }]
      };
      
      const review = new ReviewExpense();
      const payment = new ProcessPayment();
      const finish = new FinishProcess();
      
      review.on("approved", payment);
      payment.next(finish);
      
      const flow = new Flow(review);
      await flow.run(shared);
      
      expect(shared.expenses?.[0].status).toBe("paid");
    });
    
    test('should reject large expenses', async () => {
      const shared: SharedStorage = {
        expenses: [{ id: "exp2", amount: 1000 }]
      };
      
      const review = new ReviewExpense();
      const payment = new ProcessPayment();
      const finish = new FinishProcess();
      
      review.on("rejected", finish);
      
      const flow = new Flow(review);
      await flow.run(shared);
      
      expect(shared.expenses?.[0].status).toBe("rejected");
    });
    
    test('should handle expense revision and then approval', async () => {
      const shared: SharedStorage = {
        expenses: [{ id: "exp3", amount: 200 }]
      };
      
      const review = new ReviewExpense();
      const revise = new ReviseExpense();
      const payment = new ProcessPayment();
      const finish = new FinishProcess();
      
      // Set up the flow connections
      review.on("approved", payment);
      review.on("needs_revision", revise);
      review.on("rejected", finish);
      
      revise.next(review);
      payment.next(finish);
      
      const flow = new Flow(review);
      await flow.run(shared);
      
      expect(shared.expenses?.[0].status).toBe("paid");
      expect(shared.expenses?.[0].amount).toBe(75); // revised amount
    });
  });
  
  // 3. BatchNode tests
  describe('BatchNode Example: MapSummaries', () => {
    test('should process text in chunks', async () => {
      const shared: SharedStorage = {
        data: "This is a very long document that needs to be processed in chunks."
      };
      
      const mapSummaries = new MapSummaries();
      await mapSummaries.run(shared);
      
      expect(shared.summary).toContain("Chunk summary: Thi");
      expect(shared.summary?.split('\n').length).toBeGreaterThan(1);
    });
  });
  
  // 4. BatchFlow tests
  describe('BatchFlow Example: SummarizeAllFiles', () => {
    test('should process multiple files', async () => {
      const shared: SharedStorage = {
        files: ["file1.txt", "file2.txt", "file3.txt"]
      };
      
      const loadFile = new LoadFile();
      const summarizeFile = new SummarizeFile();
      
      loadFile.next(summarizeFile);
      
      const summarizeAllFiles = new SummarizeAllFiles(new Flow(loadFile));
      await summarizeAllFiles.run(shared);
      
      // The last file's summary should be in shared.summary
      expect(shared.summary).toBe("Summary: Content of...");
    });
  });
  
  // 5. ParallelBatchNode tests
  describe('ParallelBatchNode Example: TextSummarizer', () => {
    test('should process multiple texts in parallel', async () => {
      const shared: SharedStorage = {
        data: "Text 1\nText 2\nText 3\nText 4"
      };
      
      const textSummarizer = new TextSummarizer();
      await textSummarizer.run(shared);
      
      expect(shared.results?.length).toBe(4);
      expect(shared.results?.[0]).toBe("Summary of: Text ...");
      expect(shared.results?.[1]).toBe("Summary of: Text ...");
    });
  });
  
  // 6. Nested Flow tests
  describe('Nested Flow Example: Order Processing Pipeline', () => {
    test('should process a complete order through multiple flows', async () => {
      const shared: SharedStorage = {};
      
      // Build Payment Flow
      const validatePayment = new ValidatePayment();
      const processPayment = new ProcessPaymentFlow();
      const paymentConfirmation = new PaymentConfirmation();
      
      validatePayment.next(processPayment).next(paymentConfirmation);
      const paymentFlow = new Flow(validatePayment);
      
      // Build Inventory Flow
      const checkStock = new CheckStock();
      const reserveItems = new ReserveItems();
      const updateInventory = new UpdateInventory();
      
      checkStock.next(reserveItems).next(updateInventory);
      const inventoryFlow = new Flow(checkStock);
      
      // Build Shipping Flow
      const createLabel = new CreateLabel();
      const assignCarrier = new AssignCarrier();
      const schedulePickup = new SchedulePickup();
      
      createLabel.next(assignCarrier).next(schedulePickup);
      const shippingFlow = new Flow(createLabel);
      
      // Connect the flows
      paymentFlow.next(inventoryFlow).next(shippingFlow);
      
      // Create and run the master flow
      const orderPipeline = new Flow(paymentFlow);
      await orderPipeline.run(shared);
      
      // Check results
      expect(shared.payments?.[0].status).toBe("confirmed");
      expect(shared.inventory?.[0].status).toBe("updated");
      expect(shared.shipping?.[0].status).toBe("scheduled");
      expect(shared.orderComplete).toBe(true);
    });
  });
});
</file>

<file path="tests/fallback.test.ts">
// tests/fallback.test.ts
import { Node, Flow } from '../src/index';

// Define a shared storage type
type SharedStorage = {
  results?: Array<{
    attempts: number;
    result: string;
  }>;
  finalResult?: any;
};

class FallbackNode extends Node<SharedStorage> {
  private shouldFail: boolean;
  private attemptCount: number = 0;
  
  constructor(shouldFail: boolean = true, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.shouldFail = shouldFail;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    this.attemptCount++;
    if (this.shouldFail) {
      throw new Error("Intentional failure");
    }
    return "success";
  }
  
  async execFallback(prepResult: null, error: Error): Promise<string> {
    return "fallback";
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: this.attemptCount,
      result: execResult
    });
    return undefined;
  }
}

class AsyncFallbackNode extends Node<SharedStorage> {
  private shouldFail: boolean;
  private attemptCount: number = 0;
  
  constructor(shouldFail: boolean = true, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.shouldFail = shouldFail;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    this.attemptCount++;
    if (this.shouldFail) {
      throw new Error("Intentional async failure");
    }
    return "success";
  }
  
  async execFallback(prepResult: null, error: Error): Promise<string> {
    // Simulate async work
    await new Promise(resolve => setTimeout(resolve, 10));
    return "async_fallback";
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: this.attemptCount,
      result: execResult
    });
    return undefined;
  }
}

// Changed to extend Node instead of BaseNode
class ResultNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }
  
  async prep(shared: SharedStorage): Promise<any> {
    return shared.results || [];
  }
  
  async exec(prepResult: any): Promise<any> {
    return prepResult;
  }
  
  async post(shared: SharedStorage, prepResult: any, execResult: any): Promise<string | undefined> {
    shared.finalResult = execResult;
    return undefined;
  }
}

// Changed to extend Node instead of BaseNode
class NoFallbackNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    throw new Error("Test error");
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({ attempts: 1, result: execResult });
    return execResult;
  }
}

// New class to demonstrate retry with eventual success
class EventualSuccessNode extends Node<SharedStorage> {
  private succeedAfterAttempts: number;
  private attemptCount: number = 0;
  
  constructor(succeedAfterAttempts: number = 2, maxRetries: number = 3, wait: number = 0.01) {
    super(maxRetries, wait);
    this.succeedAfterAttempts = succeedAfterAttempts;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    this.attemptCount++;
    if (this.attemptCount < this.succeedAfterAttempts) {
      throw new Error(`Fail on attempt ${this.attemptCount}`);
    }
    return `success_after_${this.attemptCount}_attempts`;
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: this.attemptCount,
      result: execResult
    });
    return undefined;
  }
}

// New class to demonstrate customized error handling
class CustomErrorHandlerNode extends Node<SharedStorage> {
  private errorType: string;
  
  constructor(errorType: string = "standard", maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.errorType = errorType;
  }
  
  async prep(shared: SharedStorage): Promise<null> {
    if (!shared.results) {
      shared.results = [];
    }
    return null;
  }
  
  async exec(prepResult: null): Promise<string> {
    throw new Error(this.errorType);
  }
  
  async execFallback(prepResult: null, error: Error): Promise<string> {
    // Custom error handling based on error type
    if (error.message === "network") {
      return "network_error_handled";
    } else if (error.message === "timeout") {
      return "timeout_error_handled";
    } else {
      return "generic_error_handled";
    }
  }
  
  async post(shared: SharedStorage, prepResult: null, execResult: string): Promise<string | undefined> {
    shared.results?.push({
      attempts: 1,
      result: execResult
    });
    return undefined;
  }
}

describe('Fallback Functionality Tests with Node', () => {
  test('successful execution', async () => {
    // Test that execFallback is not called when execution succeeds
    const shared: SharedStorage = {};
    const node = new FallbackNode(false);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(1);
    expect(shared.results?.[0].result).toBe("success");
  });

  test('fallback after failure', async () => {
    // Test that execFallback is called after all retries are exhausted
    const shared: SharedStorage = {};
    const node = new AsyncFallbackNode(true, 2);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(2);
    expect(shared.results?.[0].result).toBe("async_fallback");
  });

  test('fallback in flow', async () => {
    // Test that fallback works within a Flow
    const shared: SharedStorage = {};
    const fallbackNode = new FallbackNode(true, 1);
    const resultNode = new ResultNode();
    
    fallbackNode.next(resultNode);
    
    const flow = new Flow(fallbackNode);
    await flow.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].result).toBe("fallback");
    expect(shared.finalResult).toEqual([{ attempts: 1, result: 'fallback' }]);
  });

  test('no fallback implementation', async () => {
    // Test that without overriding execFallback, Node will rethrow the error
    const shared: SharedStorage = {};
    const node = new NoFallbackNode();
    
    await expect(async () => {
      await node.run(shared);
    }).rejects.toThrow("Test error");
  });

  test('retry before fallback', async () => {
    // Test that retries are attempted before calling fallback
    const shared: SharedStorage = {};
    const node = new AsyncFallbackNode(true, 3);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(3);
    expect(shared.results?.[0].result).toBe("async_fallback");
  });
  
  test('eventual success after retries', async () => {
    // Test node that succeeds after multiple attempts
    const shared: SharedStorage = {};
    const node = new EventualSuccessNode(2, 3);
    await node.run(shared);
    
    expect(shared.results?.length).toBe(1);
    expect(shared.results?.[0].attempts).toBe(2);
    expect(shared.results?.[0].result).toBe("success_after_2_attempts");
  });
  
  test('custom error handling based on error type', async () => {
    // Test custom fallback logic based on error type
    const shared1: SharedStorage = {};
    const node1 = new CustomErrorHandlerNode("network");
    await node1.run(shared1);
    expect(shared1.results?.[0].result).toBe("network_error_handled");
    
    const shared2: SharedStorage = {};
    const node2 = new CustomErrorHandlerNode("timeout");
    await node2.run(shared2);
    expect(shared2.results?.[0].result).toBe("timeout_error_handled");
    
    const shared3: SharedStorage = {};
    const node3 = new CustomErrorHandlerNode("other");
    await node3.run(shared3);
    expect(shared3.results?.[0].result).toBe("generic_error_handled");
  });
  
  test('flow with mixed retry patterns', async () => {
    // Test complex flow with different retry patterns
    const shared: SharedStorage = {};
    
    const node1 = new FallbackNode(true, 1);
    const node2 = new AsyncFallbackNode(false, 2);
    const node3 = new EventualSuccessNode(2, 3);
    const resultNode = new ResultNode();
    
    node1.next(node2);
    node2.next(node3);
    node3.next(resultNode);
    
    const flow = new Flow(node1);
    await flow.run(shared);
    
    expect(shared.results?.length).toBe(3);
    expect(shared.results?.[0].result).toBe("fallback");
    expect(shared.results?.[1].result).toBe("success");
    expect(shared.results?.[2].result).toBe("success_after_2_attempts");
  });
});
</file>

<file path="tests/flow-basic.test.ts">
// tests/flow_basic.test.ts
import { Node, Flow } from '../src/index';

// Define a shared storage type
type SharedStorage = {
  current?: number;
  execResult?: any;
};

class NumberNode extends Node<SharedStorage, Record<string, unknown>> {
  constructor(
    private number: number,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    shared.current = this.number;
  }
}

class AddNode extends Node<SharedStorage> {
  constructor(
    private number: number,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current += this.number;
    }
  }
}

class MultiplyNode extends Node<SharedStorage> {
  constructor(
    private number: number,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current *= this.number;
    }
  }
}

class CheckPositiveNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async post(shared: SharedStorage): Promise<string> {
    if (shared.current !== undefined && shared.current >= 0) {
      return 'positive';
    } else {
      return 'negative';
    }
  }
}

class NoOpNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(): Promise<void> {
    // Do nothing, just pass
  }
}

// New class to demonstrate Node's retry capabilities
class FlakyNode extends Node<SharedStorage> {
  private attemptCount = 0;

  constructor(
    private failUntilAttempt: number,
    maxRetries: number = 3,
    wait: number = 0.1
  ) {
    super(maxRetries, wait);
  }

  async exec(): Promise<any> {
    this.attemptCount++;

    if (this.attemptCount < this.failUntilAttempt) {
      throw new Error(`Attempt ${this.attemptCount} failed`);
    }

    return `Success on attempt ${this.attemptCount}`;
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: any
  ): Promise<string> {
    shared.execResult = execRes;
    return 'default';
  }
}

// New class to demonstrate using exec method more explicitly
class ExecNode extends Node<SharedStorage> {
  constructor(
    private operation: string,
    maxRetries: number = 1,
    wait: number = 0
  ) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number> {
    // Return the current value for processing in exec
    return shared.current || 0;
  }

  async exec(currentValue: number): Promise<number> {
    switch (this.operation) {
      case 'square':
        return currentValue * currentValue;
      case 'double':
        return currentValue * 2;
      case 'negate':
        return -currentValue;
      default:
        return currentValue;
    }
  }

  async post(
    shared: SharedStorage,
    prepRes: any,
    execRes: any
  ): Promise<string> {
    shared.current = execRes;
    return 'default';
  }
}

describe('Pocket Flow Tests with Node', () => {
  test('single number', async () => {
    const shared: SharedStorage = {};
    const start = new NumberNode(5);
    const pipeline = new Flow(start);
    await pipeline.run(shared);
    expect(shared.current).toBe(5);
  });

  test('sequence with chaining', async () => {
    /**
     * Test a simple linear pipeline:
     * NumberNode(5) -> AddNode(3) -> MultiplyNode(2)
     *
     * Expected result:
     * (5 + 3) * 2 = 16
     */
    const shared: SharedStorage = {};
    const n1 = new NumberNode(5);
    const n2 = new AddNode(3);
    const n3 = new MultiplyNode(2);

    // Chain them in sequence using method chaining
    n1.next(n2).next(n3);

    const pipeline = new Flow(n1);
    await pipeline.run(shared);

    expect(shared.current).toBe(16);
  });

  test('branching positive', async () => {
    /**
     * Test a branching pipeline with positive route:
     * start = NumberNode(5)
     * check = CheckPositiveNode()
     * if 'positive' -> AddNode(10)
     * if 'negative' -> AddNode(-20)
     */
    const shared: SharedStorage = {};
    const start = new NumberNode(5);
    const check = new CheckPositiveNode();
    const addIfPositive = new AddNode(10);
    const addIfNegative = new AddNode(-20);

    // Setup with chaining
    start
      .next(check)
      .on('positive', addIfPositive)
      .on('negative', addIfNegative);

    const pipeline = new Flow(start);
    await pipeline.run(shared);

    expect(shared.current).toBe(15);
  });

  test('negative branch', async () => {
    /**
     * Same branching pipeline, but starting with -5.
     * Final result: (-5) + (-20) = -25.
     */
    const shared: SharedStorage = {};
    const start = new NumberNode(-5);
    const check = new CheckPositiveNode();
    const addIfPositive = new AddNode(10);
    const addIfNegative = new AddNode(-20);

    // Build the flow with chaining
    start
      .next(check)
      .on('positive', addIfPositive)
      .on('negative', addIfNegative);

    const pipeline = new Flow(start);
    await pipeline.run(shared);

    expect(shared.current).toBe(-25);
  });

  test('cycle until negative', async () => {
    /**
     * Demonstrate a cyclical pipeline:
     * Start with 10, check if positive -> subtract 3, then go back to check.
     * Repeat until the number becomes negative.
     */
    const shared: SharedStorage = {};
    const n1 = new NumberNode(10);
    const check = new CheckPositiveNode();
    const subtract3 = new AddNode(-3);
    const noOp = new NoOpNode();

    // Build the cycle with chaining
    n1.next(check).on('positive', subtract3).on('negative', noOp);

    subtract3.next(check);

    const pipeline = new Flow(n1);
    await pipeline.run(shared);

    // final result should be -2: (10 -> 7 -> 4 -> 1 -> -2)
    expect(shared.current).toBe(-2);
  });

  // New tests demonstrating Node features

  test('retry functionality', async () => {
    const shared: SharedStorage = {};

    // This node will fail on the first attempt but succeed on the second
    const flakyNode = new FlakyNode(2, 3, 0.01);

    const pipeline = new Flow(flakyNode);
    await pipeline.run(shared);

    // Check that we got a success message indicating it was the second attempt
    expect(shared.execResult).toBe('Success on attempt 2');
  });

  test('retry with fallback', async () => {
    const shared: SharedStorage = {};

    // This node will always fail (requires 5 attempts, but we only allow 2)
    const flakyNode = new FlakyNode(5, 2, 0.01);

    // Override the execFallback method to handle the failure
    flakyNode.execFallback = async (
      prepRes: any,
      error: Error
    ): Promise<any> => {
      return 'Fallback executed due to failure';
    };

    const pipeline = new Flow(flakyNode);
    await pipeline.run(shared);

    // Check that we got the fallback result
    expect(shared.execResult).toBe('Fallback executed due to failure');
  });

  test('exec method processing', async () => {
    const shared: SharedStorage = { current: 5 };

    const squareNode = new ExecNode('square');
    const doubleNode = new ExecNode('double');
    const negateNode = new ExecNode('negate');

    squareNode.next(doubleNode).next(negateNode);

    const pipeline = new Flow(squareNode);
    await pipeline.run(shared);

    // 5 → square → 25 → double → 50 → negate → -50
    expect(shared.current).toBe(-50);
  });
});
</file>

<file path="tests/flow-composition.test.ts">
// tests/flow_composition.test.ts
import { Node, Flow } from '../src/index';

// Define a shared storage type
type SharedStorage = {
  current?: number;
};

class NumberNode extends Node<SharedStorage> {
  constructor(private number: number, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    shared.current = this.number;
  }
}

class AddNode extends Node<SharedStorage> {
  constructor(private number: number, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current += this.number;
    }
  }
}

class MultiplyNode extends Node<SharedStorage> {
  constructor(private number: number, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<void> {
    if (shared.current !== undefined) {
      shared.current *= this.number;
    }
  }
}

describe('Flow Composition Tests with Node', () => {
  test('flow as node', async () => {
    /**
     * 1) Create a Flow (f1) starting with NumberNode(5), then AddNode(10), then MultiplyNode(2).
     * 2) Create a second Flow (f2) whose start is f1.
     * 3) Create a wrapper Flow (f3) that contains f2 to ensure proper execution.
     * Expected final result in shared.current: (5 + 10) * 2 = 30.
     */
    const shared: SharedStorage = {};
    
    // Inner flow f1
    const numberNode = new NumberNode(5);
    const addNode = new AddNode(10);
    const multiplyNode = new MultiplyNode(2);
    
    numberNode.next(addNode);
    addNode.next(multiplyNode);
    
    const f1 = new Flow(numberNode);
    
    // f2 starts with f1
    const f2 = new Flow(f1);
    
    // Wrapper flow f3 to ensure proper execution
    const f3 = new Flow(f2);
    await f3.run(shared);
    
    expect(shared.current).toBe(30);
  });
  
  test('nested flow', async () => {
    /**
     * Demonstrates nested flows with proper wrapping:
     * inner_flow: NumberNode(5) -> AddNode(3)
     * middle_flow: starts with inner_flow -> MultiplyNode(4)
     * wrapper_flow: contains middle_flow to ensure proper execution
     * Expected final result: (5 + 3) * 4 = 32.
     */
    const shared: SharedStorage = {};
    
    // Build the inner flow
    const numberNode = new NumberNode(5);
    const addNode = new AddNode(3);
    numberNode.next(addNode);
    const innerFlow = new Flow(numberNode);
    
    // Build the middle flow, whose start is the inner flow
    const multiplyNode = new MultiplyNode(4);
    innerFlow.next(multiplyNode);
    const middleFlow = new Flow(innerFlow);
    
    // Wrapper flow to ensure proper execution
    const wrapperFlow = new Flow(middleFlow);
    await wrapperFlow.run(shared);
    
    expect(shared.current).toBe(32);
  });
  
  test('flow chaining flows', async () => {
    /**
     * Demonstrates chaining two flows with proper wrapping:
     * flow1: NumberNode(10) -> AddNode(10) # final = 20
     * flow2: MultiplyNode(2) # final = 40
     * wrapper_flow: contains both flow1 and flow2 to ensure proper execution
     * Expected final result: (10 + 10) * 2 = 40.
     */
    const shared: SharedStorage = {};
    
    // flow1
    const numberNode = new NumberNode(10);
    const addNode = new AddNode(10);
    numberNode.next(addNode);
    const flow1 = new Flow(numberNode);
    
    // flow2
    const multiplyNode = new MultiplyNode(2);
    const flow2 = new Flow(multiplyNode);
    
    // Chain flow1 to flow2
    flow1.next(flow2);
    
    // Wrapper flow to ensure proper execution
    const wrapperFlow = new Flow(flow1);
    await wrapperFlow.run(shared);
    
    expect(shared.current).toBe(40);
  });
  
  test('flow with retry handling', async () => {
    /**
     * Demonstrates retry capabilities in flow composition:
     * Using nodes with retry logic in a flow structure
     */
    const shared: SharedStorage = {};
    
    // Create a faulty NumberNode that will fail once before succeeding
    class FaultyNumberNode extends Node<SharedStorage> {
      private attempts = 0;
      
      constructor(private number: number, maxRetries: number = 2, wait: number = 0.01) {
        super(maxRetries, wait);
      }
      
      async prep(shared: SharedStorage): Promise<number> {
        return this.number;
      }
      
      async exec(number: number): Promise<number> {
        this.attempts++;
        if (this.attempts === 1) {
          throw new Error("Simulated failure on first attempt");
        }
        return number;
      }
      
      async post(shared: SharedStorage, prepRes: any, execRes: any): Promise<string> {
        shared.current = execRes;
        return "default";
      }
    }
    
    // Create a flow with the faulty node followed by standard nodes
    const faultyNumberNode = new FaultyNumberNode(5);
    const addNode = new AddNode(10);
    const multiplyNode = new MultiplyNode(2);
    
    faultyNumberNode.next(addNode);
    addNode.next(multiplyNode);
    
    const flow = new Flow(faultyNumberNode);
    await flow.run(shared);
    
    // Despite the initial failure, the retry mechanism should allow
    // the flow to complete successfully: (5 + 10) * 2 = 30
    expect(shared.current).toBe(30);
  });
  
  test('nested flows with mixed retry configurations', async () => {
    /**
     * Demonstrates nesting flows with different retry configurations
     */
    const shared: SharedStorage = {};
    
    // Inner flow with a node that has high retry count
    const numberNode = new NumberNode(5, 5, 0.01); // 5 retries
    const addNode = new AddNode(3, 2, 0.01);      // 2 retries
    numberNode.next(addNode);
    const innerFlow = new Flow(numberNode);
    
    // Middle flow with a node that has medium retry count
    const multiplyNode = new MultiplyNode(4, 3, 0.01); // 3 retries
    innerFlow.next(multiplyNode);
    const middleFlow = new Flow(innerFlow);
    
    // Wrapper flow
    const wrapperFlow = new Flow(middleFlow);
    await wrapperFlow.run(shared);
    
    // The result should be the same: (5 + 3) * 4 = 32
    expect(shared.current).toBe(32);
  });
});
</file>

<file path="tests/mapreduce-pattern.test.ts">
// tests/mapreduce-pattern.test.ts
import { BatchNode, Node, Flow, ParallelBatchNode } from '../src/index';

// Mock utility function to simulate LLM calls
async function callLLM(prompt: string): Promise<string> {
  // In a real implementation, this would call an actual LLM
  return `Summary for: ${prompt.slice(0, 20)}...`;
}

// Define shared storage type for MapReduce pattern
type MapReduceSharedStorage = {
  files?: Record<string, string>;
  file_summaries?: Record<string, string>;
  all_files_summary?: string;
  text_to_process?: string;
  text_chunks?: string[];
  processed_chunks?: string[];
  final_result?: string;
};

// Document Summarization Example (Sequential)
class SummarizeAllFiles extends BatchNode<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<[string, string][]> {
    const files = shared.files || {};
    return Object.entries(files);
  }

  async exec(one_file: [string, string]): Promise<[string, string]> {
    const [filename, file_content] = one_file;
    const summary_text = await callLLM(
      `Summarize the following file:\n${file_content}`
    );
    return [filename, summary_text];
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: [string, string][],
    execRes: [string, string][]
  ): Promise<string | undefined> {
    shared.file_summaries = Object.fromEntries(execRes);
    return "summarized";
  }
}

class CombineSummaries extends Node<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<Record<string, string>> {
    return shared.file_summaries || {};
  }

  async exec(file_summaries: Record<string, string>): Promise<string> {
    // Format as: "File1: summary\nFile2: summary...\n"
    const text_list: string[] = [];
    for (const [fname, summ] of Object.entries(file_summaries)) {
      text_list.push(`${fname} summary:\n${summ}\n`);
    }
    const big_text = text_list.join("\n---\n");

    return await callLLM(
      `Combine these file summaries into one final summary:\n${big_text}`
    );
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: Record<string, string>,
    final_summary: string
  ): Promise<string | undefined> {
    shared.all_files_summary = final_summary;
    return "combined";
  }
}

// Generic MapReduce Example with Parallel Processing
class MapChunks extends ParallelBatchNode<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<string[]> {
    // Split text into chunks
    const text = shared.text_to_process || "";
    const chunkSize = 10;
    const chunks: string[] = [];
    
    for (let i = 0; i < text.length; i += chunkSize) {
      chunks.push(text.slice(i, i + chunkSize));
    }
    
    shared.text_chunks = chunks;
    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    // Process each chunk (map phase)
    // In a real application, this could be any transformation
    return chunk.toUpperCase();
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.processed_chunks = execRes;
    return "mapped";
  }
}

class ReduceResults extends Node<MapReduceSharedStorage> {
  async prep(shared: MapReduceSharedStorage): Promise<string[]> {
    return shared.processed_chunks || [];
  }

  async exec(processedChunks: string[]): Promise<string> {
    // Combine processed chunks (reduce phase)
    // In a real application, this could be any aggregation function
    return processedChunks.join(" + ");
  }

  async post(
    shared: MapReduceSharedStorage,
    prepRes: string[],
    result: string
  ): Promise<string | undefined> {
    shared.final_result = result;
    return "reduced";
  }
}

// Tests for the MapReduce pattern
describe('MapReduce Pattern Tests', () => {
  // Test Document Summarization Example
  test('Document Summarization MapReduce', async () => {
    // Create and connect nodes
    const batchNode = new SummarizeAllFiles();
    const combineNode = new CombineSummaries();
    
    batchNode.on("summarized", combineNode);
    
    const flow = new Flow(batchNode);
    
    // Prepare test data
    const shared: MapReduceSharedStorage = {
      files: {
        "file1.txt": "Alice was beginning to get very tired of sitting by her sister...",
        "file2.txt": "Some other interesting text ...",
        "file3.txt": "Yet another file with some content to summarize..."
      }
    };
    
    // Run the flow
    await flow.run(shared);
    
    // Verify results
    expect(shared.file_summaries).toBeDefined();
    expect(Object.keys(shared.file_summaries || {}).length).toBe(3);
    expect(shared.all_files_summary).toBeDefined();
    expect(typeof shared.all_files_summary).toBe('string');
  });
  
  // Test Generic MapReduce with ParallelBatchNode
  test('Parallel Text Processing MapReduce', async () => {
    // Create and connect nodes
    const mapNode = new MapChunks();
    const reduceNode = new ReduceResults();
    
    mapNode.on("mapped", reduceNode);
    
    const flow = new Flow(mapNode);
    
    // Prepare test data
    const shared: MapReduceSharedStorage = {
      text_to_process: "This is a longer text that will be processed in parallel using the MapReduce pattern."
    };
    
    // Run the flow
    await flow.run(shared);
    
    // Verify results
    expect(shared.text_chunks).toBeDefined();
    expect(shared.text_chunks?.length).toBeGreaterThan(0);
    expect(shared.processed_chunks).toBeDefined();
    expect(shared.processed_chunks?.length).toBe(shared.text_chunks?.length);
    expect(shared.final_result).toBeDefined();
    expect(typeof shared.final_result).toBe('string');
    
    // Verify the content is actually transformed
    expect(shared.processed_chunks?.every(chunk => 
      chunk === chunk.toUpperCase()
    )).toBe(true);
  });
  
  // Test changing chunk size affects parallel processing
  test('Varying Chunk Size in MapReduce', async () => {
    // This test demonstrates how chunk size affects the MapReduce process
    
    // Create a custom MapChunks class with configurable chunk size
    class ConfigurableMapChunks extends ParallelBatchNode<MapReduceSharedStorage> {
      private chunkSize: number;
      
      constructor(chunkSize: number) {
        super();
        this.chunkSize = chunkSize;
      }
      
      async prep(shared: MapReduceSharedStorage): Promise<string[]> {
        const text = shared.text_to_process || "";
        const chunks: string[] = [];
        
        for (let i = 0; i < text.length; i += this.chunkSize) {
          chunks.push(text.slice(i, i + this.chunkSize));
        }
        
        shared.text_chunks = chunks;
        return chunks;
      }
      
      async exec(chunk: string): Promise<string> {
        return chunk.toUpperCase();
      }
      
      async post(
        shared: MapReduceSharedStorage,
        prepRes: string[],
        execRes: string[]
      ): Promise<string | undefined> {
        shared.processed_chunks = execRes;
        return "mapped";
      }
    }
    
    // Test with small chunk size
    const mapNodeSmall = new ConfigurableMapChunks(5);
    const reduceNodeSmall = new ReduceResults();
    mapNodeSmall.on("mapped", reduceNodeSmall);
    const flowSmall = new Flow(mapNodeSmall);
    
    // Test with larger chunk size
    const mapNodeLarge = new ConfigurableMapChunks(20);
    const reduceNodeLarge = new ReduceResults();
    mapNodeLarge.on("mapped", reduceNodeLarge);
    const flowLarge = new Flow(mapNodeLarge);
    
    // Same input for both flows
    const text = "This is a test text for demonstrating chunk size effects.";
    
    // Run with small chunks
    const sharedSmall: MapReduceSharedStorage = {
      text_to_process: text
    };
    await flowSmall.run(sharedSmall);
    
    // Run with large chunks
    const sharedLarge: MapReduceSharedStorage = {
      text_to_process: text
    };
    await flowLarge.run(sharedLarge);
    
    // Verify different chunk counts
    if (sharedSmall.text_chunks && sharedLarge.text_chunks) {
      expect(sharedSmall.text_chunks.length).toBeGreaterThan(sharedLarge.text_chunks.length);
    }
    
    // Verify end results are identical despite different chunking
    expect(sharedSmall.final_result?.replace(/\s\+\s/g, '')).toBe(
      sharedLarge.final_result?.replace(/\s\+\s/g, '')
    );
  });
});
</file>

<file path="tests/multi-agent-pattern.test.ts">
// tests/multi-agent-pattern.test.ts
import { Node, Flow } from '../src/index';

// Define shared storage with message queue for basic agent communication
type MessageQueueSharedStorage = {
  messages: string[];
  processedMessages: string[];
  processing?: boolean;
};

// Mock utility function to simulate LLM calls
function mockLLM(prompt: string): string {
  // Simple mock LLM that responds based on the prompt
  if (prompt.includes('Generate hint')) {
    return 'This is a hint: Something cold on a stick';
  } else if (prompt.includes('Guess')) {
    return 'popsicle';
  }
  return `Response to: ${prompt.substring(0, 20)}...`;
}

// Basic Agent Communication Example
class ListenerAgent extends Node<MessageQueueSharedStorage> {
  async prep(shared: MessageQueueSharedStorage): Promise<string | undefined> {
    // Check if there are messages to process
    if (shared.messages.length === 0) {
      return undefined;
    }
    // Get the next message
    return shared.messages.shift();
  }

  async exec(message: string | undefined): Promise<string | undefined> {
    if (!message) {
      return undefined;
    }
    
    // Process the message (in real implementation, this could call an LLM)
    const response = `Processed: ${message}`;
    return response;
  }

  async post(
    shared: MessageQueueSharedStorage,
    prepRes: string | undefined,
    execRes: string | undefined
  ): Promise<string> {
    if (execRes) {
      // Store the processed message
      shared.processedMessages.push(execRes);
    }
    
    if (shared.messages.length === 0) {
      // Add a small delay to avoid tight loop CPU consumption in real implementation
      return "finished";
    }
    
    // Continue processing messages
    return "continue";
  }
}

// Taboo Game Example
// Define shared storage for the game
type TabooGameSharedStorage = {
  targetWord: string;
  forbiddenWords: string[];
  pastGuesses: string[];
  hinterQueue: string[];
  guesserQueue: string[];
  gameOver: boolean;
  maxRounds: number;
  currentRound: number;
  isCorrectGuess: boolean;
};

// Hinter agent that provides clues
class Hinter extends Node<TabooGameSharedStorage> {
  async prep(shared: TabooGameSharedStorage): Promise<any> {
    if (shared.gameOver) {
      return null;
    }

    // In test, we'll simulate waiting for a message by checking if it's our turn
    if (shared.hinterQueue.length === 0) {
      return null;
    }

    const message = shared.hinterQueue.shift();
    
    return {
      target: shared.targetWord,
      forbidden: shared.forbiddenWords,
      pastGuesses: shared.pastGuesses,
      message
    };
  }

  async exec(input: any): Promise<string | null> {
    if (!input) return null;
    
    // Generate a hint using mock LLM
    const prompt = `Generate hint for word "${input.target}" without using forbidden words: ${input.forbidden.join(', ')}`;
    const hint = mockLLM(prompt);
    return hint;
  }

  async post(
    shared: TabooGameSharedStorage,
    prepRes: any,
    hint: string | null
  ): Promise<string | undefined> {
    if (!hint) {
      if (shared.gameOver) {
        return "finished";
      }
      return "continue_hinter";
    }
    
    // Send hint to guesser
    shared.guesserQueue.push(hint);
    shared.currentRound++;
    
    return "continue_hinter";
  }
}

// Guesser agent that tries to guess the target word
class Guesser extends Node<TabooGameSharedStorage> {
  async prep(shared: TabooGameSharedStorage): Promise<string | null> {
    if (shared.gameOver) {
      return null;
    }

    // Wait for a hint from the hinter
    if (shared.guesserQueue.length === 0) {
      return null;
    }

    return shared.guesserQueue.shift() || null;
  }

  async exec(hint: string | null): Promise<string | null> {
    if (!hint) return null;
    
    // Generate a guess using mock LLM
    const prompt = `Guess the word based on the hint: ${hint}`;
    const guess = mockLLM(prompt);
    return guess;
  }

  async post(
    shared: TabooGameSharedStorage,
    hint: string | null,
    guess: string | null
  ): Promise<string | undefined> {
    if (!guess) {
      if (shared.gameOver) {
        return "finished";
      }
      return "continue_guesser";
    }
    
    // Record the guess
    shared.pastGuesses.push(guess);
    
    // Check if the guess is correct
    if (guess.toLowerCase() === shared.targetWord.toLowerCase()) {
      shared.isCorrectGuess = true;
      shared.gameOver = true;
      return "finished";
    }
    
    // Check if we've reached maximum rounds
    if (shared.currentRound >= shared.maxRounds) {
      shared.gameOver = true;
      return "finished";
    }
    
    // Send message to hinter for next round
    shared.hinterQueue.push("next_hint");
    
    return "continue_guesser";
  }
}

// Tests for Multi-Agent pattern
describe('Multi-Agent Pattern Tests', () => {
  // Test basic agent message queue
  test('Basic Agent Message Queue', async () => {
    // Create agent node
    const agent = new ListenerAgent();
    agent.on("continue", agent); // Connect to self to continue processing
    
    // Create flow
    const flow = new Flow(agent);
    
    // Create shared storage with messages
    const shared: MessageQueueSharedStorage = {
      messages: [
        "System status: all systems operational",
        "Memory usage: normal",
        "Network connectivity: stable",
        "Processing load: optimal",
      ],
      processedMessages: []
    };
    
    // Run the flow
    await flow.run(shared);
    
    // Verify results
    expect(shared.messages.length).toBe(0);
    expect(shared.processedMessages.length).toBe(4);
    expect(shared.processedMessages[0]).toBe("Processed: System status: all systems operational");
  });
  
  // Test Taboo game multi-agent interaction
  test('Taboo Game Multi-Agent Interaction', async () => {
    // Create the agents
    const hinter = new Hinter();
    const guesser = new Guesser();
    
    // Connect agents
    hinter.on("continue_hinter", hinter);
    guesser.on("continue_guesser", guesser);
    
    // Create shared game state
    const shared: TabooGameSharedStorage = {
      targetWord: "popsicle",
      forbiddenWords: ["ice", "cream", "frozen", "stick", "summer"],
      pastGuesses: [],
      hinterQueue: ["start_game"], // Initial message to start the game
      guesserQueue: [],
      gameOver: false,
      maxRounds: 3,
      currentRound: 0,
      isCorrectGuess: false
    };
    
    // Create flows
    const hinterFlow = new Flow(hinter);
    const guesserFlow = new Flow(guesser);
    
    // Run both flows concurrently to simulate multi-agent interaction
    const hinterPromise = hinterFlow.run(shared);
    const guesserPromise = guesserFlow.run(shared);
    
    // Wait for both to finish
    await Promise.all([hinterPromise, guesserPromise]);
    
    // Verify results
    expect(shared.gameOver).toBe(true);
    expect(shared.pastGuesses.length).toBeGreaterThan(0);
    expect(shared.isCorrectGuess).toBe(true);
  });
  
  // Test changing agent behavior with different parameters
  test('Configurable Agent Behavior', async () => {
    // Create a configurable agent that can be adjusted for testing
    class ConfigurableAgent extends Node<MessageQueueSharedStorage> {
      private processingDelay: number;
      
      constructor(processingDelay: number = 0) {
        super();
        this.processingDelay = processingDelay;
      }
      
      async prep(shared: MessageQueueSharedStorage): Promise<string | undefined> {
        if (shared.messages.length === 0) {
          return undefined;
        }
        return shared.messages.shift();
      }
      
      async exec(message: string | undefined): Promise<string | undefined> {
        if (!message) {
          return undefined;
        }
        
        // Simulate processing time
        if (this.processingDelay > 0) {
          await new Promise(resolve => setTimeout(resolve, this.processingDelay));
        }
        
        return `Processed with ${this.processingDelay}ms delay: ${message}`;
      }
      
      async post(
        shared: MessageQueueSharedStorage,
        prepRes: string | undefined,
        execRes: string | undefined
      ): Promise<string> {
        if (execRes) {
          shared.processedMessages.push(execRes);
        }
        
        if (shared.messages.length === 0) {
          return "finished";
        }
        return "continue";
      }
    }
    
    // Test with fast agent
    const fastAgent = new ConfigurableAgent(0);
    fastAgent.on("continue", fastAgent);
    const fastFlow = new Flow(fastAgent);
    
    const fastShared: MessageQueueSharedStorage = {
      messages: ["Message 1", "Message 2", "Message 3"],
      processedMessages: []
    };
    
    await fastFlow.run(fastShared);
    
    // Test with slow agent
    const slowAgent = new ConfigurableAgent(10);
    slowAgent.on("continue", slowAgent);
    const slowFlow = new Flow(slowAgent);
    
    const slowShared: MessageQueueSharedStorage = {
      messages: ["Message 1", "Message 2", "Message 3"],
      processedMessages: []
    };
    
    await slowFlow.run(slowShared);
    
    // Verify both processed all messages
    expect(fastShared.processedMessages.length).toBe(3);
    expect(slowShared.processedMessages.length).toBe(3);
    
    // Verify processing indicators in the output
    expect(fastShared.processedMessages[0]).toContain("Processed with 0ms delay");
    expect(slowShared.processedMessages[0]).toContain("Processed with 10ms delay");
  });
});
</file>

<file path="tests/parallel-batch-flow.test.ts">
// tests/parallel-batch-flow.test.ts
import { Node, ParallelBatchNode, Flow, ParallelBatchFlow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  batches?: number[][];
  processedNumbers?: Record<number, number[]>;
  total?: number;
};

class AsyncParallelNumberProcessor extends ParallelBatchNode<
  SharedStorage,
  { batchId: number }
> {
  private delay: number;

  constructor(delay: number = 0.1, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.delay = delay;
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    const batchId = this._params.batchId;
    return shared.batches?.[batchId] || [];
  }

  async exec(number: number): Promise<number> {
    // Simulate async processing
    await new Promise((resolve) => setTimeout(resolve, this.delay * 1000));
    return number * 2;
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number[]
  ): Promise<string | undefined> {
    if (!shared.processedNumbers) {
      shared.processedNumbers = {};
    }
    shared.processedNumbers[this._params.batchId] = execRes;
    return 'processed';
  }
}

class AsyncAggregatorNode extends Node<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    // Combine all batch results in order
    const allResults: number[] = [];
    const processed = shared.processedNumbers || {};

    for (let i = 0; i < Object.keys(processed).length; i++) {
      allResults.push(...processed[i]);
    }

    return allResults;
  }

  async exec(prepResult: number[]): Promise<number> {
    await new Promise((resolve) => setTimeout(resolve, 10));
    return prepResult.reduce((sum, val) => sum + val, 0);
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number
  ): Promise<string | undefined> {
    shared.total = execRes;
    return 'aggregated';
  }
}

// Custom ParallelBatchFlow that processes batches based on batchId
class TestParallelBatchFlow extends ParallelBatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, any>[]> {
    return (shared.batches || []).map((_, i) => ({ batchId: i }));
  }
}

describe('ParallelBatchFlow Tests', () => {
  test('parallel batch flow', async () => {
    /**
     * Test basic parallel batch processing flow with batch IDs
     */
    const shared: SharedStorage = {
      batches: [
        [1, 2, 3], // batchId: 0
        [4, 5, 6], // batchId: 1
        [7, 8, 9], // batchId: 2
      ],
    };

    const processor = new AsyncParallelNumberProcessor(0.1);
    const aggregator = new AsyncAggregatorNode();

    processor.on('processed', aggregator);
    const flow = new TestParallelBatchFlow(processor);

    const startTime = Date.now();
    await flow.run(shared);
    const executionTime = (Date.now() - startTime) / 1000;

    // Verify each batch was processed correctly
    const expectedBatchResults = {
      0: [2, 4, 6], // [1,2,3] * 2
      1: [8, 10, 12], // [4,5,6] * 2
      2: [14, 16, 18], // [7,8,9] * 2
    };

    expect(shared.processedNumbers).toEqual(expectedBatchResults);

    // Verify total
    const expectedTotal = shared
      .batches!.flat()
      .reduce((sum, num) => sum + num * 2, 0);
    expect(shared.total).toBe(expectedTotal);

    // Verify parallel execution
    expect(executionTime).toBeLessThan(0.2);
  });

  test('error handling', async () => {
    /**
     * Test error handling in parallel batch flow
     */
    class ErrorProcessor extends AsyncParallelNumberProcessor {
      async exec(item: number): Promise<number> {
        if (item === 2) {
          throw new Error(`Error processing item ${item}`);
        }
        return item;
      }
    }

    const shared: SharedStorage = {
      batches: [
        [1, 2, 3], // Contains error-triggering value
        [4, 5, 6],
      ],
    };

    const processor = new ErrorProcessor();
    const flow = new TestParallelBatchFlow(processor);

    await expect(async () => {
      await flow.run(shared);
    }).rejects.toThrow('Error processing item 2');
  });

  test('multiple batch sizes', async () => {
    /**
     * Test parallel batch flow with varying batch sizes
     */
    const shared: SharedStorage = {
      batches: [
        [1], // batchId: 0
        [2, 3, 4], // batchId: 1
        [5, 6], // batchId: 2
        [7, 8, 9, 10], // batchId: 3
      ],
    };

    const processor = new AsyncParallelNumberProcessor(0.05);
    const aggregator = new AsyncAggregatorNode();

    processor.on('processed', aggregator);
    const flow = new TestParallelBatchFlow(processor);

    await flow.run(shared);

    // Verify each batch was processed correctly
    const expectedBatchResults = {
      0: [2], // [1] * 2
      1: [4, 6, 8], // [2,3,4] * 2
      2: [10, 12], // [5,6] * 2
      3: [14, 16, 18, 20], // [7,8,9,10] * 2
    };

    expect(shared.processedNumbers).toEqual(expectedBatchResults);

    // Verify total
    const expectedTotal = shared
      .batches!.flat()
      .reduce((sum, num) => sum + num * 2, 0);
    expect(shared.total).toBe(expectedTotal);
  });
});
</file>

<file path="tests/parallel-batch-node.test.ts">
// tests/parallel-batch-node.test.ts
import { ParallelBatchNode, Flow } from '../src/index';

// Define shared storage type
type SharedStorage = {
  inputNumbers?: number[];
  processedNumbers?: number[];
  executionOrder?: number[];
  finalResults?: number[];
};

class AsyncParallelNumberProcessor extends ParallelBatchNode<SharedStorage> {
  private delay: number;

  constructor(delay: number = 0.1, maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
    this.delay = delay;
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    return shared.inputNumbers || [];
  }

  async exec(number: number): Promise<number> {
    await new Promise((resolve) => setTimeout(resolve, this.delay * 1000));
    return number * 2;
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number[]
  ): Promise<string | undefined> {
    shared.processedNumbers = execRes;
    return 'processed';
  }
}

class ErrorProcessor extends ParallelBatchNode<SharedStorage> {
  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    return shared.inputNumbers || [];
  }

  async exec(item: number): Promise<number> {
    if (item === 2) {
      throw new Error(`Error processing item ${item}`);
    }
    return item;
  }
}

class OrderTrackingProcessor extends ParallelBatchNode<SharedStorage> {
  private executionOrder: number[] = [];

  constructor(maxRetries: number = 1, wait: number = 0) {
    super(maxRetries, wait);
  }

  async prep(shared: SharedStorage): Promise<number[]> {
    this.executionOrder = [];
    shared.executionOrder = this.executionOrder;
    return shared.inputNumbers || [];
  }

  async exec(item: number): Promise<number> {
    const delay = item % 2 === 0 ? 0.1 : 0.05;
    await new Promise((resolve) => setTimeout(resolve, delay * 1000));
    this.executionOrder.push(item);
    return item;
  }

  async post(
    shared: SharedStorage,
    prepRes: number[],
    execRes: number[]
  ): Promise<string | undefined> {
    shared.executionOrder = this.executionOrder;
    return undefined;
  }
}

describe('AsyncParallelBatchNode Tests', () => {
  test('parallel processing', async () => {
    // Test that numbers are processed in parallel by measuring execution time
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: 5 }, (_, i) => i),
    };

    const processor = new AsyncParallelNumberProcessor(0.1);

    // Record start time
    const startTime = Date.now();
    await processor.run(shared);
    const endTime = Date.now();

    // Check results
    const expected = [0, 2, 4, 6, 8]; // Each number doubled
    expect(shared.processedNumbers).toEqual(expected);

    // Since processing is parallel, total time should be approximately
    // equal to the delay of a single operation, not delay * number_of_items
    const executionTime = endTime - startTime;
    expect(executionTime).toBeLessThan(200); // Should be around 100ms plus minimal overhead
  });

  test('empty input', async () => {
    // Test processing of empty input
    const shared: SharedStorage = {
      inputNumbers: [],
    };

    const processor = new AsyncParallelNumberProcessor();
    await processor.run(shared);

    expect(shared.processedNumbers).toEqual([]);
  });

  test('single item', async () => {
    // Test processing of a single item
    const shared: SharedStorage = {
      inputNumbers: [42],
    };

    const processor = new AsyncParallelNumberProcessor();
    await processor.run(shared);

    expect(shared.processedNumbers).toEqual([84]);
  });

  test('large batch', async () => {
    // Test processing of a large batch of numbers
    const inputSize = 100;
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: inputSize }, (_, i) => i),
    };

    const processor = new AsyncParallelNumberProcessor(0.01);
    await processor.run(shared);

    const expected = Array.from({ length: inputSize }, (_, i) => i * 2);
    expect(shared.processedNumbers).toEqual(expected);
  });

  test('error handling', async () => {
    // Test error handling during parallel processing
    const shared: SharedStorage = {
      inputNumbers: [1, 2, 3],
    };

    const processor = new ErrorProcessor();

    await expect(async () => {
      await processor.run(shared);
    }).rejects.toThrow('Error processing item 2');
  });

  test('concurrent execution', async () => {
    // Test that tasks are actually running concurrently by tracking execution order
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: 4 }, (_, i) => i), // [0, 1, 2, 3]
    };

    const processor = new OrderTrackingProcessor();
    await processor.run(shared);

    // Odd numbers should finish before even numbers due to shorter delay
    expect(shared.executionOrder?.indexOf(1)).toBeLessThan(
      shared.executionOrder?.indexOf(0) as number
    );
    expect(shared.executionOrder?.indexOf(3)).toBeLessThan(
      shared.executionOrder?.indexOf(2) as number
    );
  });

  test('integration with Flow', async () => {
    // Test integration with Flow
    const shared: SharedStorage = {
      inputNumbers: Array.from({ length: 5 }, (_, i) => i),
    };

    class ProcessResultsNode extends ParallelBatchNode<SharedStorage> {
      async prep(shared: SharedStorage): Promise<number[]> {
        return shared.processedNumbers || [];
      }

      async exec(num: number): Promise<number> {
        return num + 1;
      }

      async post(
        shared: SharedStorage,
        prepRes: number[],
        execRes: number[]
      ): Promise<string | undefined> {
        shared.finalResults = execRes;
        return 'completed';
      }
    }

    const processor = new AsyncParallelNumberProcessor();
    const resultsProcessor = new ProcessResultsNode();

    processor.on('processed', resultsProcessor);

    const pipeline = new Flow(processor);
    await pipeline.run(shared);

    // Each number should be doubled and then incremented
    const expected = [1, 3, 5, 7, 9];
    expect(shared.finalResults).toEqual(expected);
  });
});
</file>

<file path="tests/qa-pattern.test.ts">
// tests/qa-pattern.test.ts
import { Node, Flow } from '../src/index';

// Mock the prompt function since we're in a test environment
const mockUserInput = "What is PocketFlow?";
global.prompt = jest.fn().mockImplementation(() => mockUserInput);

// Mock utility function for LLM calls
async function callLlm(question: string): Promise<string> {
  // Simple mock LLM call that returns a predefined answer based on the question
  if (question.includes("PocketFlow")) {
    return "PocketFlow is a TypeScript library for building reliable AI pipelines with a focus on composition and reusability.";
  }
  return "I don't know the answer to that question.";
}

// Define the shared store type as shown in the guide
interface QASharedStore {
  question?: string;
  answer?: string;
  [key: string]: unknown;
}

// Implement the GetQuestionNode from the guide
class GetQuestionNode extends Node<QASharedStore> {
  async exec(_: unknown): Promise<string> {
    // Get question directly from user input
    const userQuestion = prompt("Enter your question: ") || "";
    return userQuestion;
  }

  async post(
    shared: QASharedStore,
    _: unknown,
    execRes: string
  ): Promise<string | undefined> {
    // Store the user's question
    shared.question = execRes;
    return "default"; // Go to the next node
  }
}

// Implement the AnswerNode from the guide
class AnswerNode extends Node<QASharedStore> {
  async prep(shared: QASharedStore): Promise<string> {
    // Read question from shared
    return shared.question || "";
  }

  async exec(question: string): Promise<string> {
    // Call LLM to get the answer
    return await callLlm(question);
  }

  async post(
    shared: QASharedStore,
    _: unknown,
    execRes: string
  ): Promise<string | undefined> {
    // Store the answer in shared
    shared.answer = execRes;
    return undefined;
  }
}

// Create a function to set up the QA flow
function createQaFlow(): Flow {
  // Create nodes
  const getQuestionNode = new GetQuestionNode();
  const answerNode = new AnswerNode();

  // Connect nodes in sequence
  getQuestionNode.next(answerNode);

  // Create flow starting with input node
  return new Flow(getQuestionNode);
}

// Tests for the QA pattern
describe('QA Pattern Tests', () => {
  // Test the basic QA flow
  test('Basic QA Flow with mocked user input', async () => {
    // Create shared store
    const shared: QASharedStore = {
      question: undefined,
      answer: undefined,
    };
    
    // Create and run the flow
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    
    // Verify results
    expect(shared.question).toBe(mockUserInput);
    expect(shared.answer).toBe("PocketFlow is a TypeScript library for building reliable AI pipelines with a focus on composition and reusability.");
  });
  
  // Test with a different question (simulating a different user input)
  test('QA Flow with unknown question', async () => {
    // Change the mock implementation for this test
    global.prompt = jest.fn().mockImplementation(() => "What is the meaning of life?");
    
    const shared: QASharedStore = {
      question: undefined,
      answer: undefined,
    };
    
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    
    expect(shared.question).toBe("What is the meaning of life?");
    expect(shared.answer).toBe("I don't know the answer to that question.");
  });
  
  // Test error handling (missing question)
  test('QA Flow with missing question', async () => {
    // Mock a null or empty response
    global.prompt = jest.fn().mockImplementation(() => "");
    
    const shared: QASharedStore = {
      question: undefined,
      answer: undefined,
    };
    
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    
    expect(shared.question).toBe("");
    expect(shared.answer).toBe("I don't know the answer to that question.");
  });
});
</file>

<file path="tests/rag-pattern.test.ts">
// tests/rag-pattern.test.ts
import { BaseNode, Node, BatchNode, Flow } from '../src/index';

// Mock utility functions to simulate real operations
async function getEmbedding(text: string): Promise<number[]> {
  // Simple mock embedding - converts string to vector of character codes
  // In real applications, this would call an embedding API
  return Array.from(text.substring(0, 5)).map(char => char.charCodeAt(0));
}

async function createIndex(embeddings: number[][]): Promise<{ embeddings: number[][] }> {
  // Simple mock index creation
  return { embeddings };
}

async function searchIndex(index: { embeddings: number[][] }, queryEmbedding: number[], options: { topK: number }): Promise<[number[][], number[][]]> {
  // Mock search function that returns indices and distances
  // In real applications, this would do vector similarity search
  const similarities = index.embeddings.map((emb, idx) => {
    // Simple dot product as similarity
    const similarity = emb.reduce((sum, val, i) => sum + val * (queryEmbedding[i] || 0), 0);
    return [idx, similarity];
  });
  
  // Sort by similarity (descending)
  similarities.sort((a, b) => b[1] - a[1]);
  
  // Return top-k indices and distances
  const topK = Math.min(options.topK, similarities.length);
  const indices = [similarities.slice(0, topK).map(s => s[0])];
  const distances = [similarities.slice(0, topK).map(s => s[1])];
  
  return [indices, distances];
}

async function callLlm(prompt: string): Promise<string> {
  // Simple mock LLM call
  return `Answer based on: ${prompt.substring(0, 30)}...`;
}

// Define shared storage type for RAG pattern
type RAGSharedStorage = {
  files?: string[];
  allChunks?: string[];
  allEmbeds?: number[][];
  index?: any;
  question?: string;
  qEmb?: number[];
  retrievedChunk?: string;
  answer?: string;
};

// Stage 1: Offline Indexing Nodes
class ChunkDocs extends BatchNode<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<string[]> {
    return shared.files || [];
  }

  async exec(filepath: string): Promise<string[]> {
    // Mock file reading - in real usage, you would read actual files
    const text = `This is mock content for ${filepath}. It contains some sample text for testing the RAG pattern.`;
    
    // Chunk by 20 chars each
    const chunks: string[] = [];
    const size = 20;
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.substring(i, i + size));
    }
    return chunks;
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: string[],
    execResList: string[][]
  ): Promise<string | undefined> {
    // Flatten chunks from all files
    const allChunks: string[] = [];
    for (const chunkList of execResList) {
      allChunks.push(...chunkList);
    }
    shared.allChunks = allChunks;
    return undefined;
  }
}

class EmbedDocs extends BatchNode<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<string[]> {
    return shared.allChunks || [];
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: string[],
    execResList: number[][]
  ): Promise<string | undefined> {
    shared.allEmbeds = execResList;
    return undefined;
  }
}

class StoreIndex extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<number[][]> {
    return shared.allEmbeds || [];
  }

  async exec(allEmbeds: number[][]): Promise<unknown> {
    return await createIndex(allEmbeds);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: number[][],
    index: unknown
  ): Promise<string | undefined> {
    shared.index = index;
    return undefined;
  }
}

// Stage 2: Online Query & Answer Nodes
class EmbedQuery extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<string> {
    return shared.question || '';
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: string,
    qEmb: number[]
  ): Promise<string | undefined> {
    shared.qEmb = qEmb;
    return undefined;
  }
}

class RetrieveDocs extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<[number[], unknown, string[]]> {
    return [shared.qEmb || [], shared.index || {}, shared.allChunks || []];
  }

  async exec(inputs: [number[], unknown, string[]]): Promise<string> {
    const [qEmb, index, chunks] = inputs;
    const [I, D] = await searchIndex(index as { embeddings: number[][] }, qEmb, { topK: 1 });
    const bestId = I[0][0];
    const relevantChunk = chunks[bestId];
    return relevantChunk;
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: [number[], unknown, string[]],
    relevantChunk: string
  ): Promise<string | undefined> {
    shared.retrievedChunk = relevantChunk;
    return undefined;
  }
}

class GenerateAnswer extends Node<RAGSharedStorage> {
  async prep(shared: RAGSharedStorage): Promise<[string, string]> {
    return [shared.question || '', shared.retrievedChunk || ''];
  }

  async exec(inputs: [string, string]): Promise<string> {
    const [question, chunk] = inputs;
    const prompt = `Question: ${question}\nContext: ${chunk}\nAnswer:`;
    return await callLlm(prompt);
  }

  async post(
    shared: RAGSharedStorage,
    prepRes: [string, string],
    answer: string
  ): Promise<string | undefined> {
    shared.answer = answer;
    return undefined;
  }
}

// Tests for the RAG pattern
describe('RAG Pattern Tests', () => {
  // Test the offline indexing flow
  test('Offline Indexing Flow', async () => {
    // Create and connect nodes
    const chunkNode = new ChunkDocs();
    const embedNode = new EmbedDocs();
    const storeNode = new StoreIndex();
    
    chunkNode.next(embedNode);
    embedNode.next(storeNode);
    
    const offlineFlow = new Flow(chunkNode);
    
    // Prepare test data
    const shared: RAGSharedStorage = {
      files: ['doc1.txt', 'doc2.txt'],
    };
    
    // Run the flow
    await offlineFlow.run(shared);
    
    // Verify results
    expect(shared.allChunks).toBeDefined();
    expect(shared.allChunks?.length).toBeGreaterThan(0);
    expect(shared.allEmbeds).toBeDefined();
    expect(shared.allEmbeds?.length).toBe(shared.allChunks?.length);
    expect(shared.index).toBeDefined();
  });
  
  // Test the online query and answer flow
  test('Online Query & Answer Flow', async () => {
    // First run the offline indexing to prepare the data
    const chunkNode = new ChunkDocs();
    const embedNode = new EmbedDocs();
    const storeNode = new StoreIndex();
    
    chunkNode.next(embedNode);
    embedNode.next(storeNode);
    
    const offlineFlow = new Flow(chunkNode);
    
    const shared: RAGSharedStorage = {
      files: ['doc1.txt', 'doc2.txt'],
    };
    
    await offlineFlow.run(shared);
    
    // Now create and run the online flow
    const embedQNode = new EmbedQuery();
    const retrieveNode = new RetrieveDocs();
    const generateNode = new GenerateAnswer();
    
    embedQNode.next(retrieveNode);
    retrieveNode.next(generateNode);
    
    const onlineFlow = new Flow(embedQNode);
    
    // Set the question
    shared.question = 'What is the content about?';
    
    // Run the flow
    await onlineFlow.run(shared);
    
    // Verify results
    expect(shared.qEmb).toBeDefined();
    expect(shared.retrievedChunk).toBeDefined();
    expect(shared.answer).toBeDefined();
    expect(typeof shared.answer).toBe('string');
  });
  
  // Test the complete RAG pipeline
  test('Complete RAG Pipeline', async () => {
    // Create a combined flow for both offline and online stages
    
    // Offline stage nodes
    const chunkNode = new ChunkDocs();
    const embedNode = new EmbedDocs();
    const storeNode = new StoreIndex();
    
    // Online stage nodes
    const embedQNode = new EmbedQuery();
    const retrieveNode = new RetrieveDocs();
    const generateNode = new GenerateAnswer();
    
    // Connect offline stage
    chunkNode.next(embedNode);
    embedNode.next(storeNode);
    
    // Connect online stage
    storeNode.next(embedQNode);
    embedQNode.next(retrieveNode);
    retrieveNode.next(generateNode);
    
    // Create flow
    const fullRagFlow = new Flow(chunkNode);
    
    // Prepare test data
    const shared: RAGSharedStorage = {
      files: ['doc1.txt', 'doc2.txt'],
      question: 'What is the content about?',
    };
    
    // Run the full flow
    await fullRagFlow.run(shared);
    
    // Verify results
    expect(shared.allChunks).toBeDefined();
    expect(shared.allEmbeds).toBeDefined();
    expect(shared.index).toBeDefined();
    expect(shared.qEmb).toBeDefined();
    expect(shared.retrievedChunk).toBeDefined();
    expect(shared.answer).toBeDefined();
    expect(typeof shared.answer).toBe('string');
  });
});
</file>

<file path=".cursorrules">
================================================
File: docs/guide.md
================================================

---

layout: default
title: "Agentic Coding"

---

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agents involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
> {: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps             |   Human    |     AI     | Comment                                                                                        |
| :---------------- | :--------: | :--------: | :--------------------------------------------------------------------------------------------- |
| 1. Requirements   |  ★★★ High  |  ★☆☆ Low   | Humans understand the requirements and context.                                                |
| 2. Flow           | ★★☆ Medium | ★★☆ Medium | Humans specify the high-level design, and the AI fills in the details.                         |
| 3. Utilities      | ★★☆ Medium | ★★☆ Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Node           |  ★☆☆ Low   |  ★★★ High  | The AI helps design the node types and data handling based on the flow.                        |
| 5. Implementation |  ★☆☆ Low   |  ★★★ High  | The AI implements the flow based on the design.                                                |
| 6. Optimization   | ★★☆ Medium | ★★☆ Medium | Humans evaluate the results, and the AI helps optimize.                                        |
| 7. Reliability    |  ★☆☆ Low   |  ★★★ High  | The AI writes test cases and addresses corner cases.                                           |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit.

   - Understand AI systems' strengths and limitations:
     - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
     - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
     - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
   - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
   - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.

   - Identify applicable design patterns (e.g., [Map Reduce](./design_pattern/mapreduce.md), [Agent](./design_pattern/agent.md), [RAG](./design_pattern/rag.md)).
     - For each node in the flow, start with a high-level one-line description of what it does.
     - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
     - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
     - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
   - Outline the flow and draw it in a mermaid diagram. For example:

     ```mermaid
     flowchart LR
         start[Start] --> batch[Batch]
         batch --> check[Check]
         check -->|OK| process
         check -->|Error| fix[Fix]
         fix --> check

         subgraph process[Process]
           step1[Step 1] --> step2[Step 2]
         end

         process --> endNode[End]
     ```

   - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
     > {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.

   - Think of your AI system as the brain. It needs a body—these _external utility functions_—to interact with the real world:
       <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

     - Reading inputs (e.g., retrieving Slack messages, reading emails)
     - Writing outputs (e.g., generating reports, sending emails)
     - Using external tools (e.g., calling LLMs, searching the web)
     - **NOTE**: _LLM-based tasks_ (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are _core functions_ internal in the AI system.

   - For each utility function, implement it and write a simple test.
   - Document their input/output, as well as why they are necessary. For example:
     - `name`: `getEmbedding` (`src/utils/getEmbedding.ts`)
     - `input`: `string`
     - `output`: a vector of 3072 numbers
     - `necessity`: Used by the second node to embed text
   - Example utility implementation:

     ```typescript
     // src/utils/callLlm.ts
     import { OpenAI } from "openai";

     export async function callLlm(prompt: string): Promise<string> {
       const client = new OpenAI({
         apiKey: process.env.OPENAI_API_KEY,
       });

       const response = await client.chat.completions.create({
         model: "gpt-4o",
         messages: [{ role: "user", content: prompt }],
       });

       return response.choices[0].message.content || "";
     }
     ```

   - > **Sometimes, design Utilies before Flow:** For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
     > {: .best-practice }

4. **Node Design**: Plan how each node will read and write data, and use utility functions.

   - One core design principle for PocketFlow is to use a [shared store](./core_abstraction/communication.md), so start with a shared store design:

     - For simple systems, use an in-memory object.
     - For more complex systems or when persistence is required, use a database.
     - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
     - Example shared store design:

       ```typescript
       interface SharedStore {
         user: {
           id: string;
           context: {
             weather: { temp: number; condition: string };
             location: string;
           };
         };
         results: Record<string, unknown>;
       }

       const shared: SharedStore = {
         user: {
           id: "user123",
           context: {
             weather: { temp: 72, condition: "sunny" },
             location: "San Francisco",
           },
         },
         results: {}, // Empty object to store outputs
       };
       ```

   - For each [Node](./core_abstraction/node.md), describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Node (or BatchNode, or ParallelBatchNode)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function
     - `post`: Write "embedding" to the shared store

5. **Implementation**: Implement the initial nodes and flows based on the design.

   - 🎉 If you've reached this step, humans have finished the design. Now _Agentic Coding_ begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Avoid `try` logic so you can quickly identify any weak points in the system.
   - Add logging throughout the code to facilitate debugging.

6. **Optimization**:

   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:

     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3–6 hundreds of times.
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     > {: .best-practice }

7. **Reliability**
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `maxRetries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my-project/
├── src/
│   ├── index.ts
│   ├── nodes.ts
│   ├── flow.ts
│   ├── types.ts
│   └── utils/
│       ├── callLlm.ts
│       └── searchWeb.ts
├── docs/
│   └── design.md
├── package.json
└── tsconfig.json
```

- **`docs/design.md`**: Contains project documentation for each step above. This should be _high-level_ and _no-code_.
- **`src/types.ts`**: Contains shared type definitions and interfaces used throughout the project.
- **`src/utils/`**: Contains all utility functions.
  - It's recommended to dedicate one TypeScript file to each API call, for example `callLlm.ts` or `searchWeb.ts`.
  - Each file should export functions that can be imported elsewhere in the project
  - Include test cases for each utility function using `utilityFunctionName.test.ts`
- **`src/nodes.ts`**: Contains all the node definitions.

  ```typescript
  // src/types.ts
  export interface QASharedStore {
    question?: string;
    answer?: string;
  }
  ```

  ```typescript
  // src/nodes.ts
  import { Node } from "pocketflow";
  import { callLlm } from "./utils/callLlm";
  import { QASharedStore } from "./types";
  import PromptSync from "prompt-sync";

  const prompt = PromptSync();

  export class GetQuestionNode extends Node<QASharedStore> {
    async exec(): Promise<string> {
      // Get question directly from user input
      const userQuestion = prompt("Enter your question: ") || "";
      return userQuestion;
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the user's question
      shared.question = execRes;
      return "default"; // Go to the next node
    }
  }

  export class AnswerNode extends Node<QASharedStore> {
    async prep(shared: QASharedStore): Promise<string> {
      // Read question from shared
      return shared.question || "";
    }

    async exec(question: string): Promise<string> {
      // Call LLM to get the answer
      return await callLlm(question);
    }

    async post(
      shared: QASharedStore,
      _: unknown,
      execRes: string
    ): Promise<string | undefined> {
      // Store the answer in shared
      shared.answer = execRes;
      return undefined;
    }
  }
  ```

- **`src/flow.ts`**: Implements functions that create flows by importing node definitions and connecting them.

  ```typescript
  // src/flow.ts
  import { Flow } from "pocketflow";
  import { GetQuestionNode, AnswerNode } from "./nodes";
  import { QASharedStore } from "./types";

  export function createQaFlow(): Flow {
    // Create nodes
    const getQuestionNode = new GetQuestionNode();
    const answerNode = new AnswerNode();

    // Connect nodes in sequence
    getQuestionNode.next(answerNode);

    // Create flow starting with input node
    return new Flow<QASharedStore>(getQuestionNode);
  }
  ```

- **`src/index.ts`**: Serves as the project's entry point.

  ```typescript
  // src/index.ts
  import { createQaFlow } from "./flow";
  import { QASharedStore } from "./types";

  // Example main function
  async function main(): Promise<void> {
    const shared: QASharedStore = {
      question: undefined, // Will be populated by GetQuestionNode from user input
      answer: undefined, // Will be populated by AnswerNode
    };

    // Create the flow and run it
    const qaFlow = createQaFlow();
    await qaFlow.run(shared);
    console.log(`Question: ${shared.question}`);
    console.log(`Answer: ${shared.answer}`);
  }

  // Run the main function
  main().catch(console.error);
  ```

- **`package.json`**: Contains project metadata and dependencies.

- **`tsconfig.json`**: Contains TypeScript compiler configuration.

================================================
File: docs/index.md
================================================

---

layout: default
title: "Home"
nav_order: 1

---

---
layout: default
title: "Home"
nav_order: 1
---

# PocketFlow.js

A [100-line](https://github.com/The-Pocket/PocketFlow-Typescript/blob/main/src/index.ts) minimalist LLM framework for _Agents, Task Decomposition, RAG, etc_.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworks—([Multi-](./design_pattern/multi_agent.html))[Agents](./design_pattern/agent.html), [Workflow](./design_pattern/workflow.html), [RAG](./design_pattern/rag.html), and more.
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" width="400"/>
</div>

## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node](./core_abstraction/node.md) handles simple (LLM) tasks.
- [Flow](./core_abstraction/flow.md) connects nodes through **Actions** (labeled edges).
- [Shared Store](./core_abstraction/communication.md) enables communication between nodes within flows.
- [Batch](./core_abstraction/batch.md) nodes/flows allow for data-intensive tasks.
- [(Advanced) Parallel](./core_abstraction/parallel.md) nodes/flows handle I/O-bound tasks.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" width="700"/>
</div>

## Design Pattern

From there, it’s easy to implement popular design patterns:

- [Agent](./design_pattern/agent.md) autonomously makes decisions.
- [Workflow](./design_pattern/workflow.md) chains multiple tasks into pipelines.
- [RAG](./design_pattern/rag.md) integrates data retrieval with generation.
- [Map Reduce](./design_pattern/mapreduce.md) splits data tasks into Map and Reduce steps.
- [Structured Output](./design_pattern/structure.md) formats outputs consistently.
- [(Advanced) Multi-Agents](./design_pattern/multi_agent.md) coordinate multiple agents.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" width="700"/>
</div>

## Utility Function

We **do not** provide built-in utilities. Instead, we offer _examples_—please _implement your own_:

- [LLM Wrapper](./utility_function/llm.md)
- [Viz and Debug](./utility_function/viz.md)
- [Web Search](./utility_function/websearch.md)
- [Chunking](./utility_function/chunking.md)
- [Embedding](./utility_function/embedding.md)
- [Vector Databases](./utility_function/vector.md)
- [Text-to-Speech](./utility_function/text_to_speech.md)

**Why not built-in?**: I believe it's a _bad practice_ for vendor-specific APIs in a general framework:

- _API Volatility_: Frequent changes lead to heavy maintenance for hardcoded APIs.
- _Flexibility_: You may want to switch vendors, use fine-tuned models, or run them locally.
- _Optimizations_: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps?

Check out [Agentic Coding Guidance](./guide.md), the fastest way to develop LLM projects with PocketFlow.js!

================================================
File: docs/core_abstraction/batch.md
================================================

---

layout: default
title: "Batch"
parent: "Core Abstraction"
nav_order: 4

---

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:

- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **array** of items to process.
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prepRes, execResList)`**: after all items are processed, receives a **list** of results (`execResList`) and returns an **Action**.

### Example: Summarize a Large File

```typescript
type SharedStorage = {
  data: string;
  summary?: string;
};

class MapSummaries extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // Chunk content into manageable pieces
    const content = shared.data;
    const chunks: string[] = [];
    const chunkSize = 10000;

    for (let i = 0; i < content.length; i += chunkSize) {
      chunks.push(content.slice(i, i + chunkSize));
    }

    return chunks;
  }

  async exec(chunk: string): Promise<string> {
    const prompt = `Summarize this chunk in 10 words: ${chunk}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    _: string[],
    summaries: string[]
  ): Promise<string> {
    shared.summary = summaries.join("\n");
    return "default";
  }
}

// Usage
const flow = new Flow(new MapSummaries());
await flow.run({ data: "very long text content..." });
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Example: Summarize Many Files

```typescript
type SharedStorage = {
  files: string[];
};

type FileParams = {
  filename: string;
};

class SummarizeAllFiles extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    return shared.files.map((filename) => ({ filename }));
  }
}

// Create a per-file summarization flow
const summarizeFile = new SummarizeFile();
const summarizeAllFiles = new SummarizeAllFiles(summarizeFile);

await summarizeAllFiles.run({ files: ["file1.txt", "file2.txt"] });
```

### Under the Hood

1. `prep(shared)` returns a list of param objects—e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each object and:
   - Merges it with the BatchFlow's own `params`
   - Calls `flow.run(shared)` using the merged result
3. This means the sub-Flow runs **repeatedly**, once for every param object.

---

## 3. Nested Batches

You can nest BatchFlows to handle hierarchical data processing:

```typescript
type DirectoryParams = {
  directory: string;
};

type FileParams = DirectoryParams & {
  filename: string;
};

class FileBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<FileParams[]> {
    const directory = this._params.directory;
    const files = await getFilesInDirectory(directory).filter((f) =>
      f.endsWith(".txt")
    );

    return files.map((filename) => ({
      directory, // Pass on directory from parent
      filename, // Add filename for this batch item
    }));
  }
}

class DirectoryBatchFlow extends BatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<DirectoryParams[]> {
    return ["/path/to/dirA", "/path/to/dirB"].map((directory) => ({
      directory,
    }));
  }
}

// Process all files in all directories
const processingNode = new ProcessingNode();
const fileFlow = new FileBatchFlow(processingNode);
const dirFlow = new DirectoryBatchFlow(fileFlow);
await dirFlow.run({});
```

================================================
File: docs/core_abstraction/communication.md
================================================

---

layout: default
title: "Communication"
parent: "Core Abstraction"
nav_order: 3

---

# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)**

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate _Data Schema_ from _Compute Logic_! This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](./batch.md).
     > {: .best-practice }

2. **Params (only for [Batch](./batch.md))**
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:

```typescript
interface SharedStore {
  data: Record<string, unknown>;
  summary: Record<string, unknown>;
  config: Record<string, unknown>;
  // ...other properties
}

const shared: SharedStore = { data: {}, summary: {}, config: {} /* ... */ };
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```typescript
interface SharedStore {
  data: string;
  summary: string;
}

class LoadData extends Node<SharedStore> {
  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write data to shared store
    shared.data = "Some text content";
    return "default";
  }
}

class Summarize extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<unknown> {
    // We read data from shared store
    return shared.data;
  }

  async exec(prepRes: unknown): Promise<unknown> {
    // Call LLM to summarize
    const prompt = `Summarize: ${prepRes}`;
    const summary = await callLlm(prompt);
    return summary;
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    // We write summary to shared store
    shared.summary = execRes as string;
    return "default";
  }
}

const loadData = new LoadData();
const summarize = new Summarize();
loadData.next(summarize);
const flow = new Flow(loadData);

const shared: SharedStore = { data: "", summary: "" };
flow.run(shared);
```

Here:

- `LoadData` writes to `shared.data`.
- `Summarize` reads from `shared.data`, summarizes, and writes to `shared.summary`.

---

## 2. Params

**Params** let you store _per-Node_ or _per-Flow_ config that doesn't need to live in the shared store. They are:

- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `setParams()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow.
>
> If you need to set child node params, see [Batch](./batch.md).
> {: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```typescript
interface SharedStore {
  data: Record<string, string>;
  summary: Record<string, string>;
}

interface SummarizeParams {
  filename: string;
}

// 1) Create a Node that uses params
class SummarizeFile extends Node<SharedStore, SummarizeParams> {
  async prep(shared: SharedStore): Promise<unknown> {
    // Access the node's param
    const filename = this._params.filename;
    return shared.data[filename] || "";
  }

  async exec(prepRes: unknown): Promise<unknown> {
    const prompt = `Summarize: ${prepRes}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStore,
    prepRes: unknown,
    execRes: unknown
  ): Promise<string | undefined> {
    const filename = this._params.filename;
    shared.summary[filename] = execRes as string;
    return "default";
  }
}

// 2) Set params
const node = new SummarizeFile();

// 3) Set Node params directly (for testing)
node.setParams({ filename: "doc1.txt" });
node.run(shared);

// 4) Create Flow
const flow = new Flow<SharedStore>(node);

// 5) Set Flow params (overwrites node params)
flow.setParams({ filename: "doc2.txt" });
flow.run(shared); // The node summarizes doc2, not doc1
```

================================================
File: docs/core_abstraction/flow.md
================================================

---

layout: default
title: "Flow"
parent: "Core Abstraction"
nav_order: 2

---

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `nodeA.next(nodeB)`
   This means if `nodeA.post()` returns `"default"`, go to `nodeB`.
   (Equivalent to `nodeA.on("default", nodeB)`)

2. **Named action transition**: `nodeA.on("actionName", nodeB)`
   This means if `nodeA.post()` returns `"actionName"`, go to `nodeB`.

It's possible to create loops, branching, or multi-step flows.

## 2. Method Chaining

The transition methods support **chaining** for more concise flow creation:

### Chaining `on()` Methods

The `on()` method returns the current node, so you can chain multiple action definitions:

```typescript
// All transitions from the same node
nodeA.on("approved", nodeB).on("rejected", nodeC).on("needs_review", nodeD);

// Equivalent to:
nodeA.on("approved", nodeB);
nodeA.on("rejected", nodeC);
nodeA.on("needs_review", nodeD);
```

### Chaining `next()` Methods

The `next()` method returns the target node, allowing you to create linear sequences in a single expression:

```typescript
// Creates a linear A → B → C → D sequence
nodeA.next(nodeB).next(nodeC).next(nodeD);

// Equivalent to:
nodeA.next(nodeB);
nodeB.next(nodeC);
nodeC.next(nodeD);
```

### Combining Chain Types

You can combine both chaining styles for complex flows:

```typescript
nodeA
  .on("action1", nodeB.next(nodeC).next(nodeD))
  .on("action2", nodeE.on("success", nodeF).on("failure", nodeG));
```

## 3. Creating a Flow

A **Flow** begins with a **start** node. You call `const flow = new Flow(someNode)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```typescript
nodeA.next(nodeB);
const flow = new Flow(nodeA);
flow.run(shared);
```

- When you run the flow, it executes `nodeA`.
- Suppose `nodeA.post()` returns `"default"`.
- The flow then sees `"default"` Action is linked to `nodeB` and runs `nodeB`.
- `nodeB.post()` returns `"default"` but we didn't define a successor for `nodeB`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```typescript
// Define the flow connections
review.on("approved", payment); // If approved, process payment
review.on("needs_revision", revise); // If needs changes, go to revision
review.on("rejected", finish); // If rejected, finish the process

revise.next(review); // After revision, go back for another review
payment.next(finish); // After payment, finish the process

const flow = new Flow(review);
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action.
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
>
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
> {: .warning }

## 4. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.
2. Combine multiple smaller Flows into a larger Flow for reuse.
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `undefined` for `execRes` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```typescript
// Create a sub-flow
nodeA.next(nodeB);
const subflow = new Flow(nodeA);

// Connect it to another node
subflow.next(nodeC);

// Create the parent flow
const parentFlow = new Flow(subflow);
```

When `parentFlow.run()` executes:

1. It starts `subflow`
2. `subflow` runs through its nodes (`nodeA->nodeB`)
3. After `subflow` completes, execution continues to `nodeC`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```typescript
// Payment processing sub-flow
validatePayment.next(processPayment).next(paymentConfirmation);
const paymentFlow = new Flow(validatePayment);

// Inventory sub-flow
checkStock.next(reserveItems).next(updateInventory);
const inventoryFlow = new Flow(checkStock);

// Shipping sub-flow
createLabel.next(assignCarrier).next(schedulePickup);
const shippingFlow = new Flow(createLabel);

// Connect the flows into a main order pipeline
paymentFlow.next(inventoryFlow).next(shippingFlow);

// Create the master flow
const orderPipeline = new Flow(paymentFlow);

// Run the entire pipeline
orderPipeline.run(sharedData);
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

================================================
File: docs/core_abstraction/node.md
================================================

---

layout: default
title: "Node"
parent: "Core Abstraction"
nav_order: 1

---

# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`

   - **Read and preprocess data** from `shared` store.
   - Examples: _query DB, read files, or serialize data into a string_.
   - Return `prepRes`, which is used by `exec()` and `post()`.

2. `exec(prepRes)`

   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: _(mostly) LLM calls, remote APIs, tool use_.
   - ⚠️ This shall be only for compute and **NOT** access `shared`.
   - ⚠️ If retries enabled, ensure idempotent implementation.
   - Return `execRes`, which is passed to `post()`.

3. `post(shared, prepRes, execRes)`
   - **Postprocess and write data** back to `shared`.
   - Examples: _update DB, change states, log results_.
   - **Decide the next action** by returning a _string_ (`action = "default"` if _None_).

> **Why 3 steps?** To enforce the principle of _separation of concerns_. The data storage and data processing are operated separately.
>
> All steps are _optional_. E.g., you can only implement `prep` and `post` if you just need to process data.
> {: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting).
  `wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```typescript
const myNode = new SummarizeFile(3, 10); // maxRetries = 3, wait = 10 seconds
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `maxRetries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `this.currentRetry`.

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```typescript
execFallback(prepRes: unknown, error: Error): unknown {
  return "There was an error processing your request.";
}
```

By default, it just re-raises the exception.

### Example: Summarize file

```typescript
type SharedStore = {
  data: string;
  summary?: string;
};

class SummarizeFile extends Node<SharedStore> {
  prep(shared: SharedStore): string {
    return shared.data;
  }

  exec(content: string): string {
    if (!content) return "Empty file content";

    const prompt = `Summarize this text in 10 words: ${content}`;
    return callLlm(prompt);
  }

  execFallback(_: string, error: Error): string {
    return "There was an error processing your request.";
  }

  post(shared: SharedStore, _: string, summary: string): string | undefined {
    shared.summary = summary;
    return undefined; // "default" action
  }
}

// Example usage
const node = new SummarizeFile(3); // maxRetries = 3
const shared: SharedStore = { data: "Long text to summarize..." };
const action = node.run(shared);

console.log("Action:", action);
console.log("Summary:", shared.summary);
```

================================================
File: docs/core_abstraction/parallel.md
================================================

---

layout: default
title: "(Advanced) Parallel"
parent: "Core Abstraction"
nav_order: 6

---

# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple operations **concurrently**—for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute.

> Parallel nodes and flows excel at overlapping I/O-bound work—like LLM calls, database queries, API requests, or file I/O. TypeScript's Promise-based implementation allows for truly concurrent execution of asynchronous operations.
> {: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
>
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism.
>
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
>   {: .best-practice }

## ParallelBatchNode

Like **BatchNode**, but runs operations in **parallel** using Promise.all():

```typescript
class TextSummarizer extends ParallelBatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string[]> {
    // e.g., multiple texts
    return shared.texts || [];
  }

  async exec(text: string): Promise<string> {
    const prompt = `Summarize: ${text}`;
    return await callLlm(prompt);
  }

  async post(
    shared: SharedStorage,
    prepRes: string[],
    execRes: string[]
  ): Promise<string | undefined> {
    shared.summaries = execRes;
    return "default";
  }
}

const node = new TextSummarizer();
const flow = new Flow(node);
```

## ParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using Promise.all():

```typescript
class SummarizeMultipleFiles extends ParallelBatchFlow<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, any>[]> {
    return (shared.files || []).map((f) => ({ filename: f }));
  }
}

const subFlow = new Flow(new LoadAndSummarizeFile());
const parallelFlow = new SummarizeMultipleFiles(subFlow);
await parallelFlow.run(shared);
```

================================================
File: docs/design_pattern/agent.md
================================================

---

layout: default
title: "Agent"
parent: "Design Pattern"
nav_order: 1

---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](../core_abstraction/flow.md) between nodes—and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide action—for example:

```typescript
`
### CONTEXT
Task: ${task}
Previous Actions: ${prevActions}
Current State: ${state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters: query (str)

[2] answer
  Description: Conclude based on the results
  Parameters: result (str)

### NEXT ACTION
Decide the next action based on the current context.
Return your response in YAML format:

\`\`\`yaml
thinking: <reasoning process>
action: <action_name>
parameters: <parameters>
\`\`\``;
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide _relevant, minimal context._ For example, rather than including an entire chat history, retrieve the most relevant via [RAG](./rag.md).

2. **Action Space:** Provide _a well-structured and unambiguous_ set of actions—avoiding overlap like separate `read_databases` or `read_csvs`.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks instead of all at once.
- **Overview-zoom-in:** First provide high-level structure, then allow drilling into details.
- **Parameterized/Programmable:** Enable parameterized or programmable actions.
- **Backtracking:** Let the agent undo the last step instead of restarting entirely.

## Example: Search Agent

This agent:

1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

````typescript
interface SharedState {
  query?: string;
  context?: Array<{ term: string; result: string }>;
  search_term?: string;
  answer?: string;
}

class DecideAction extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    const context = shared.context
      ? JSON.stringify(shared.context)
      : "No previous search";
    return [shared.query || "", context];
  }

  async exec([query, context]: [string, string]): Promise<any> {
    const prompt = `
Given input: ${query}
Previous search results: ${context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
\`\`\`yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
\`\`\``;
    const resp = await callLlm(prompt);
    const yamlStr = resp.split("```yaml")[1].split("```")[0].trim();
    return yaml.load(yamlStr);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    result: any
  ): Promise<string> {
    if (result.action === "search") {
      shared.search_term = result.search_term;
    }
    return result.action;
  }
}

class SearchWeb extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.search_term || "";
  }

  async exec(searchTerm: string): Promise<string> {
    return await searchWeb(searchTerm);
  }

  async post(shared: SharedState, _: string, execRes: string): Promise<string> {
    shared.context = [
      ...(shared.context || []),
      { term: shared.search_term || "", result: execRes },
    ];
    return "decide";
  }
}

class DirectAnswer extends Node<SharedState> {
  async prep(shared: SharedState): Promise<[string, string]> {
    return [
      shared.query || "",
      shared.context ? JSON.stringify(shared.context) : "",
    ];
  }

  async exec([query, context]: [string, string]): Promise<string> {
    return await callLlm(`Context: ${context}\nAnswer: ${query}`);
  }

  async post(
    shared: SharedState,
    _: [string, string],
    execRes: string
  ): Promise<undefined> {
    shared.answer = execRes;
    return undefined;
  }
}

// Connect nodes
const decide = new DecideAction();
const search = new SearchWeb();
const answer = new DirectAnswer();

decide.on("search", search);
decide.on("answer", answer);
search.on("decide", decide); // Loop back

const flow = new Flow(decide);
await flow.run({ query: "Who won the Nobel Prize in Physics 2024?" });
````

================================================
File: docs/design_pattern/mapreduce.md
================================================

---

layout: default
title: "Map Reduce"
parent: "Design Pattern"
nav_order: 4

---

# Map Reduce

MapReduce is a design pattern suitable when you have either:

- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```typescript
type SharedStorage = {
  files?: Record<string, string>;
  file_summaries?: Record<string, string>;
  all_files_summary?: string;
};

class SummarizeAllFiles extends BatchNode<SharedStorage> {
  async prep(shared: SharedStorage): Promise<[string, string][]> {
    return Object.entries(shared.files || {}); // [["file1.txt", "aaa..."], ["file2.txt", "bbb..."], ...]
  }

  async exec([filename, content]: [string, string]): Promise<[string, string]> {
    const summary = await callLLM(`Summarize the following file:\n${content}`);
    return [filename, summary];
  }

  async post(
    shared: SharedStorage,
    _: [string, string][],
    summaries: [string, string][]
  ): Promise<string> {
    shared.file_summaries = Object.fromEntries(summaries);
    return "summarized";
  }
}

class CombineSummaries extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<Record<string, string>> {
    return shared.file_summaries || {};
  }

  async exec(summaries: Record<string, string>): Promise<string> {
    const text_list = Object.entries(summaries).map(
      ([fname, summ]) => `${fname} summary:\n${summ}\n`
    );

    return await callLLM(
      `Combine these file summaries into one final summary:\n${text_list.join(
        "\n---\n"
      )}`
    );
  }

  async post(
    shared: SharedStorage,
    _: Record<string, string>,
    finalSummary: string
  ): Promise<string> {
    shared.all_files_summary = finalSummary;
    return "combined";
  }
}

// Create and connect flow
const batchNode = new SummarizeAllFiles();
const combineNode = new CombineSummaries();
batchNode.on("summarized", combineNode);

// Run the flow with test data
const flow = new Flow(batchNode);
flow.run({
  files: {
    "file1.txt":
      "Alice was beginning to get very tired of sitting by her sister...",
    "file2.txt": "Some other interesting text ...",
  },
});
```

> **Performance Tip**: The example above works sequentially. You can speed up the map phase by using `ParallelBatchNode` instead of `BatchNode`. See [(Advanced) Parallel](../core_abstraction/parallel.md) for more details.
> {: .note }

================================================
File: docs/design_pattern/rag.md
================================================

---

layout: default
title: "RAG"
parent: "Design Pattern"
nav_order: 3

---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---

## Stage 1: Offline Indexing

We create three Nodes:

1. `ChunkDocs` – [chunks](../utility_function/chunking.md) raw text.
2. `EmbedDocs` – [embeds](../utility_function/embedding.md) each chunk.
3. `StoreIndex` – stores embeddings into a [vector database](../utility_function/vector.md).

```typescript
type SharedStore = {
  files?: string[];
  allChunks?: string[];
  allEmbeds?: number[][];
  index?: any;
};

class ChunkDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.files || [];
  }

  async exec(filepath: string): Promise<string[]> {
    const text = fs.readFileSync(filepath, "utf-8");
    // Simplified chunking for example
    const chunks: string[] = [];
    const size = 100;
    for (let i = 0; i < text.length; i += size) {
      chunks.push(text.substring(i, i + size));
    }
    return chunks;
  }

  async post(
    shared: SharedStore,
    _: string[],
    chunks: string[][]
  ): Promise<undefined> {
    shared.allChunks = chunks.flat();
    return undefined;
  }
}

class EmbedDocs extends BatchNode<SharedStore> {
  async prep(shared: SharedStore): Promise<string[]> {
    return shared.allChunks || [];
  }

  async exec(chunk: string): Promise<number[]> {
    return await getEmbedding(chunk);
  }

  async post(
    shared: SharedStore,
    _: string[],
    embeddings: number[][]
  ): Promise<undefined> {
    shared.allEmbeds = embeddings;
    return undefined;
  }
}

class StoreIndex extends Node<SharedStore> {
  async prep(shared: SharedStore): Promise<number[][]> {
    return shared.allEmbeds || [];
  }

  async exec(allEmbeds: number[][]): Promise<unknown> {
    return await createIndex(allEmbeds);
  }

  async post(
    shared: SharedStore,
    _: number[][],
    index: unknown
  ): Promise<undefined> {
    shared.index = index;
    return undefined;
  }
}

// Create indexing flow
const chunkNode = new ChunkDocs();
const embedNode = new EmbedDocs();
const storeNode = new StoreIndex();

chunkNode.next(embedNode).next(storeNode);
const offlineFlow = new Flow(chunkNode);
```

---

## Stage 2: Online Query & Answer

We have 3 nodes:

1. `EmbedQuery` – embeds the user's question.
2. `RetrieveDocs` – retrieves top chunk from the index.
3. `GenerateAnswer` – calls the LLM with the question + chunk to produce the final answer.

```typescript
type OnlineStore = SharedStore & {
  question?: string;
  qEmb?: number[];
  retrievedChunk?: string;
  answer?: string;
};

class EmbedQuery extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<string> {
    return shared.question || "";
  }

  async exec(question: string): Promise<number[]> {
    return await getEmbedding(question);
  }

  async post(
    shared: OnlineStore,
    _: string,
    qEmb: number[]
  ): Promise<undefined> {
    shared.qEmb = qEmb;
    return undefined;
  }
}

class RetrieveDocs extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[number[], any, string[]]> {
    return [shared.qEmb || [], shared.index, shared.allChunks || []];
  }

  async exec([qEmb, index, chunks]: [
    number[],
    any,
    string[]
  ]): Promise<string> {
    const [ids] = await searchIndex(index, qEmb, { topK: 1 });
    return chunks[ids[0][0]];
  }

  async post(
    shared: OnlineStore,
    _: [number[], any, string[]],
    chunk: string
  ): Promise<undefined> {
    shared.retrievedChunk = chunk;
    return undefined;
  }
}

class GenerateAnswer extends Node<OnlineStore> {
  async prep(shared: OnlineStore): Promise<[string, string]> {
    return [shared.question || "", shared.retrievedChunk || ""];
  }

  async exec([question, chunk]: [string, string]): Promise<string> {
    return await callLlm(`Question: ${question}\nContext: ${chunk}\nAnswer:`);
  }

  async post(
    shared: OnlineStore,
    _: [string, string],
    answer: string
  ): Promise<undefined> {
    shared.answer = answer;
    return undefined;
  }
}

// Create query flow
const embedQNode = new EmbedQuery();
const retrieveNode = new RetrieveDocs();
const generateNode = new GenerateAnswer();

embedQNode.next(retrieveNode).next(generateNode);
const onlineFlow = new Flow(embedQNode);
```

Usage example:

```typescript
const shared = {
  files: ["doc1.txt", "doc2.txt"], // any text files
};
await offlineFlow.run(shared);
```

================================================
File: docs/design_pattern/structure.md
================================================

---

layout: default
title: "Structured Output"
parent: "Design Pattern"
nav_order: 5

---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:

- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

## TypeScript Implementation

When using PocketFlow with structured output, follow these TypeScript patterns:

1. **Define Types** for your structured input/output
2. **Implement Validation** in your Node methods
3. **Use Type-Safe Operations** throughout your flow

### Example Text Summarization

````typescript
// Define types
type SummaryResult = {
  summary: string[];
};

type SharedStorage = {
  text?: string;
  result?: SummaryResult;
};

class SummarizeNode extends Node<SharedStorage> {
  async prep(shared: SharedStorage): Promise<string | undefined> {
    return shared.text;
  }

  async exec(text: string | undefined): Promise<SummaryResult> {
    if (!text) return { summary: ["No text provided"] };

    const prompt = `
Please summarize the following text as YAML, with exactly 3 bullet points

${text}

Output:
\`\`\`yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
\`\`\``;

    // Simulated LLM call
    const response =
      "```yaml\nsummary:\n  - First point\n  - Second insight\n  - Final conclusion\n```";

    // Parse YAML response
    const yamlStr = response.split("```yaml")[1].split("```")[0].trim();

    // Extract bullet points
    const result: SummaryResult = {
      summary: yamlStr
        .split("\n")
        .filter((line) => line.trim().startsWith("- "))
        .map((line) => line.trim().substring(2)),
    };

    // Validate
    if (!result.summary || !Array.isArray(result.summary)) {
      throw new Error("Invalid summary structure");
    }

    return result;
  }

  async post(
    shared: SharedStorage,
    _: string | undefined,
    result: SummaryResult
  ): Promise<string | undefined> {
    shared.result = result;
    return "default";
  }
}
````

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

**In YAML**

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

================================================
File: docs/design_pattern/workflow.md
================================================

---

layout: default
title: "Workflow"
parent: "Design Pattern"
nav_order: 2

---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - You don't want to make each task **too coarse**, because it may be _too complex for one LLM call_.
> - You don't want to make each task **too granular**, because then _the LLM call doesn't have enough context_ and results are _not consistent across nodes_.
>
> You usually need multiple _iterations_ to find the _sweet spot_. If the task has too many _edge cases_, consider using [Agents](./agent.md).
> {: .best-practice }

### Example: Article Writing

```typescript
interface SharedState {
  topic?: string;
  outline?: string;
  draft?: string;
  final_article?: string;
}

// Helper function to simulate LLM call
async function callLLM(prompt: string): Promise<string> {
  return `Response to: ${prompt}`;
}

class GenerateOutline extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.topic || "";
  }

  async exec(topic: string): Promise<string> {
    return await callLLM(
      `Create a detailed outline for an article about ${topic}`
    );
  }

  async post(shared: SharedState, _: string, outline: string): Promise<string> {
    shared.outline = outline;
    return "default";
  }
}

class WriteSection extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.outline || "";
  }

  async exec(outline: string): Promise<string> {
    return await callLLM(`Write content based on this outline: ${outline}`);
  }

  async post(shared: SharedState, _: string, draft: string): Promise<string> {
    shared.draft = draft;
    return "default";
  }
}

class ReviewAndRefine extends Node<SharedState> {
  async prep(shared: SharedState): Promise<string> {
    return shared.draft || "";
  }

  async exec(draft: string): Promise<string> {
    return await callLLM(`Review and improve this draft: ${draft}`);
  }

  async post(
    shared: SharedState,
    _: string,
    final: string
  ): Promise<undefined> {
    shared.final_article = final;
    return undefined;
  }
}

// Connect nodes in sequence
const outline = new GenerateOutline();
const write = new WriteSection();
const review = new ReviewAndRefine();

outline.next(write).next(review);

// Create and run flow
const writingFlow = new Flow(outline);
writingFlow.run({ topic: "AI Safety" });
```

For _dynamic cases_, consider using [Agents](./agent.md).

================================================
File: docs/utility_function/llm.md
================================================

---

layout: default
title: "LLM Wrapper"
parent: "Utility Function"
nav_order: 1

---

# LLM Wrappers

Check out popular libraries like [LangChain](https://github.com/langchain-ai/langchainjs) (13.8k+ GitHub stars), [ModelFusion](https://github.com/vercel/modelfusion) (1.2k+ GitHub stars), or [Firebase GenKit](https://firebase.google.com/docs/genkit) for unified LLM interfaces.
Here, we provide some minimal example implementations:

1. OpenAI

   ```typescript
   import { OpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
     const r = await client.chat.completions.create({
       model: "gpt-4o",
       messages: [{ role: "user", content: prompt }],
     });
     return r.choices[0].message.content || "";
   }

   // Example usage
   callLlm("How are you?").then(console.log);
   ```

   > Store the API key in an environment variable like OPENAI_API_KEY for security.
   > {: .best-practice }

2. Claude (Anthropic)

   ```typescript
   import Anthropic from "@anthropic-ai/sdk";

   async function callLlm(prompt: string): Promise<string> {
     const client = new Anthropic({
       apiKey: "YOUR_API_KEY_HERE",
     });
     const response = await client.messages.create({
       model: "claude-3-7-sonnet-20250219",
       max_tokens: 3000,
       messages: [{ role: "user", content: prompt }],
     });
     return response.content[0].text;
   }
   ```

3. Google (Vertex AI)

   ```typescript
   import { VertexAI } from "@google-cloud/vertexai";

   async function callLlm(prompt: string): Promise<string> {
     const vertexAI = new VertexAI({
       project: "YOUR_PROJECT_ID",
       location: "us-central1",
     });

     const generativeModel = vertexAI.getGenerativeModel({
       model: "gemini-1.5-flash",
     });

     const response = await generativeModel.generateContent({
       contents: [{ role: "user", parts: [{ text: prompt }] }],
     });

     return response.response.candidates[0].content.parts[0].text;
   }
   ```

4. Azure (Azure OpenAI)

   ```typescript
   import { AzureOpenAI } from "openai";

   async function callLlm(prompt: string): Promise<string> {
     const client = new AzureOpenAI({
       apiKey: "YOUR_API_KEY_HERE",
       azure: {
         apiVersion: "2023-05-15",
         endpoint: "https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
       },
     });

     const r = await client.chat.completions.create({
       model: "<YOUR_DEPLOYMENT_NAME>",
       messages: [{ role: "user", content: prompt }],
     });

     return r.choices[0].message.content || "";
   }
   ```

5. Ollama (Local LLM)

   ```typescript
   import ollama from "ollama";

   async function callLlm(prompt: string): Promise<string> {
     const response = await ollama.chat({
       model: "llama2",
       messages: [{ role: "user", content: prompt }],
     });
     return response.message.content;
   }
   ```

## Improvements

Feel free to enhance your `callLlm` function as needed. Here are examples:

- Handle chat history:

```typescript
interface Message {
  role: "user" | "assistant" | "system";
  content: string;
}

async function callLlm(messages: Message[]): Promise<string> {
  const client = new OpenAI({ apiKey: "YOUR_API_KEY_HERE" });
  const r = await client.chat.completions.create({
    model: "gpt-4o",
    messages: messages,
  });
  return r.choices[0].message.content || "";
}
```

- Add in-memory caching

```typescript
import { memoize } from "lodash";

const callLlmMemoized = memoize(async (prompt: string): Promise<string> => {
  // Your implementation here
  return "";
});

async function callLlm(prompt: string, useCache = true): Promise<string> {
  if (useCache) {
    return callLlmMemoized(prompt);
  }
  // Call the underlying function directly
  return callLlmInternal(prompt);
}

class SummarizeNode {
  private curRetry = 0;

  async exec(text: string): Promise<string> {
    return callLlm(`Summarize: ${text}`, this.curRetry === 0);
  }
}
```

- Enable logging:

```typescript
async function callLlm(prompt: string): Promise<string> {
  console.info(`Prompt: ${prompt}`);
  // Your implementation here
  const response = ""; // Response from your implementation
  console.info(`Response: ${response}`);
  return response;
}
```
</file>

<file path=".gitignore">
# OS generated files
.DS_Store
.DS_Store?
._*
.direnv
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db


# IDE specific files
.idea/
.vscode/
*.swp
*.swo
*~

# Node
node_modules/
npm-debug.log
yarn-debug.log
yarn-error.log
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
venv/
ENV/

# Logs and databases
*.log
*.sql
*.sqlite

# Build output
dist/
build/
out/

# Coverage reports
coverage/
.coverage
.coverage.*
htmlcov/

# Misc
*.bak
*.tmp
*.temp


test.ipynb
.pytest_cache/
</file>

<file path=".npmignore">
src/
tests/
jest.config.js
tsconfig.json
.npmignore
</file>

<file path="CODE_OF_CONDUCT.md">
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity and
orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

- Demonstrating empathy and kindness toward other people
- Being respectful of differing opinions, viewpoints, and experiences
- Giving and gracefully accepting constructive feedback
- Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
- Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

- The use of sexualized language or imagery, and sexual attention or
  advances of any kind
- Trolling, insulting or derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others' private information, such as a physical or email
  address, without their explicit permission
- Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Project maintainers are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies
</file>

<file path="CONTRIBUTING.md">
# Contributing to Pocket Flow TypeScript

Thank you for your interest in contributing to Pocket Flow! This document provides guidelines and instructions for contributing to this project.

## Table of Contents

- [Code of Conduct](#code-of-conduct)
- [Getting Started](#getting-started)
- [Development Workflow](#development-workflow)
- [Pull Request Process](#pull-request-process)
- [Coding Standards](#coding-standards)
- [Testing](#testing)
- [Documentation](#documentation)
- [Community](#community)

## Code of Conduct

By participating in this project, you agree to abide by the [Code of Conduct](CODE_OF_CONDUCT.md). Please read it to understand the expectations for all contributors.

## Getting Started

### Prerequisites

- Node.js (recommended version: 18.x or later)
- npm (recommended version: 8.x or later)
- Git

### Setup Development Environment

1. Fork the repository on GitHub
2. Clone your fork:
   ```bash
   git clone https://github.com/YOUR_USERNAME/PocketFlow-Typescript.git
   cd PocketFlow-Typescript
   ```
3. Add the original repository as upstream:
   ```bash
   git remote add upstream https://github.com/The-Pocket/PocketFlow-Typescript.git
   ```
4. Install dependencies:
   ```bash
   npm install
   ```

## Development Workflow

1. Create a new branch for your feature or bugfix:

   ```bash
   git checkout -b feature/your-feature-name
   # or
   git checkout -b fix/issue-you-are-fixing
   ```

2. Make your changes

3. Run tests to ensure your changes don't break existing functionality:

   ```bash
   npm test
   ```

4. Update documentation if necessary

5. Commit your changes with a descriptive commit message:

   ```bash
   git commit -m "Description of changes"
   ```

6. Push your branch to your fork:

   ```bash
   git push origin feature/your-feature-name
   ```

7. Create a Pull Request from your fork to the original repository

## Pull Request Process

1. Ensure your PR has a clear title and description
2. Link any relevant issues with "Fixes #issue-number" in the PR description
3. Make sure all tests pass
4. Request a code review from maintainers
5. Address any feedback from code reviews
6. Once approved, a maintainer will merge your PR

## Coding Standards

- Follow the established code style in the project
- Use TypeScript for type safety
- Keep functions small and focused on a single responsibility
- Use meaningful variable and function names
- Comment your code when necessary, but prefer self-documenting code
- Avoid any unnecessary dependencies

## Testing

- Write tests for new features and bug fixes
- Ensure all tests pass before submitting a PR
- Aim for high test coverage
- Follow existing testing patterns

## Documentation

- Update relevant documentation for any new features
- Include JSDoc comments for public APIs
- Keep documentation up-to-date with code changes
- Consider adding examples for complex features

## Community

- Join our [Discord](https://discord.gg/hUHHE9Sa6T) for discussions and questions
- Be respectful and helpful to other community members
- Share knowledge and help others learn

Thank you for contributing to Pocket Flow!
</file>

<file path="jest.config.js">
module.exports = {
    preset: 'ts-jest',
    testEnvironment: 'node',
    testMatch: ['**/tests/**/*.test.ts'],
  };
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Victor Duarte and Zachary Huang

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.json">
{
  "name": "pocketflow",
  "description": "A minimalist LLM framework",
  "version": "1.0.4",
  "main": "./dist/index.js",
  "module": "./dist/index.mjs",
  "types": "./dist/index.d.ts",
  "engines": {
    "node": ">=18.0.0"
  },
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "import": "./dist/index.mjs",
      "require": "./dist/index.js"
    }
  },
  "files": [
    "dist"
  ],
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/The-Pocket/PocketFlow-Typescript.git"
  },
  "keywords": [
    "pocketflow",
    "typescript",
    "llm",
    "ai",
    "framework",
    "workflow",
    "minimalist"
  ],
  "scripts": {
    "build": "tsup",
    "test": "jest",
    "test:watch": "jest --watch",
    "prepublishOnly": "npm run build"
  },
  "devDependencies": {
    "@types/jest": "^29.5.14",
    "jest": "^29.7.0",
    "ts-jest": "^29.3.0",
    "tsup": "^8.4.0",
    "typescript": "^5.8.2"
  }
}
</file>

<file path="README.md">
<div align="center">
  <img src="https://raw.githubusercontent.com/The-Pocket/.github/main/assets/title.png" width="600"/>
</div>

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
[![Docs](https://img.shields.io/badge/docs-latest-blue)](https://the-pocket.github.io/PocketFlow/)
<a href="https://discord.gg/hUHHE9Sa6T">
<img src="https://img.shields.io/discord/1346833819172601907?logo=discord&style=flat">
</a>

# PocketFlow.js

PocketFlow.js is a TypeScript port of the original [Python version](https://github.com/The-Pocket/PocketFlow) - a minimalist LLM framework.

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Documentation](#documentation)
- [Testing](#testing)
- [Contributing](#contributing)
- [Community](#community)
- [License](#license)

## Features

- **Lightweight**: Zero bloat, zero dependencies, zero vendor lock-in.

- **Expressive**: Everything you love—([Multi-](https://the-pocket.github.io/PocketFlow/design_pattern/multi_agent.html))[Agents](https://the-pocket.github.io/PocketFlow/design_pattern/agent.html), [Workflow](https://the-pocket.github.io/PocketFlow/design_pattern/workflow.html), [RAG](https://the-pocket.github.io/PocketFlow/design_pattern/rag.html), and more.

- **[Agentic Coding](https://zacharyhuang.substack.com/p/agentic-coding-the-most-fun-way-to)**: Let AI Agents (e.g., Cursor AI) build Agents—10x productivity boost!

## Installation

```bash
npm install pocketflow
```

Alternatively, you can simply copy the [source code](src/index.ts) directly into your project.

## Quick Start

Run the following command to create a new PocketFlow project:

```bash
npx create-pocketflow
```

Use cursor/windsurf/any other LLM builtin IDE to open the project.  
You can type the following prompt to the agent to confirm the project is setup correctly:

```
Help me describe briefly about PocketFlow.js
```

Simply start typing your prompt, and the AI agent will build the project for you.  
Here's a simple example:

```
I want to create an application that can write novel:

1. User can enter a novel title
2. It will generate a outline of the novel
3. It will generate a chapter based on the outline
4. It will save the chapter to ./output/title_name.md

First, read the requirements carefully.
Then, start with design.md first. Stop there until further instructions.
```

Once you have the design, and you have no questions, start the implementation by simply typing:

```
Start implementing the design.
```

## Documentation

- Check out the [official documentation](https://the-pocket.github.io/PocketFlow/) for comprehensive guides and examples. The TypeScript version is still under development, so some features may not be available.
- For an in-depth design explanation, read our [design essay](https://github.com/The-Pocket/.github/blob/main/profile/pocketflow.md)

## Testing

To run tests locally:

```bash
# Install dependencies
npm install

# Run tests
npm test
```

## Contributing

We welcome contributions from the community! Here's how you can help:

### Code of Conduct

Please read and follow our [Code of Conduct](CODE_OF_CONDUCT.md) to foster an inclusive community.

### CI/CD Workflow

We use GitHub Actions for continuous integration and deployment:

- **CI Workflow**: Automatically runs tests and builds the project on each push and pull request to the main branch.
- **Code Quality**: Checks TypeScript compilation to ensure code quality.
- **Release**: Publishes the package to npm when a new release is created.

Note: To publish to npm, maintainers need to configure the `NPM_TOKEN` secret in the repository settings.

### How to Contribute

1. **Fork the Repository**

   - Create your own fork of the repo

2. **Create a Branch**

   - Create a feature branch (`git checkout -b feature/amazing-feature`)
   - For bug fixes, use (`git checkout -b fix/bug-description`)

3. **Make Your Changes**

   - Follow the code style and conventions
   - Add or update tests as needed
   - Keep your changes focused and related to a single issue

4. **Test Your Changes**

   - Ensure all tests pass with `npm test`
   - Add new tests if appropriate

5. **Commit Your Changes**

   - Use clear and descriptive commit messages
   - Reference issue numbers in commit messages when applicable

6. **Submit a Pull Request**
   - Provide a clear description of the changes
   - Link any related issues
   - Answer any questions or feedback during review

### Creating a CursorRule

To create a CursorRule to make AI agents work more effectively on the codebase:

1. Visit [gitingest.com](https://gitingest.com/)
2. Paste the link to the docs folder (e.g., https://github.com/The-Pocket/PocketFlow-Typescript/tree/main/docs) to generate content
3. Remove the following from the generated result:
   - All utility function files except for llm
   - The design_pattern/multi_agent.md file
   - All \_config.yaml and index.md files, except for docs/index.md
4. Save the result as a CursorRule to help AI agents understand the codebase structure better

### Development Setup

```bash
# Clone your forked repository
git clone https://github.com/yourusername/PocketFlow-Typescript.git
cd PocketFlow-Typescript

# Install dependencies
npm install

# Run tests
npm test
```

### Reporting Bugs

When reporting bugs, please include:

- A clear, descriptive title
- Detailed steps to reproduce the issue
- Expected and actual behavior
- Environment information (OS, Node.js version, etc.)
- Any additional context or screenshots

## Community

- Join our [Discord server](https://discord.gg/hUHHE9Sa6T) for discussions and support
- Follow us on [GitHub](https://github.com/The-Pocket)

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="tsconfig.json">
{
    "compilerOptions": {
      "target": "es2018",
      "module": "commonjs",
      "declaration": true,
      "outDir": "./dist",
      "strict": true,
      "esModuleInterop": true,
      "skipLibCheck": true,
      "forceConsistentCasingInFileNames": true,
      "rootDir": "./src"
    },
    "include": ["src/**/*"],
    "exclude": ["node_modules", "dist", "tests"]
  }
</file>

<file path="tsup.config.ts">
import { defineConfig } from 'tsup';

export default defineConfig({
  entry: ['src/index.ts'],
  format: ['cjs', 'esm'],
  dts: true,
  splitting: false,
  sourcemap: true,
  clean: true,
  outDir: 'dist',
  outExtension({ format }) {
    return {
      js: format === 'cjs' ? '.js' : '.mjs',
    };
  }
});
</file>

</files>
